<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ethereum Project Infrastructure</title>
    <url>/blog/Ethereum-Project-Infrastructure/</url>
    <content><![CDATA[<h1 id="Prepare"><a href="#Prepare" class="headerlink" title="Prepare"></a>Prepare</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir kickstart</span><br><span class="line"><span class="built_in">cd</span> kickstart</span><br><span class="line">npm init <span class="comment"># create package.json file</span></span><br><span class="line">npm install --save ganache-cli mocha solc fs-extra web3@1.0.0-beta.26</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Create a blockchain kickstart website.</p>
<h2 id="Contract"><a href="#Contract" class="headerlink" title="Contract"></a>Contract</h2><p><img src="contract.png" alt="contract"></p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p><img src="structure.png" alt="structure"><br><img src="overview.png" alt="overview"><br><img src="interact.png" alt="interact"></p>
<ol>
<li>create a factory contract. It has a function to deploy a new instance of <code>Campaign</code></li>
<li>User clicks <code>Create Campaign</code></li>
<li>We instruct web3/metamask to show user a transaction that invokes <code>Campaign Factory</code></li>
<li>User pays deployment costs. Factory deploy a new copy of <code>Campaign</code></li>
<li>We tell <code>Campaign Factory</code> to give us a list of all deployed campaigns.</li>
</ol>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>ethereum</tag>
        <tag>web3</tag>
      </tags>
  </entry>
  <entry>
    <title>Dapp: Lottery Contract</title>
    <url>/blog/Dapp-Lottery-Contract/</url>
    <content><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Lottery Contract</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variables</span></span><br><span class="line">Name            Purpose</span><br><span class="line">manager:    Address of person who created the contract</span><br><span class="line">players:    Array of addresses of people who entered</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function</span></span><br><span class="line">Name            Purpose</span><br><span class="line">enter:      Enters a player into lottery</span><br><span class="line">pickWinner: Randomly picks a winner and sends them the prize pool</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="Solidity-Variable"><a href="#Solidity-Variable" class="headerlink" title="Solidity Variable"></a>Solidity Variable</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Name                Notes                                    Examples</span><br><span class="line">string          Sequence of characthers                     <span class="string">"hi there"</span></span><br><span class="line">bool            Boolen value                                <span class="literal">true</span>, <span class="literal">false</span></span><br><span class="line">int             Integer, positive or negative.              0, -300000</span><br><span class="line">uint            unsigned <span class="built_in">integer</span>                            0, 3000000</span><br><span class="line">fixed/ufixed    <span class="string">'Fixes'</span> point number                        20.0001, -43.0002</span><br><span class="line">address         Has methods tied to it <span class="keyword">for</span> sending money    0x01C65bfDeD8c69ef3C28d4EF58F1dA</span><br></pre></td></tr></table></figure>

<h1 id="Reference-type"><a href="#Reference-type" class="headerlink" title="Reference type"></a>Reference type</h1><ul>
<li><code>fixed array</code>:  Array the contains a single type of element. length is fixed. int[3] –&gt; [1,2,3]</li>
<li><code>dynamic array</code>: Array the contains a single type of element. length can change over time. int[] –&gt;[1,2,3]</li>
<li><code>mapping</code>: Collection of key value pairs.  mapping(string =&gt; int)</li>
<li><code>struct</code>: Collection of key value pairs that can have different types</li>
</ul>
<h1 id="msg-global-variable"><a href="#msg-global-variable" class="headerlink" title="msg global variable"></a>msg global variable</h1><ul>
<li><code>msg.data</code>: ‘Data’ field from the call or transaction that invoked the current function</li>
<li><code>msg.gas</code> : Amount of gas the current function invocation has available</li>
<li><code>msg.sender</code>: Address of account that started the current function invocation</li>
<li><code>msg.value</code>: Amount of ether (in wei) that was sent along with the function invocation</li>
</ul>
<h1 id="Lottery-logic"><a href="#Lottery-logic" class="headerlink" title="Lottery logic"></a>Lottery logic</h1><p><img src="logic.png" alt="logic"></p>
<p><img src="logic2.png" alt="logic2"></p>
<figure class="highlight js"><figcaption><span>:contracts/Lottery.sol</span></figcaption><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.17</span>;</span><br><span class="line"></span><br><span class="line">contract Lottery &#123;</span><br><span class="line">    address public manager;</span><br><span class="line">    address[] public players;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">constructor</span> () public &#123;</span><br><span class="line">        manager = msg.sender;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">enter</span>(<span class="params"></span>)  <span class="title">public</span> <span class="title">payable</span></span>&#123;</span><br><span class="line">        <span class="built_in">require</span>(msg.value &gt;= <span class="number">.01</span> ether, <span class="string">"send at least 0.01 ether"</span>);</span><br><span class="line">        players.push(msg.sender);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">random</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> uint(sha3(block.difficulty, now, players));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// function modifier, reduce code we need to write</span></span><br><span class="line">    modifier onlyManagerCanCall() &#123;</span><br><span class="line">        <span class="comment">// only manage can pick winner</span></span><br><span class="line">        <span class="built_in">require</span>(msg.sender == manager, <span class="string">"you don't have authority"</span>);</span><br><span class="line">        _; <span class="comment">// all code in your function will replace the "_"</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">pickWinner</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">onlyManagerCanCall</span></span>&#123;</span><br><span class="line">        uint index = random() % players.length;</span><br><span class="line">        <span class="comment">// this.balance has all the ether in current smart contract</span></span><br><span class="line">        players[index].transfer(address(<span class="keyword">this</span>).balance); </span><br><span class="line">        <span class="comment">// inital dynamic array whose length is 0;</span></span><br><span class="line">        players = <span class="keyword">new</span> address[](<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">getPlayers</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span> (<span class="params">address[]</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> players;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>You can test this on remix.</p>
<h1 id="Create-a-test-case"><a href="#Create-a-test-case" class="headerlink" title="Create a test case"></a>Create a test case</h1><figure class="highlight js"><figcaption><span>:test/Lottery.test.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> assert = <span class="built_in">require</span>(<span class="string">'assert'</span>);</span><br><span class="line"><span class="keyword">const</span> ganache = <span class="built_in">require</span>(<span class="string">'ganache-cli'</span>);</span><br><span class="line"><span class="keyword">const</span> Web3 = <span class="built_in">require</span>(<span class="string">'web3'</span>)</span><br><span class="line"><span class="keyword">const</span> web3 = <span class="keyword">new</span> Web3(ganache.provider());</span><br><span class="line"><span class="keyword">const</span> &#123;interface, bytecode&#125; = <span class="built_in">require</span>(<span class="string">'../compile'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> lottery;</span><br><span class="line"><span class="keyword">let</span> accounts;</span><br><span class="line"></span><br><span class="line">beforeEach(<span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">    <span class="comment">// Get a list of all accounts</span></span><br><span class="line">    accounts = <span class="keyword">await</span> web3.eth.getAccounts();</span><br><span class="line">    </span><br><span class="line">    lottery = <span class="keyword">await</span> <span class="keyword">new</span> web3.eth.Contract(<span class="built_in">JSON</span>.parse(interface))</span><br><span class="line">    .deploy(&#123;<span class="attr">data</span>: bytecode&#125;)</span><br><span class="line">    .send(&#123;<span class="attr">from</span>: accounts[<span class="number">0</span>], <span class="attr">gas</span>: <span class="string">'1000000'</span>&#125;)</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">describe(<span class="string">'Lottery Contract'</span>, () =&gt; &#123;</span><br><span class="line">    it(<span class="string">'deploys a contract'</span>, () =&gt; &#123;</span><br><span class="line">        assert.ok(lottery.options.address);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'allow one account to enter'</span>, <span class="keyword">async</span>() =&gt; &#123;</span><br><span class="line">        <span class="keyword">await</span> lottery.methods.enter().send(&#123;<span class="attr">from</span>: accounts[<span class="number">0</span>], <span class="attr">value</span>: web3.utils.toWei(<span class="string">'0.02'</span>, <span class="string">'ether'</span>)&#125;);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">const</span> players = <span class="keyword">await</span> lottery.methods.getPlayers().call(&#123;</span><br><span class="line">            <span class="keyword">from</span>: accounts[<span class="number">0</span>]</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        assert.equal(accounts[<span class="number">0</span>], players[<span class="number">0</span>]);</span><br><span class="line">        assert.equal(<span class="number">1</span>, players.length);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'allow multi account to enter'</span>, <span class="keyword">async</span>() =&gt; &#123;</span><br><span class="line">        <span class="keyword">await</span> lottery.methods.enter().send(&#123;<span class="attr">from</span>: accounts[<span class="number">0</span>], <span class="attr">value</span>: web3.utils.toWei(<span class="string">'0.02'</span>, <span class="string">'ether'</span>)&#125;);</span><br><span class="line">        <span class="keyword">await</span> lottery.methods.enter().send(&#123;<span class="attr">from</span>: accounts[<span class="number">1</span>], <span class="attr">value</span>: web3.utils.toWei(<span class="string">'0.02'</span>, <span class="string">'ether'</span>)&#125;);</span><br><span class="line">        <span class="keyword">await</span> lottery.methods.enter().send(&#123;<span class="attr">from</span>: accounts[<span class="number">2</span>], <span class="attr">value</span>: web3.utils.toWei(<span class="string">'0.02'</span>, <span class="string">'ether'</span>)&#125;);</span><br><span class="line">        <span class="keyword">const</span> players = <span class="keyword">await</span> lottery.methods.getPlayers().call(&#123;</span><br><span class="line">            <span class="keyword">from</span>: accounts[<span class="number">0</span>]</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        assert.equal(accounts[<span class="number">0</span>], players[<span class="number">0</span>]);</span><br><span class="line">        assert.equal(accounts[<span class="number">1</span>], players[<span class="number">1</span>]);</span><br><span class="line">        assert.equal(accounts[<span class="number">2</span>], players[<span class="number">2</span>]);</span><br><span class="line">        assert.equal(<span class="number">3</span>, players.length);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'requires a minimum amount of ether to enter'</span>, <span class="keyword">async</span>() =&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">await</span> lottery.methods.enter().send(&#123;</span><br><span class="line">                <span class="keyword">from</span>: accounts[<span class="number">0</span>],</span><br><span class="line">                value: <span class="number">0</span></span><br><span class="line">            &#125;);</span><br><span class="line">            assert(<span class="literal">false</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">            assert(err);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'only manage can call pickWinner'</span>, <span class="keyword">async</span>() =&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">await</span> lottery.methods.pickWinner().send(&#123;</span><br><span class="line">                <span class="keyword">from</span>: accounts[<span class="number">1</span>],</span><br><span class="line">            &#125;);</span><br><span class="line">            assert(<span class="literal">false</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">            assert(err);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'sends money to the winner and resets the players'</span>, <span class="keyword">async</span>() =&gt; &#123;</span><br><span class="line">        <span class="keyword">await</span> lottery.methods.enter().send(&#123;</span><br><span class="line">            <span class="keyword">from</span>: accounts[<span class="number">0</span>],</span><br><span class="line">            value: web3.utils.toWei(<span class="string">'2'</span>, <span class="string">'ether'</span>)</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> initialBalance = <span class="keyword">await</span> web3.eth.getBalance(accounts[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">await</span> lottery.methods.pickWinner().send(&#123;<span class="attr">from</span>: accounts[<span class="number">0</span>]&#125;);</span><br><span class="line">        <span class="keyword">const</span> finalBalance = <span class="keyword">await</span> web3.eth.getBalance(accounts[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">const</span> difference = finalBalance - initialBalance;</span><br><span class="line">        <span class="comment">// console.log(difference);</span></span><br><span class="line">        assert(difference &gt; web3.utils.toWei(<span class="string">'1.8'</span>, <span class="string">'ether'</span>));</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight js"><figcaption><span>:./compile.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> path = <span class="built_in">require</span>(<span class="string">'path'</span>);</span><br><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">'fs'</span>);</span><br><span class="line"><span class="keyword">const</span> solc = <span class="built_in">require</span>(<span class="string">'solc'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> Path = path.resolve(__dirname, <span class="string">'contracts'</span>, <span class="string">'Lottery.sol'</span>);</span><br><span class="line"><span class="keyword">const</span> source = fs.readFileSync(Path, <span class="string">'utf8'</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = solc.compile(source, <span class="number">1</span>).contracts[<span class="string">':Lottery'</span>];</span><br><span class="line"><span class="comment">// console.log(solc.compile(source, 1))</span></span><br></pre></td></tr></table></figure>

<p>then rum <code>npm run test</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code/blockchain  npm run <span class="built_in">test</span></span><br><span class="line">&gt; inbox@ <span class="built_in">test</span> /Users/mac/Code/blockchain</span><br><span class="line">&gt; mocha</span><br><span class="line">Lottery Contract</span><br><span class="line">(node:32122) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 data listeners added. Use emitter.setMaxListeners() to increase <span class="built_in">limit</span></span><br><span class="line">    ✓ deploys a contract</span><br><span class="line">    ✓ allow one account to enter (56ms)</span><br><span class="line">    ✓ allow multi account to enter (103ms)</span><br><span class="line">    ✓ requires a minimum amount of ether to enter</span><br><span class="line">    ✓ only manage can call pickWinner</span><br><span class="line">    ✓ sends money to the winner and resets the players (74ms)</span><br></pre></td></tr></table></figure>

<h1 id="Web-of-ethereum-architecture"><a href="#Web-of-ethereum-architecture" class="headerlink" title="Web of ethereum architecture"></a>Web of ethereum architecture</h1><p><img src="arch.png" alt=""></p>
<p><img src="metamask.png" alt=""></p>
<p>Metamask running in Chrome will inject web3 v2.0 automatically.</p>
<p>Our app will use web3 v1.0, and we want to hijack our provider into Metamask one.</p>
<h2 id="install-react-create-new-project-and-install-web3"><a href="#install-react-create-new-project-and-install-web3" class="headerlink" title="install react, create new project and install web3"></a>install react, create new project and install web3</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code: sudo npm install -g create-react-app</span><br><span class="line">mac@HansonMac  ~/Code: create-react-app lottery-react</span><br><span class="line"></span><br><span class="line">Creating a new React app <span class="keyword">in</span> /Users/mac/Code/lottery-react.</span><br><span class="line"></span><br><span class="line">Installing packages. This might take a couple of minutes.</span><br><span class="line">Installing react, react-dom, and react-scripts...</span><br><span class="line"></span><br><span class="line">yarn add v1.10.1</span><br><span class="line">[1/4] 🔍  Resolving packages...</span><br><span class="line">[2/4] 🚚  Fetching packages...</span><br><span class="line">[3/4] 🔗  Linking dependencies...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">mac@HansonMac  ~/Code: yarn add web3@1.0.0-beta.26</span><br><span class="line">mac@HansonMac  ~/Code: npm run start</span><br></pre></td></tr></table></figure>


<h2 id="Hijack-Metamask-web3-to-our-version"><a href="#Hijack-Metamask-web3-to-our-version" class="headerlink" title="Hijack Metamask web3 to our version"></a>Hijack Metamask web3 to our version</h2><figure class="highlight js"><figcaption><span>:src/web3.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Web3 <span class="keyword">from</span> <span class="string">'web3'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// inject our web3 v1.0</span></span><br><span class="line"><span class="keyword">const</span> web3 = <span class="keyword">new</span> Web3(<span class="built_in">window</span>.web3.currentProvider);</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> web3;</span><br></pre></td></tr></table></figure>

<figure class="highlight js"><figcaption><span>:src/App.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">// add this to App.js</span></span><br><span class="line"><span class="keyword">import</span> web3 <span class="keyword">from</span> <span class="string">'./web3'</span></span><br></pre></td></tr></table></figure>

<h2 id="Deploy-Lottery-contract-to-Rinkeby"><a href="#Deploy-Lottery-contract-to-Rinkeby" class="headerlink" title="Deploy Lottery contract to Rinkeby"></a>Deploy Lottery contract to Rinkeby</h2><figure class="highlight js"><figcaption><span>:./deploy.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> HDWalletProvider = <span class="built_in">require</span>(<span class="string">'truffle-hdwallet-provider'</span>);</span><br><span class="line"><span class="keyword">const</span> Web3 = <span class="built_in">require</span>(<span class="string">'web3'</span>);</span><br><span class="line"><span class="keyword">const</span> &#123;interface, bytecode&#125; = <span class="built_in">require</span>(<span class="string">'./compile'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> provider = <span class="keyword">new</span> HDWalletProvider (</span><br><span class="line">    <span class="string">'dinosaur erupt zoo ...'</span>,</span><br><span class="line">    <span class="string">'https://rinkeby.infura.io/v3/451daf892abb4101b6845******'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> web3 = <span class="keyword">new</span> Web3(provider);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> deploy = <span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">    <span class="keyword">const</span> accounts = <span class="keyword">await</span> web3.eth.getAccounts();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'Attempting to deploy from account'</span>, accounts[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> result = <span class="keyword">await</span> <span class="keyword">new</span> web3.eth.Contract(<span class="built_in">JSON</span>.parse(interface))</span><br><span class="line">    .deploy(&#123;<span class="attr">data</span>: bytecode&#125;)</span><br><span class="line">    .send(&#123;<span class="attr">gas</span>: <span class="string">'5000000'</span>, <span class="attr">from</span>: accounts[<span class="number">0</span>]&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">console</span>.log(interface);</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'Contract deployed to'</span>, result.options.address);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">deploy();</span><br></pre></td></tr></table></figure>

<p>after run <code>node deploy.js</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Attempting to deploy from account 0x01C65bfDeD8c69ef3C28d4EF58F1dA46DeAF13Cd</span><br><span class="line">[&#123;<span class="string">"constant"</span>:<span class="literal">true</span>,<span class="string">"inputs"</span>:[],<span class="string">"name"</span>:<span class="string">"manager"</span>,<span class="string">"outputs"</span>:[&#123;<span class="string">"name"</span>:<span class="string">""</span>,<span class="string">"type"</span>:<span class="string">"address"</span>&#125;],<span class="string">"payable"</span>:<span class="literal">false</span>,<span class="string">"stateMutability"</span>:<span class="string">"view"</span>,<span class="string">"type"</span>:<span class="string">"function"</span>&#125;,&#123;<span class="string">"constant"</span>:<span class="literal">false</span>,<span class="string">"inputs"</span>:[],<span class="string">"name"</span>:<span class="string">"pickWinner"</span>,<span class="string">"outputs"</span>:[],<span class="string">"payable"</span>:<span class="literal">false</span>,<span class="string">"stateMutability"</span>:<span class="string">"nonpayable"</span>,<span class="string">"type"</span>:<span class="string">"function"</span>&#125;,&#123;<span class="string">"constant"</span>:<span class="literal">true</span>,<span class="string">"inputs"</span>:[],<span class="string">"name"</span>:<span class="string">"random"</span>,<span class="string">"outputs"</span>:[&#123;<span class="string">"name"</span>:<span class="string">""</span>,<span class="string">"type"</span>:<span class="string">"uint256"</span>&#125;],<span class="string">"payable"</span>:<span class="literal">false</span>,<span class="string">"stateMutability"</span>:<span class="string">"view"</span>,<span class="string">"type"</span>:<span class="string">"function"</span>&#125;,&#123;<span class="string">"constant"</span>:<span class="literal">true</span>,<span class="string">"inputs"</span>:[],<span class="string">"name"</span>:<span class="string">"getPlayers"</span>,<span class="string">"outputs"</span>:[&#123;<span class="string">"name"</span>:<span class="string">""</span>,<span class="string">"type"</span>:<span class="string">"address[]"</span>&#125;],<span class="string">"payable"</span>:<span class="literal">false</span>,<span class="string">"stateMutability"</span>:<span class="string">"view"</span>,<span class="string">"type"</span>:<span class="string">"function"</span>&#125;,&#123;<span class="string">"constant"</span>:<span class="literal">false</span>,<span class="string">"inputs"</span>:[],<span class="string">"name"</span>:<span class="string">"enter"</span>,<span class="string">"outputs"</span>:[],<span class="string">"payable"</span>:<span class="literal">true</span>,<span class="string">"stateMutability"</span>:<span class="string">"payable"</span>,<span class="string">"type"</span>:<span class="string">"function"</span>&#125;,&#123;<span class="string">"constant"</span>:<span class="literal">true</span>,<span class="string">"inputs"</span>:[&#123;<span class="string">"name"</span>:<span class="string">""</span>,<span class="string">"type"</span>:<span class="string">"uint256"</span>&#125;],<span class="string">"name"</span>:<span class="string">"players"</span>,<span class="string">"outputs"</span>:[&#123;<span class="string">"name"</span>:<span class="string">""</span>,<span class="string">"type"</span>:<span class="string">"address"</span>&#125;],<span class="string">"payable"</span>:<span class="literal">false</span>,<span class="string">"stateMutability"</span>:<span class="string">"view"</span>,<span class="string">"type"</span>:<span class="string">"function"</span>&#125;,&#123;<span class="string">"inputs"</span>:[],<span class="string">"payable"</span>:<span class="literal">false</span>,<span class="string">"stateMutability"</span>:<span class="string">"nonpayable"</span>,<span class="string">"type"</span>:<span class="string">"constructor"</span>&#125;]</span><br><span class="line">Contract deployed to 0x2a8683527b110f5f37502CD5c7C62f574A75499A</span><br></pre></td></tr></table></figure>

<h2 id="Import-ABI"><a href="#Import-ABI" class="headerlink" title="Import ABI"></a>Import ABI</h2><p>create a new file <code>lottery.js</code> in <code>src</code> folder. And copy all the ABI and deployed address.</p>
<figure class="highlight js"><figcaption><span>:src/lottery.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> web3 <span class="keyword">from</span> <span class="string">`./web3`</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> address = <span class="string">'0x2a8683527b110f5f37502CD5c7C62f574A75499A'</span>;</span><br><span class="line"><span class="keyword">const</span> abi = [&#123;</span><br><span class="line">    <span class="string">"constant"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"inputs"</span>: [],</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"manager"</span>,</span><br><span class="line">    <span class="string">"outputs"</span>: [&#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"address"</span></span><br><span class="line">    &#125;],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="string">"constant"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"inputs"</span>: [],</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"pickWinner"</span>,</span><br><span class="line">    <span class="string">"outputs"</span>: [],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="string">"constant"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"inputs"</span>: [],</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"random"</span>,</span><br><span class="line">    <span class="string">"outputs"</span>: [&#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">    &#125;],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="string">"constant"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"inputs"</span>: [],</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"getPlayers"</span>,</span><br><span class="line">    <span class="string">"outputs"</span>: [&#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"address[]"</span></span><br><span class="line">    &#125;],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="string">"constant"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"inputs"</span>: [],</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"enter"</span>,</span><br><span class="line">    <span class="string">"outputs"</span>: [],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"payable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="string">"constant"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"inputs"</span>: [&#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"uint256"</span></span><br><span class="line">    &#125;],</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"players"</span>,</span><br><span class="line">    <span class="string">"outputs"</span>: [&#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"address"</span></span><br><span class="line">    &#125;],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"view"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"function"</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="string">"inputs"</span>: [],</span><br><span class="line">    <span class="string">"payable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"stateMutability"</span>: <span class="string">"nonpayable"</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"constructor"</span></span><br><span class="line">&#125;];</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> <span class="keyword">new</span> web3.eth.Contract(abi, address);</span><br></pre></td></tr></table></figure>

<p><img src="abi.png" alt="abi"></p>
<h2 id="Rendering-Contract-Data"><a href="#Rendering-Contract-Data" class="headerlink" title="Rendering Contract Data"></a>Rendering Contract Data</h2><p>Modify <code>src/App.js</code> code. See source code here:<br><a href="https://github.com/hansonzhao007/lottery/blob/master/src/App.js" target="_blank" rel="noopener">https://github.com/hansonzhao007/lottery/blob/master/src/App.js</a></p>
<h2 id="Deploying-React-app-to-GitHub-Pages"><a href="#Deploying-React-app-to-GitHub-Pages" class="headerlink" title="Deploying React app to GitHub Pages"></a>Deploying React app to GitHub Pages</h2><p>create a new repository, and connect your project.</p>
<p>install <code>gh-pages</code> package</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install gh-pages --save-dev</span><br></pre></td></tr></table></figure>

<p>Add some properties to the app’s package.json file</p>
<figure class="highlight js"><figcaption><span>:./package.json</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="string">"homepage"</span>: <span class="string">"http://gitname.github.io/react-gh-pages"</span></span><br><span class="line"></span><br><span class="line"><span class="string">"scripts"</span>: &#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="string">"predeploy"</span>: <span class="string">"npm run build"</span>,</span><br><span class="line">  <span class="string">"deploy"</span>: <span class="string">"gh-pages -d build"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Generate a production build of your app, and deploy it to GitHub Pages</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm run deploy</span><br></pre></td></tr></table></figure>

<p>You can see the example on <a href="http://xszhao.science/lottery" target="_blank" rel="noopener">http://xszhao.science/lottery</a></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://github.com/gitname/react-gh-pages" target="_blank" rel="noopener">Deploying a React App* to GitHub Pages</a></p>
]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>ethereum</tag>
        <tag>web3</tag>
      </tags>
  </entry>
  <entry>
    <title>Write ethereum test code</title>
    <url>/blog/Write-ethereum-test-code/</url>
    <content><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>The fold tree is like following:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--| contracts</span><br><span class="line">----| Inbox.sol</span><br><span class="line">--| <span class="built_in">test</span></span><br><span class="line">----| Inbox_test.js</span><br><span class="line">--| compile.js</span><br><span class="line">--| package.json</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><h2 id="Smart-Contract"><a href="#Smart-Contract" class="headerlink" title="Smart Contract"></a>Smart Contract</h2><figure class="highlight js"><figcaption><span>:Inbox.sol</span></figcaption><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.4</span><span class="number">.17</span>;</span><br><span class="line"></span><br><span class="line">contract Inbox &#123;</span><br><span class="line">    <span class="comment">// a globle store</span></span><br><span class="line">    string public message;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">Inbox</span>(<span class="params">string initialMessage</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        message = initialMessage;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">setMessage</span>(<span class="params">string newMessage</span>) <span class="title">public</span> </span>&#123;</span><br><span class="line">        message = newMessage;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Compile-Script"><a href="#Compile-Script" class="headerlink" title="Compile Script"></a>Compile Script</h2><figure class="highlight js"><figcaption><span>:compile.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> path = <span class="built_in">require</span>(<span class="string">'path'</span>);</span><br><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">'fs'</span>);</span><br><span class="line"><span class="keyword">const</span> solc = <span class="built_in">require</span>(<span class="string">'solc'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> inboxPath = path.resolve(__dirname, <span class="string">'contracts'</span>, <span class="string">'Inbox.sol'</span>);</span><br><span class="line"><span class="keyword">const</span> source = fs.readFileSync(inboxPath, <span class="string">'utf8'</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(solc.compile(source, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// export the smart contract bytecode and interface, which are used to create or deploy a contract, interact with the contract.</span></span><br><span class="line"><span class="comment">// module.exports = solc.compile(source, 1).contracts[':Inbox'];</span></span><br></pre></td></tr></table></figure>

<p>run <code>node compile</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123; contracts:</span><br><span class="line">  <span class="comment"># we will see multiple contract pairs if we use compile.js to compile multiple smart contracts</span></span><br><span class="line">   &#123; <span class="string">':Inbox'</span>: <span class="comment"># contract name</span></span><br><span class="line">      &#123; assembly: [Object],</span><br><span class="line">        <span class="comment"># `bytecode is the actual content that is deployed on ethereum network`</span></span><br><span class="line">        bytecode: <span class="string">'608060405234801561001057600080fd5b5060405161038c38038061038c83398101604052805101805161003a906000906020840190610041565b50506100dc565b828054600181600116156101000203166002900490600052602060002090601f016020900481019282601f1061008257805160ff19168380011785556100af565b828001600101855582156100af579182015b828111156100af578251825591602001919060010190610094565b506100bb9291506100bf565b5090565b6100d991905b808211156100bb57600081556001016100c5565b90565b6102a1806100eb6000396000f30060806040526004361061004b5763ffffffff7c0100000000000000000000000000000000000000000000000000000000600035041663368b87728114610050578063e21f37ce146100ab575b600080fd5b34801561005c57600080fd5b506040805160206004803580820135601f81018490048402850184019095528484526100a99436949293602493928401919081908401838280828437509497506101359650505050505050565b005b3480156100b757600080fd5b506100c061014c565b6040805160208082528351818301528351919283929083019185019080838360005b838110156100fa5781810151838201526020016100e2565b50505050905090810190601f1680156101275780820380516001836020036101000a031916815260200191505b509250505060405180910390f35b80516101489060009060208401906101da565b5050565b6000805460408051602060026001851615610100026000190190941693909304601f810184900484028201840190925281815292918301828280156101d25780601f106101a7576101008083540402835291602001916101d2565b820191906000526020600020905b8154815290600101906020018083116101b557829003601f168201915b505050505081565b828054600181600116156101000203166002900490600052602060002090601f016020900481019282601f1061021b57805160ff1916838001178555610248565b82800160010185558215610248579182015b8281111561024857825182559160200191906001019061022d565b50610254929150610258565b5090565b61027291905b80821115610254576000815560010161025e565b905600a165627a7a72305820387e3a84c249a8fe3de554c17239e7aefa478da030f9a901807f3b1989c4d9020029'</span>,</span><br><span class="line">        functionHashes: [Object],</span><br><span class="line">        gasEstimates: [Object],</span><br><span class="line">        <span class="comment"># `Our contract ABI`</span></span><br><span class="line">        <span class="comment"># `List out all the function we can use`</span></span><br><span class="line">        interface: <span class="string">'[&#123;"constant":false,"inputs":[&#123;"name":"newMessage","type":"string"&#125;],"name":"setMessage","outputs":[],"payable":false,"stateMutability":"nonpayable","type":"function"&#125;,&#123;"constant":true,"inputs":[],"name":"message","outputs":[&#123;"name":"","type":"string"&#125;],"payable":false,"stateMutability":"view","type":"function"&#125;,&#123;"inputs":[&#123;"name":"initialMessage","type":"string"&#125;],"payable":false,"stateMutability":"nonpayable","type":"constructor"&#125;]'</span>,</span><br><span class="line">        metadata: <span class="string">'&#123;"compiler":&#123;"version":"0.4.25+commit.59dbf8f1"&#125;,"language":"Solidity","output":&#123;"abi":[&#123;"constant":false,"inputs":[&#123;"name":"newMessage","type":"string"&#125;],"name":"setMessage","outputs":[],"payable":false,"stateMutability":"nonpayable","type":"function"&#125;,&#123;"constant":true,"inputs":[],"name":"message","outputs":[&#123;"name":"","type":"string"&#125;],"payable":false,"stateMutability":"view","type":"function"&#125;,&#123;"inputs":[&#123;"name":"initialMessage","type":"string"&#125;],"payable":false,"stateMutability":"nonpayable","type":"constructor"&#125;],"devdoc":&#123;"methods":&#123;&#125;&#125;,"userdoc":&#123;"methods":&#123;&#125;&#125;&#125;,"settings":&#123;"compilationTarget":&#123;"":"Inbox"&#125;,"evmVersion":"byzantium","libraries":&#123;&#125;,"optimizer":&#123;"enabled":true,"runs":200&#125;,"remappings":[]&#125;,"sources":&#123;"":&#123;"keccak256":"0xcbb1d3ea37d011841c666c5ac8aeb21eacca3d151de9e40dde61095138cc17c0","urls":["bzzr://2d2ff492c4461ba96d5e54274865acbc8bdb43f00337d4222681d001a6ea05f3"]&#125;&#125;,"version":1&#125;'</span>,</span><br><span class="line">        opcodes: <span class="string">'PUSH1 0x80 PUSH1 0x40 MSTORE CALLVALUE DUP1 ISZERO PUSH2 0x10 JUMPI PUSH1 0x0 DUP1 REVERT JUMPDEST POP PUSH1 0x40MLOAD PUSH2 0x38C CODESIZE SUB DUP1 PUSH2 0x38C DUP4 CODECOPY DUP2 ADD PUSH1 0x40 MSTORE DUP1 MLOAD ADD DUP1 MLOAD PUSH2 0x3A SWAP1PUSH1 0x0 SWAP1 PUSH1 0x20 DUP5 ADD SWAP1 PUSH2 0x41 JUMP JUMPDEST POP POP PUSH2 0xDC JUMP JUMPDEST DUP3 DUP1 SLOAD PUSH1 0x1 DUP2 PUSH1 0x1 AND ISZERO PUSH2 0x100 MUL SUB AND PUSH1 0x2 SWAP1 DIV SWAP1 PUSH1 0x0 MSTORE PUSH1 0x20 PUSH1 0x0 KECCAK256 SWAP1 PUSH1 0x1F ADD PUSH1 0x20 SWAP1 DIV DUP2 ADD SWAP3 DUP3 PUSH1 0x1F LT PUSH2 0x82 JUMPI DUP1 MLOAD PUSH1 0xFF NOT AND DUP4 DUP1 ADD OR DUP6 SSTORE PUSH2 0xAF JUMP JUMPDEST DUP3 DUP1 ADD PUSH1 0x1 ADD DUP6 SSTORE DUP3 ISZERO PUSH2 0xAF JUMPI SWAP2 DUP3 ADD JUMPDEST DUP3 DUP2 GT ISZERO PUSH2 0xAF JUMPI DUP3 MLOAD DUP3 SSTORE SWAP2 PUSH1 0x20 ADD SWAP2 SWAP1 PUSH1 0x1 ADD SWAP1 PUSH2 0x94 JUMP JUMPDEST POP PUSH2 0xBB SWAP3 SWAP2 POP PUSH2 0xBF JUMP JUMPDEST POP SWAP1 JUMP JUMPDEST PUSH2 0xD9 SWAP2 SWAP1 JUMPDEST DUP1 DUP3 GT ISZERO PUSH2 0xBB JUMPI PUSH1 0x0 DUP2 SSTORE PUSH1 0x1 ADD PUSH2 0xC5 JUMP JUMPDEST SWAP1 JUMP JUMPDEST PUSH2 0x2A1 DUP1 PUSH2 0xEB PUSH1 0x0 CODECOPY PUSH1 0x0 RETURN STOP PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x4 CALLDATASIZE LT PUSH2 0x4B JUMPI PUSH4 0xFFFFFFFF PUSH29 0x100000000000000000000000000000000000000000000000000000000 PUSH1 0x0 CALLDATALOAD DIV AND PUSH4 0x368B8772 DUP2 EQ PUSH2 0x50 JUMPI DUP1 PUSH4 0xE21F37CE EQ PUSH2 0xAB JUMPI JUMPDEST PUSH1 0x0 DUP1 REVERT JUMPDEST CALLVALUE DUP1 ISZERO PUSH2 0x5C JUMPI PUSH1 0x0 DUP1 REVERT JUMPDEST POP PUSH1 0x40 DUP1 MLOAD PUSH1 0x20 PUSH1 0x4 DUP1 CALLDATALOAD DUP1 DUP3 ADD CALLDATALOAD PUSH1 0x1F DUP2 ADD DUP5 SWAP1 DIV DUP5 MUL DUP6 ADD DUP5 ADD SWAP1 SWAP6 MSTORE DUP5 DUP5 MSTORE PUSH2 0xA9 SWAP5 CALLDATASIZE SWAP5 SWAP3 SWAP4 PUSH1 0x24 SWAP4 SWAP3 DUP5 ADD SWAP2 SWAP1 DUP2 SWAP1 DUP5 ADD DUP4 DUP3 DUP1 DUP3 DUP5 CALLDATACOPY POP SWAP5 SWAP8 POP PUSH2 0x135 SWAP7POP POP POP POP POP POP POP JUMP JUMPDEST STOP JUMPDEST CALLVALUE DUP1 ISZERO PUSH2 0xB7 JUMPI PUSH1 0x0 DUP1 REVERT JUMPDEST POP PUSH2 0xC0 PUSH2 0x14C JUMP JUMPDEST PUSH1 0x40 DUP1 MLOAD PUSH1 0x20 DUP1 DUP3 MSTORE DUP4 MLOAD DUP2 DUP4 ADD MSTORE DUP4 MLOAD SWAP2 SWAP3 DUP4 SWAP3 SWAP1 DUP4 ADD SWAP2 DUP6 ADD SWAP1 DUP1 DUP4 DUP4 PUSH1 0x0 JUMPDEST DUP4 DUP2 LT ISZERO PUSH2 0xFA JUMPI DUP2 DUP2 ADD MLOAD DUP4 DUP3 ADD MSTORE PUSH1 0x20 ADD PUSH2 0xE2 JUMP JUMPDEST POP POP POP POP SWAP1 POP SWAP1 DUP2 ADD SWAP1 PUSH1 0x1F AND DUP1 ISZERO PUSH2 0x127 JUMPI DUP1 DUP3 SUB DUP1 MLOAD PUSH1 0x1 DUP4 PUSH1 0x20 SUB PUSH2 0x100 EXP SUB NOT AND DUP2 MSTORE PUSH1 0x20 ADD SWAP2 POP JUMPDEST POP SWAP3 POP POP POP PUSH1 0x40 MLOAD DUP1 SWAP2 SUB SWAP1 RETURN JUMPDEST DUP1 MLOAD PUSH2 0x148 SWAP1 PUSH1 0x0 SWAP1 PUSH1 0x20 DUP5 ADD SWAP1 PUSH2 0x1DA JUMP JUMPDEST POP POP JUMP JUMPDEST PUSH1 0x0 DUP1 SLOAD PUSH1 0x40 DUP1MLOAD PUSH1 0x20 PUSH1 0x2 PUSH1 0x1 DUP6 AND ISZERO PUSH2 0x100 MUL PUSH1 0x0 NOT ADD SWAP1 SWAP5 AND SWAP4 SWAP1 SWAP4 DIV PUSH1 0x1F DUP2 ADD DUP5 SWAP1 DIV DUP5 MUL DUP3 ADD DUP5 ADD SWAP1 SWAP3 MSTORE DUP2 DUP2 MSTORE SWAP3 SWAP2 DUP4 ADD DUP3 DUP3 DUP1 ISZERO PUSH2 0x1D2 JUMPI DUP1 PUSH1 0x1F LT PUSH2 0x1A7 JUMPI PUSH2 0x100 DUP1 DUP4 SLOAD DIV MUL DUP4 MSTORE SWAP2 PUSH1 0x20 ADD SWAP2PUSH2 0x1D2 JUMP JUMPDEST DUP3 ADD SWAP2 SWAP1 PUSH1 0x0 MSTORE PUSH1 0x20 PUSH1 0x0 KECCAK256 SWAP1 JUMPDEST DUP2 SLOAD DUP2 MSTORE SWAP1 PUSH1 0x1 ADD SWAP1 PUSH1 0x20 ADD DUP1 DUP4 GT PUSH2 0x1B5 JUMPI DUP3 SWAP1 SUB PUSH1 0x1F AND DUP3 ADD SWAP2 JUMPDEST POP POP POP POP POP DUP2 JUMP JUMPDEST DUP3 DUP1 SLOAD PUSH1 0x1 DUP2 PUSH1 0x1 AND ISZERO PUSH2 0x100 MUL SUB AND PUSH1 0x2 SWAP1 DIV SWAP1 PUSH1 0x0 MSTORE PUSH1 0x20 PUSH1 0x0 KECCAK256 SWAP1 PUSH1 0x1F ADD PUSH1 0x20 SWAP1 DIV DUP2 ADD SWAP3 DUP3 PUSH1 0x1F LT PUSH2 0x21B JUMPI DUP1 MLOAD PUSH1 0xFF NOT AND DUP4 DUP1 ADD OR DUP6 SSTORE PUSH2 0x248 JUMP JUMPDEST DUP3 DUP1 ADD PUSH1 0x1 ADD DUP6SSTORE DUP3 ISZERO PUSH2 0x248 JUMPI SWAP2 DUP3 ADD JUMPDEST DUP3 DUP2 GT ISZERO PUSH2 0x248 JUMPI DUP3 MLOAD DUP3 SSTORE SWAP2 PUSH1 0x20 ADD SWAP2 SWAP1 PUSH1 0x1 ADD SWAP1 PUSH2 0x22D JUMP JUMPDEST POP PUSH2 0x254 SWAP3 SWAP2 POP PUSH2 0x258 JUMP JUMPDEST POP SWAP1 JUMP JUMPDEST PUSH2 0x272 SWAP2 SWAP1 JUMPDEST DUP1 DUP3 GT ISZERO PUSH2 0x254 JUMPI PUSH1 0x0 DUP2 SSTORE PUSH1 0x1 ADD PUSH20x25E JUMP JUMPDEST SWAP1 JUMP STOP LOG1 PUSH6 0x627A7A723058 KECCAK256 CODESIZE PUSH31 0x3A84C249A8FE3DE554C17239E7AEFA478DA030F9A901807F3B1989C4D90200 0x29 '</span>,</span><br><span class="line">        runtimeBytecode: <span class="string">'60806040526004361061004b5763ffffffff7c0100000000000000000000000000000000000000000000000000000000600035041663368b87728114610050578063e21f37ce146100ab575b600080fd5b34801561005c57600080fd5b506040805160206004803580820135601f81018490048402850184019095528484526100a99436949293602493928401919081908401838280828437509497506101359650505050505050565b005b3480156100b757600080fd5b506100c061014c565b6040805160208082528351818301528351919283929083019185019080838360005b838110156100fa5781810151838201526020016100e2565b50505050905090810190601f1680156101275780820380516001836020036101000a031916815260200191505b509250505060405180910390f35b80516101489060009060208401906101da565b5050565b6000805460408051602060026001851615610100026000190190941693909304601f810184900484028201840190925281815292918301828280156101d25780601f106101a7576101008083540402835291602001916101d2565b820191906000526020600020905b8154815290600101906020018083116101b557829003601f168201915b505050505081565b828054600181600116156101000203166002900490600052602060002090601f016020900481019282601f1061021b57805160ff1916838001178555610248565b82800160010185558215610248579182015b8281111561024857825182559160200191906001019061022d565b50610254929150610258565b5090565b61027291905b80821115610254576000815560010161025e565b905600a165627a7a72305820387e3a84c249a8fe3de554c17239e7aefa478da030f9a901807f3b1989c4d9020029'</span>,</span><br><span class="line">        srcmap: <span class="string">'26:242:0:-;;;87:86;8:9:-1;5:2;;;30:1;27;20:12;5:2;87:86:0;;;;;;;;;;;;;;;;;142:24;;;;:7;;:24;;;;;:::i;:::-;;87:86;26:242;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;-1:-1:-1;26:242:0;;;-1:-1:-1;26:242:0;:::i;:::-;;;:::o;:::-;;;;;;;;;;;;;;;;;;;;:::o;:::-;;;;;;;'</span>,</span><br><span class="line">        srcmapRuntime: <span class="string">'26:242:0:-;;;;;;;;;;;;;;;;;;;;;;;;;;;;183:83;;8:9:-1;5:2;;;30:1;27;20:12;5:2;-1:-1;183:83:0;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;-1:-1:-1;183:83:0;;-1:-1:-1;183:83:0;;-1:-1:-1;;;;;;;183:83:0;;;55:21;;8:9:-1;5:2;;;30:1;27;20:12;5:2;55:21:0;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;8:100:-1;33:3;30:1;27:10;8:100;;;90:11;;;84:18;71:11;;;64:39;52:2;45:10;8:100;;;12:14;55:21:0;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;183:83;239:20;;;;:7;;:20;;;;;:::i;:::-;;183:83;:::o;55:21::-;;;;;;;;;;;;;;;-1:-1:-1;;55:21:0;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;:::o;26:242::-;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;-1:-1:-1;26:242:0;;;-1:-1:-1;26:242:0;:::i;:::-;;;:::o;:::-;;;;;;;;;;;;;;;;;;;;:::o'</span> &#125; &#125;,</span><br><span class="line">  errors:</span><br><span class="line">   [ <span class="string">':7:5: Warning: Defining constructors as functions with the same name as the contract is deprecated. Use "constructor(...) &#123; ... &#125;" instead.\n    function Inbox(string initialMessage) public &#123;\n    ^ (Relevant source part starts here and spans across multiple lines).\n'</span> ],</span><br><span class="line">  sourceList: [ <span class="string">''</span> ],</span><br><span class="line">  sources: &#123; <span class="string">''</span>: &#123; AST: [Object] &#125; &#125; &#125;</span><br></pre></td></tr></table></figure>
<h2 id="Test-case"><a href="#Test-case" class="headerlink" title="Test case"></a>Test case</h2><figure class="highlight js"><figcaption><span>:Inbox_test.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> assert = <span class="built_in">require</span>(<span class="string">'assert'</span>);</span><br><span class="line"><span class="keyword">const</span> ganache = <span class="built_in">require</span>(<span class="string">'ganache-cli'</span>);</span><br><span class="line"><span class="keyword">const</span> Web3 = <span class="built_in">require</span>(<span class="string">'web3'</span>)</span><br><span class="line"><span class="keyword">const</span> web3 = <span class="keyword">new</span> Web3(ganache.provider());</span><br><span class="line"><span class="comment">// in order to use the compiled module, change the last line in compile.js to </span></span><br><span class="line"><span class="comment">// module.exports = solc.compile(source, 1).contracts[':Inbox'];</span></span><br><span class="line"><span class="keyword">const</span> &#123;interface, bytecode&#125; = <span class="built_in">require</span>(<span class="string">'../compile'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> accounts;</span><br><span class="line"></span><br><span class="line">beforeEach(<span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">    <span class="comment">// Get a list of all accounts</span></span><br><span class="line">    accounts = <span class="keyword">await</span> web3.eth.getAccounts();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Use one of those accounts to deploy</span></span><br><span class="line">    <span class="comment">// the constract</span></span><br><span class="line">    inbox = <span class="keyword">await</span> <span class="keyword">new</span> web3.eth.Contract(<span class="built_in">JSON</span>.parse(interface)) </span><br><span class="line">    <span class="comment">// use web3 module eth, and the function Contract of eth module. </span></span><br><span class="line">    <span class="comment">// This function can be used to interact with the contract exist </span></span><br><span class="line">    <span class="comment">// in blockchain or deploy a new contract instance.</span></span><br><span class="line">    .deploy(&#123;<span class="attr">data</span>: bytecode, <span class="attr">arguments</span>: [<span class="string">'Hi there!'</span>]&#125;)</span><br><span class="line">    <span class="comment">// create a smart contract, create a new instance and call Inbox() function</span></span><br><span class="line">    <span class="comment">// passing argument 'Hi there!'. deploy the instance to ethereum</span></span><br><span class="line">    .send(&#123;<span class="attr">from</span>: accounts[<span class="number">0</span>], <span class="attr">gas</span>: <span class="string">'1000000'</span>&#125;);</span><br><span class="line">    <span class="comment">// Instructs web3 to send out a transaction that creates this contract</span></span><br><span class="line">    <span class="comment">// specify who is going to deploying the contract</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">describe(<span class="string">'Inbox'</span>, () =&gt; &#123;</span><br><span class="line">    it(<span class="string">'deploys a contract'</span>, () =&gt; &#123;</span><br><span class="line">        assert.ok(inbox.options.address);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'has a default message'</span>, <span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">        <span class="keyword">const</span> message = <span class="keyword">await</span> inbox.methods.message().call();</span><br><span class="line">        assert.equal(message, <span class="string">'Hi there!'</span>);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    it(<span class="string">'can change message'</span>, <span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">        <span class="comment">// because you want to modify, so you need to identify who is going to change and the gas you want to pay</span></span><br><span class="line">        <span class="keyword">await</span> inbox.methods.setMessage(<span class="string">'bye'</span>).send(&#123;<span class="attr">from</span>: accounts[<span class="number">0</span>], <span class="attr">gas</span>: <span class="string">'1000000'</span>&#125;);</span><br><span class="line">        <span class="keyword">const</span> message = <span class="keyword">await</span> inbox.methods.message().call();</span><br><span class="line">        assert.equal(message, <span class="string">'bye'</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>then run <code>npm run text</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; inbox@ <span class="built_in">test</span> /Users/mac/Code/blockchain</span><br><span class="line">&gt; mocha</span><br><span class="line"></span><br><span class="line">  Inbox</span><br><span class="line">(node:24604) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 data listeners added. Use emitter.setMaxListeners() to increase <span class="built_in">limit</span></span><br><span class="line">Contract &#123;</span><br><span class="line">  currentProvider: [Getter/Setter],</span><br><span class="line">  _requestManager:</span><br><span class="line">   RequestManager &#123;</span><br><span class="line">     provider:</span><br><span class="line">      l &#123;</span><br><span class="line">        domain: null,</span><br><span class="line">        _events: [Object],</span><br><span class="line">        _eventsCount: 1,</span><br><span class="line">        _maxListeners: undefined,</span><br><span class="line">        options: [Object],</span><br><span class="line">        engine: [Object],</span><br><span class="line">        manager: [Object],</span><br><span class="line">        sendAsync: [Function: bound ],</span><br><span class="line">        send: [Function: bound ],</span><br><span class="line">        close: [Function: bound ],</span><br><span class="line">        _queueRequest: [Function: bound ],</span><br><span class="line">        _processRequestQueue: [Function: bound ],</span><br><span class="line">        _requestQueue: [],</span><br><span class="line">        _requestInProgress: <span class="literal">false</span> &#125;,</span><br><span class="line">     providers:</span><br><span class="line">      &#123; WebsocketProvider: [Function: WebsocketProvider],</span><br><span class="line">        HttpProvider: [Function: HttpProvider],</span><br><span class="line">        IpcProvider: [Function: IpcProvider] &#125;,</span><br><span class="line">     subscriptions: &#123;&#125; &#125;,</span><br><span class="line">  givenProvider: null,</span><br><span class="line">  <span class="comment"># the communication layer with actual block chain</span></span><br><span class="line">  providers:</span><br><span class="line">   &#123; WebsocketProvider: [Function: WebsocketProvider],</span><br><span class="line">     HttpProvider: [Function: HttpProvider],</span><br><span class="line">     IpcProvider: [Function: IpcProvider] &#125;,</span><br><span class="line">  _provider:</span><br><span class="line">   l &#123;</span><br><span class="line">     domain: null,</span><br><span class="line">     _events: &#123; data: [Array] &#125;,</span><br><span class="line">     _eventsCount: 1,</span><br><span class="line">     _maxListeners: undefined,</span><br><span class="line">     options:</span><br><span class="line">      &#123; vmErrorsOnRPCResponse: <span class="literal">true</span>,</span><br><span class="line">        verbose: <span class="literal">false</span>,</span><br><span class="line">        asyncRequestProcessing: <span class="literal">false</span>,</span><br><span class="line">        logger: [Object],</span><br><span class="line">        seed: <span class="string">'lE5BIQtDPC'</span>,</span><br><span class="line">        mnemonic: <span class="string">'fetch frequent marble basket mad split print order census canyon spot ugly'</span>,</span><br><span class="line">        network_id: 1541867444670,</span><br><span class="line">        total_accounts: 10,</span><br><span class="line">        gasPrice: <span class="string">'0x77359400'</span>,</span><br><span class="line">        default_balance_ether: 100,</span><br><span class="line">        unlocked_accounts: [],</span><br><span class="line">        hdPath: <span class="string">'m/44\'</span>/60\<span class="string">'/0\'</span>/0/<span class="string">',</span></span><br><span class="line"><span class="string">        gasLimit: '</span>0x6691b7<span class="string">',</span></span><br><span class="line"><span class="string">        defaultTransactionGasLimit: '</span>0x15f90<span class="string">',</span></span><br><span class="line"><span class="string">        time: null,</span></span><br><span class="line"><span class="string">        debug: false,</span></span><br><span class="line"><span class="string">        allowUnlimitedContractSize: false &#125;,</span></span><br><span class="line"><span class="string">     engine:</span></span><br><span class="line"><span class="string">      s &#123;</span></span><br><span class="line"><span class="string">        domain: null,</span></span><br><span class="line"><span class="string">        _events: [Object],</span></span><br><span class="line"><span class="string">        _eventsCount: 1,</span></span><br><span class="line"><span class="string">        _maxListeners: 100,</span></span><br><span class="line"><span class="string">        _blockTracker: [Object],</span></span><br><span class="line"><span class="string">        _ready: [Object],</span></span><br><span class="line"><span class="string">        currentBlock: [Object],</span></span><br><span class="line"><span class="string">        _providers: [Array],</span></span><br><span class="line"><span class="string">        manager: [Object] &#125;,</span></span><br><span class="line"><span class="string">     manager:</span></span><br><span class="line"><span class="string">      s &#123;</span></span><br><span class="line"><span class="string">        state: [Object],</span></span><br><span class="line"><span class="string">        options: [Object],</span></span><br><span class="line"><span class="string">        initialized: true,</span></span><br><span class="line"><span class="string">        initialization_error: null,</span></span><br><span class="line"><span class="string">        post_initialization_callbacks: [],</span></span><br><span class="line"><span class="string">        engine: [Object],</span></span><br><span class="line"><span class="string">        currentBlock: [Object] &#125;,</span></span><br><span class="line"><span class="string">     sendAsync: [Function: bound ],</span></span><br><span class="line"><span class="string">     send: [Function: bound ],</span></span><br><span class="line"><span class="string">     close: [Function: bound ],</span></span><br><span class="line"><span class="string">     _queueRequest: [Function: bound ],</span></span><br><span class="line"><span class="string">     _processRequestQueue: [Function: bound ],</span></span><br><span class="line"><span class="string">     _requestQueue: [],</span></span><br><span class="line"><span class="string">     _requestInProgress: false &#125;,</span></span><br><span class="line"><span class="string">  setProvider: [Function],</span></span><br><span class="line"><span class="string">  BatchRequest: [Function: bound Batch],</span></span><br><span class="line"><span class="string">  extend:</span></span><br><span class="line"><span class="string">   &#123; [Function: ex]</span></span><br><span class="line"><span class="string">     formatters:</span></span><br><span class="line"><span class="string">      &#123; inputDefaultBlockNumberFormatter: [Function: inputDefaultBlockNumberFormatter],</span></span><br><span class="line"><span class="string">        inputBlockNumberFormatter: [Function: inputBlockNumberFormatter],</span></span><br><span class="line"><span class="string">        inputCallFormatter: [Function: inputCallFormatter],</span></span><br><span class="line"><span class="string">        inputTransactionFormatter: [Function: inputTransactionFormatter],</span></span><br><span class="line"><span class="string">        inputAddressFormatter: [Function: inputAddressFormatter],</span></span><br><span class="line"><span class="string">        inputPostFormatter: [Function: inputPostFormatter],</span></span><br><span class="line"><span class="string">        inputLogFormatter: [Function: inputLogFormatter],</span></span><br><span class="line"><span class="string">        inputSignFormatter: [Function: inputSignFormatter],</span></span><br><span class="line"><span class="string">        outputBigNumberFormatter: [Function: outputBigNumberFormatter],</span></span><br><span class="line"><span class="string">        outputTransactionFormatter: [Function: outputTransactionFormatter],</span></span><br><span class="line"><span class="string">        outputTransactionReceiptFormatter: [Function: outputTransactionReceiptFormatter],</span></span><br><span class="line"><span class="string">        outputBlockFormatter: [Function: outputBlockFormatter],</span></span><br><span class="line"><span class="string">        outputLogFormatter: [Function: outputLogFormatter],</span></span><br><span class="line"><span class="string">        outputPostFormatter: [Function: outputPostFormatter],</span></span><br><span class="line"><span class="string">        outputSyncingFormatter: [Function: outputSyncingFormatter] &#125;,</span></span><br><span class="line"><span class="string">     utils:</span></span><br><span class="line"><span class="string">      &#123; _fireError: [Function: _fireError],</span></span><br><span class="line"><span class="string">        _jsonInterfaceMethodToString: [Function: _jsonInterfaceMethodToString],</span></span><br><span class="line"><span class="string">        _flattenTypes: [Function: _flattenTypes],</span></span><br><span class="line"><span class="string">        randomHex: [Function: randomHex],</span></span><br><span class="line"><span class="string">        _: [Object],</span></span><br><span class="line"><span class="string">        BN: [Object],</span></span><br><span class="line"><span class="string">        isBN: [Function: isBN],</span></span><br><span class="line"><span class="string">        isBigNumber: [Function: isBigNumber],</span></span><br><span class="line"><span class="string">        isHex: [Function: isHex],</span></span><br><span class="line"><span class="string">        isHexStrict: [Function: isHexStrict],</span></span><br><span class="line"><span class="string">        sha3: [Object],</span></span><br><span class="line"><span class="string">        keccak256: [Object],</span></span><br><span class="line"><span class="string">        soliditySha3: [Function: soliditySha3],</span></span><br><span class="line"><span class="string">        isAddress: [Function: isAddress],</span></span><br><span class="line"><span class="string">        checkAddressChecksum: [Function: checkAddressChecksum],</span></span><br><span class="line"><span class="string">        toChecksumAddress: [Function: toChecksumAddress],</span></span><br><span class="line"><span class="string">        toHex: [Function: toHex],</span></span><br><span class="line"><span class="string">        toBN: [Function: toBN],</span></span><br><span class="line"><span class="string">        bytesToHex: [Function: bytesToHex],</span></span><br><span class="line"><span class="string">        hexToBytes: [Function: hexToBytes],</span></span><br><span class="line"><span class="string">        hexToNumberString: [Function: hexToNumberString],</span></span><br><span class="line"><span class="string">        hexToNumber: [Function: hexToNumber],</span></span><br><span class="line"><span class="string">        toDecimal: [Function: hexToNumber],</span></span><br><span class="line"><span class="string">        numberToHex: [Function: numberToHex],</span></span><br><span class="line"><span class="string">        fromDecimal: [Function: numberToHex],</span></span><br><span class="line"><span class="string">        hexToUtf8: [Function: hexToUtf8],</span></span><br><span class="line"><span class="string">        hexToString: [Function: hexToUtf8],</span></span><br><span class="line"><span class="string">        toUtf8: [Function: hexToUtf8],</span></span><br><span class="line"><span class="string">        utf8ToHex: [Function: utf8ToHex],</span></span><br><span class="line"><span class="string">        stringToHex: [Function: utf8ToHex],</span></span><br><span class="line"><span class="string">        fromUtf8: [Function: utf8ToHex],</span></span><br><span class="line"><span class="string">        hexToAscii: [Function: hexToAscii],</span></span><br><span class="line"><span class="string">        toAscii: [Function: hexToAscii],</span></span><br><span class="line"><span class="string">        asciiToHex: [Function: asciiToHex],</span></span><br><span class="line"><span class="string">        fromAscii: [Function: asciiToHex],</span></span><br><span class="line"><span class="string">        unitMap: [Object],</span></span><br><span class="line"><span class="string">        toWei: [Function: toWei],</span></span><br><span class="line"><span class="string">        fromWei: [Function: fromWei],</span></span><br><span class="line"><span class="string">        padLeft: [Function: leftPad],</span></span><br><span class="line"><span class="string">        leftPad: [Function: leftPad],</span></span><br><span class="line"><span class="string">        padRight: [Function: rightPad],</span></span><br><span class="line"><span class="string">        rightPad: [Function: rightPad],</span></span><br><span class="line"><span class="string">        toTwosComplement: [Function: toTwosComplement] &#125;,</span></span><br><span class="line"><span class="string">     Method: [Function: Method] &#125;,</span></span><br><span class="line"><span class="string">  clearSubscriptions: [Function],</span></span><br><span class="line"><span class="string">  options:</span></span><br><span class="line"><span class="string">   &#123; address: [Getter/Setter],</span></span><br><span class="line"><span class="string">     jsonInterface: [Getter/Setter],</span></span><br><span class="line"><span class="string">     data: undefined,</span></span><br><span class="line"><span class="string">     from: undefined,</span></span><br><span class="line"><span class="string">     gasPrice: undefined,</span></span><br><span class="line"><span class="string">     gas: undefined &#125;,</span></span><br><span class="line"><span class="string">  defaultAccount: [Getter/Setter],</span></span><br><span class="line"><span class="string">  defaultBlock: [Getter/Setter],</span></span><br><span class="line"><span class="string">  # methods you define in the smart contract</span></span><br><span class="line"><span class="string">  methods:</span></span><br><span class="line"><span class="string">   &#123; setMessage: [Function: bound _createTxObject],</span></span><br><span class="line"><span class="string">     '</span>0x368b8772<span class="string">': [Function: bound _createTxObject],</span></span><br><span class="line"><span class="string">     '</span>setMessage(string)<span class="string">': [Function: bound _createTxObject],</span></span><br><span class="line"><span class="string">     message: [Function: bound _createTxObject],</span></span><br><span class="line"><span class="string">     '</span>0xe21f37ce<span class="string">': [Function: bound _createTxObject],</span></span><br><span class="line"><span class="string">     '</span>message()<span class="string">': [Function: bound _createTxObject] &#125;,</span></span><br><span class="line"><span class="string">  events: &#123; allEvents: [Function: bound ] &#125;,</span></span><br><span class="line"><span class="string">  _address: '</span>0x2E4631F207D3564B7937Bf355d947c187D9a1Fad<span class="string">',</span></span><br><span class="line"><span class="string">  _jsonInterface:</span></span><br><span class="line"><span class="string">   [ &#123; constant: false,</span></span><br><span class="line"><span class="string">       inputs: [Array],</span></span><br><span class="line"><span class="string">       name: '</span>setMessage<span class="string">',</span></span><br><span class="line"><span class="string">       outputs: [],</span></span><br><span class="line"><span class="string">       payable: false,</span></span><br><span class="line"><span class="string">       stateMutability: '</span>nonpayable<span class="string">',</span></span><br><span class="line"><span class="string">       type: '</span><span class="keyword">function</span><span class="string">',</span></span><br><span class="line"><span class="string">       signature: '</span>0x368b8772<span class="string">' &#125;,</span></span><br><span class="line"><span class="string">     &#123; constant: true,</span></span><br><span class="line"><span class="string">       inputs: [],</span></span><br><span class="line"><span class="string">       name: '</span>message<span class="string">',</span></span><br><span class="line"><span class="string">       outputs: [Array],</span></span><br><span class="line"><span class="string">       payable: false,</span></span><br><span class="line"><span class="string">       stateMutability: '</span>view<span class="string">',</span></span><br><span class="line"><span class="string">       type: '</span><span class="keyword">function</span><span class="string">',</span></span><br><span class="line"><span class="string">       signature: '</span>0xe21f37ce<span class="string">' &#125;,</span></span><br><span class="line"><span class="string">     &#123; inputs: [Array],</span></span><br><span class="line"><span class="string">       payable: false,</span></span><br><span class="line"><span class="string">       stateMutability: '</span>nonpayable<span class="string">',</span></span><br><span class="line"><span class="string">       type: '</span>constructor<span class="string">',</span></span><br><span class="line"><span class="string">       constant: undefined,</span></span><br><span class="line"><span class="string">       signature: '</span>constructor<span class="string">' &#125; ] &#125;</span></span><br><span class="line"><span class="string">    ✓ deploy a contract</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  1 passing (225ms)</span></span><br></pre></td></tr></table></figure>

<h1 id="Deploy-to-ethereum"><a href="#Deploy-to-ethereum" class="headerlink" title="Deploy to ethereum"></a>Deploy to ethereum</h1><p><img src="deploy.png" alt="deploy"></p>
<ul>
<li><code>Infura API</code>: provide the interface of real ethereum block chain.</li>
<li><code>Provider</code>: a interface layer to communicate with real block chain using your account.</li>
</ul>
<h2 id="Get-infura-api"><a href="#Get-infura-api" class="headerlink" title="Get infura api"></a>Get infura api</h2><p>Go to infura.io to creata a new account and project. </p>
<p><img src="infura.png" alt=""></p>
<h2 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h2><figure class="highlight js"><figcaption><span>:deploy.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> HDWalletProvider = <span class="built_in">require</span>(<span class="string">'truffle-hdwallet-provider'</span>);</span><br><span class="line"><span class="keyword">const</span> Web3 = <span class="built_in">require</span>(<span class="string">'web3'</span>);</span><br><span class="line"><span class="keyword">const</span> &#123;interface, bytecode&#125; = <span class="built_in">require</span>(<span class="string">'./compile'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> provider = <span class="keyword">new</span> HDWalletProvider (</span><br><span class="line">    <span class="string">'dinosaur erupt zoo ...'</span>, <span class="comment">// your account</span></span><br><span class="line">    <span class="string">'https://rinkeby.infura.io/v3/451daf892abb41***'</span> <span class="comment">// block chain api, you can obtain this from infura.io website inside your project.</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> web3 = <span class="keyword">new</span> Web3(provider);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> deploy = <span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">    <span class="keyword">const</span> accounts = <span class="keyword">await</span> web3.eth.getAccounts();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'Attempting to deploy from account'</span>, accounts[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> result = <span class="keyword">await</span> <span class="keyword">new</span> web3.eth.Contract(<span class="built_in">JSON</span>.parse(interface))</span><br><span class="line">    .deploy(&#123;<span class="attr">data</span>: bytecode, <span class="attr">arguments</span>: [<span class="string">'Hi there!'</span>]&#125;)</span><br><span class="line">    .send(&#123;<span class="attr">gas</span>: <span class="string">'5000000'</span>, <span class="attr">from</span>: accounts[<span class="number">0</span>]&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'Contract deployed to'</span>, result.options.address);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">deploy();</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code/blockchain  node deploy.js</span><br><span class="line">Attempting to deploy from account 0x01C65bfDeD8c69ef3C28d4EF58F1dA46DeAF13Cd</span><br><span class="line">Contract deployed to 0xF0f127B8eC22da8e811262B287BA6b8244B22891</span><br></pre></td></tr></table></figure>

<p>copy <code>0xF0f127B8eC22da8e811262B287BA6b8244B22891</code> to <code>rinkey.etherscan.io</code> to see the results.</p>
<p><img src="rinkeby.png" alt="rinkeby"></p>
<h1 id="Interact-with-real-ethereum"><a href="#Interact-with-real-ethereum" class="headerlink" title="Interact with real ethereum"></a>Interact with real ethereum</h1><p>Go to remix website</p>
<p>copy <code>0xF0f127B8eC22da8e811262B287BA6b8244B22891</code> to <code>At Address</code>, and click.</p>
<p>You will see the contract deployed in real ethereum network.</p>
<p><img src="remix.png" alt=""></p>
<p>You can then try to setMessage, for example: ‘Test real ethereum’.</p>
<p>After your submition, the transaction will be in <code>pending</code> state.</p>
<p><img src="pending.png" alt="pending"></p>
<p>While in the pending state, the value of message will not change. After the transaction is confirmed, <code>message</code> function will return the new value: ‘Test real ethereum’.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://stackoverflow.com/questions/50201353/unhandledpromiserejectionwarning-error-the-contract-code-couldnt-be-stored-p" target="_blank" rel="noopener">Error: The contract code couldn’t be stored, please check your gas limit</a></li>
<li><a href="https://rinkeby.etherscan.io/" target="_blank" rel="noopener">rinkey.etherscan.io</a></li>
</ul>
]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>ethereum</tag>
        <tag>web3</tag>
      </tags>
  </entry>
  <entry>
    <title>Review: bLSM:* A General Purpose Log Structured Merge Tree</title>
    <url>/blog/Review-bLSM-%E2%88%97-A-General-Purpose-Log-Structured-Merge-Tree/</url>
    <content><![CDATA[<h1 id="Big-question"><a href="#Big-question" class="headerlink" title="Big question"></a>Big question</h1><p>LSM tree will sacrifice read performance with write performance. This paper, <code>bLSM</code> ( a Log Structured Merge (LSM) tree with the advantages of B-Trees and log structured approaches), claims to have near optimal <code>read</code> and <code>scan</code> performance, and a bounded <code>write latency</code> with <code>spring and gear</code> merge scheduler.</p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>bLSM is designed as a backing storage for Yahoo’s geographically distributed key-value storage system, and Walnut, a elastic cloud storage system.</p>
<p><img src="result.png" alt="summary"></p>
<h1 id="Specific-question"><a href="#Specific-question" class="headerlink" title="Specific question"></a>Specific question</h1><h2 id="Reduce-read-amplification"><a href="#Reduce-read-amplification" class="headerlink" title="Reduce read amplification"></a>Reduce read amplification</h2><p>It seems like they are using fractal tree structure. But the design seems not clear to me. I haven’t get a clue about how they implement the idea.</p>
<h2 id="Write-pause"><a href="#Write-pause" class="headerlink" title="Write pause"></a>Write pause</h2><p><code>level scheduler</code>: A merge scheduler.</p>
<p>The explanation in the paper is not clear to me. Hard to understand.</p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>LSM tree</tag>
        <tag>kv</tag>
      </tags>
  </entry>
  <entry>
    <title>Review: ElasticBF: Fine-grained and Elastic Bloom Filter Towards Efficient Read for LSM-tree-based KV Stores</title>
    <url>/blog/Review-ElasticBF-Fine-grained-and-Elastic-Bloom-Filter-Towards-Efficient-Read-for-LSM-tree-based-KV-Stores/</url>
    <content><![CDATA[<h1 id="Big-question"><a href="#Big-question" class="headerlink" title="Big question"></a>Big question</h1><p>Current leveldb implementation uses uniform setting for all Bloom filters for hot and cold SSTable, which is not efficient to reduce unnecessary I/O.</p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>In leveldb, in order to reduce unnecessary I/O for non-exist data, bloom filter about SSTables are cached in the memory. When user search for a key, the key is first checked in related bloom filter. Then if bloom filter returns true, that SSTable is fetched from disk (one I/O) and leveldb uses binary search to locate that key.</p>
<p>Since bloom filter has a parameter, called <code>false positive rate (FPR)</code>. It may tell a lie when the key is not exist in SSTable. This will cause unnecessary I/O. In order to reduce those I/O, we must reduce <code>FPR</code> and use more memory.</p>
<p>$$ P = \left( 1 - e ^ { - k n / m } \right) ^ { k } $$</p>
<a id="more"></a>
<ul>
<li>k: number of function</li>
<li>n: number of inserted key</li>
<li>m: number of bit</li>
</ul>
<p>Assuming we uses 10 bit per key, and the function number k is 3, then the FPR will be 1.74%. </p>
<h1 id="Specific-questions"><a href="#Specific-questions" class="headerlink" title="Specific questions"></a>Specific questions</h1><p><img src="frequency.png" alt="frequency"></p>
<p>Because SSTable access pattern is not uniform. Using uniform size of bloom filter is not efficient. The basic idea is allocate more memory to hot data and less memory to cold data.</p>
<h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p><img src="solution.png" alt="solution"></p>
<p>The solution is building multiple filters when constructing each SSTable, and each filter is usually allocated with smaller memory size, which is called filter unit. Hot SSTable will cache more filter unit in memory.</p>
<h2 id="A-cost-function"><a href="#A-cost-function" class="headerlink" title="A cost function"></a>A cost function</h2><p>$$ E [Extra IO ] = \sum _ { i = 1 } ^ { n } f _ { i } \cdot f p _ { i } $$</p>
<ul>
<li>n: number of SSTable</li>
<li>fi: access frequency of SSTable i</li>
<li>fpi: false positive rate of SSTable i</li>
</ul>
<p>ElasticBF adjusts the number of filter units for each SSTable only when the metric E[Extra IO] could be reduced under the fixed memory usage.</p>
<h2 id="Multi-Queue-design"><a href="#Multi-Queue-design" class="headerlink" title="Multi Queue design"></a>Multi Queue design</h2><blockquote>
<p>The adjustment of Bloom filter proceeds as follows. Each time when a SSTable is accessed, we first increase its access frequency by one and update E[Extra IO], then we check whether E[Extra IO] could be decreased if enabling one more filter unit for this SSTable and disabling some filter units in other SSTables so as to guarantee the same memory usage. However, one critical issue is how to quickly find which filters should be disabled but not incurring a large overhead</p>
</blockquote>
<p><img src="computation.png" alt="compuation"></p>
<p>This paper maintained a LRU queue. Each element of a queue corresponds to one SSTable and keeps the metadata of the SSTable, including the enabled filter units residing in memory. Qi manages the SSTables which already enabled exactly i filter units, e.g., each SSTable in Q2 enabled two filter units.</p>
<p>Each metadata will have a <code>currentTime + lifeTime</code> counter as <code>expiredTime</code>.</p>
<ul>
<li>currentTime: number of total Get requests</li>
<li>lifeTime: when will this SSTable expire, the life interval</li>
</ul>
<p>A read will cause the metadata been updated (update currentTime).</p>
<div class="note danger"><p>I guess those multi level queue is priority queue, sorted by the the expireTime. Then this means every access will first locate the metadata, then reorder the queue.</p>
<p>How to located the metadata, brute force? How about the reorder overhead?</p></div>

<h1 id="My-comments"><a href="#My-comments" class="headerlink" title="My comments"></a>My comments</h1><p>This Elastic idea is not the first I head. In SIGMOD17,  a paper: Monkey, discussed how to use different size of bloom filter for SSTable in different level in LevelDB. Their idea is allocate more memory to higher lever SSTable and less memory to lower level SSTable.</p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>LSM tree</tag>
        <tag>kv</tag>
      </tags>
  </entry>
  <entry>
    <title>Review: Mutant: Balancing Storage Cost and Latency in LSM Tree Data Stores</title>
    <url>/blog/Review-Mutant-Balancing-Storage-Cost-and-Latency-in-LSM-Tree-Data-Stores/</url>
    <content><![CDATA[<h1 id="Big-question"><a href="#Big-question" class="headerlink" title="Big question"></a>Big question</h1><p>Cloud databases should support dynamic allocation of hot and cold data to fast and slow device. So the client can get the best performance within their budget.</p>
<p>The author introduced a system that support dynamic allocation of storage system based on LSM-tree.</p>
<a id="more"></a>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Locality-in-SSTable"><a href="#Locality-in-SSTable" class="headerlink" title="Locality in SSTable"></a>Locality in SSTable</h2><p><img src="frequency.png" alt="frequency"></p>
<p>There are some locality in SSTable access. (This paper uses QuizUp workload to do the analysis)</p>
<ul>
<li><code>Figure 3</code> indicates that younger data will have more access while older data have less access.</li>
<li><code>Figure 4</code> illustrates that the hottest data can be 4 orders of magnitudes than the coldest data.</li>
<li><code>Figure 5</code> says that As more records are inserted over time, the number of infrequently accessed SSTables (“cold” SSTables) increases, while the number of frequently accessed SSTables (“hot” SSTables) stays about the same.</li>
</ul>
<h2 id="Locality-in-SSTable-components"><a href="#Locality-in-SSTable-components" class="headerlink" title="Locality in SSTable components"></a>Locality in SSTable components</h2><p>Components consist of <code>metadata</code>(including <code>bloomfilter</code>, <code>record index</code>) and <code>data records</code>.</p>
<h1 id="Specific-questions"><a href="#Specific-questions" class="headerlink" title="Specific questions"></a>Specific questions</h1><h2 id="Get-the-optimal-SSTable-to-be-stored-in-fast-storage"><a href="#Get-the-optimal-SSTable-to-be-stored-in-fast-storage" class="headerlink" title="Get the optimal SSTable to be stored in fast storage"></a>Get the optimal SSTable to be stored in fast storage</h2><p>With those pattern, this paper try to solve this specific question:</p>
<blockquote>
<p>Find a subset of SSTables to be stored in the fast storage (optimization goal) such that the sum of fast storage SSTable accesses is maximized, (constraint) while bounding the volume of SSTables in fast storage.</p>
</blockquote>
<p>They use this model to do some optimization.</p>
<p>$$<br>P _ { f } S _ { f } + P _ { s } S _ { s } \leq C _ { \max }<br>$$</p>
<p>$$<br>S _ { f } + S _ { s } = S<br>$$</p>
<ul>
<li>Pf, Ps: unit price for fast and slow device</li>
<li>Sf, Ss: sum of all SSTable sizes in the fast and slow storage</li>
</ul>
<p>$$<br>S _ { f } &lt; \frac { C _ { \max } - P _ { s } S } { P _ { f } - P _ { s } } = S _ { f , \max }$$</p>
<p>maximize<br>$$\sum _ { i \in SSTables} A _ { i } x _ { i }$$<br>subject to<br>$$\sum _ { i \in SSTables} S _ { i } x _ { i } \leq S _ { f , max }  and  x _ { i } \in { 0,1 } $$</p>
<ul>
<li>Ai: number of accesses to SSTable i</li>
<li>Si: the size of SSTable i</li>
<li>Xi: SSTable in fast storage or not</li>
</ul>
<p>This becomes a 0/1 knapsack problem. They use a greedy algorithm to solve this problem.</p>
<div class="note danger"><p>My comment:<br>Why not just sorting by the frequency, then put as many as SSTable to fast device directly.</p></div>

<h2 id="Solving-spike-or-dips"><a href="#Solving-spike-or-dips" class="headerlink" title="Solving spike or dips"></a>Solving spike or dips</h2><p>They use a counter to record the access frequency, but they can be affected severely by spike or dips. So they implement a filter through exponential average to smooth the frequency counter.</p>
<div class="note danger"><p>My comment:<br>This is like a digital signal filer. Transform function.</p></div>
<!-- 
# Approach

# Results

Write one or more paragraphs to summarize the results for each experiment, each figure, and each table -->

<h1 id="My-thoughts"><a href="#My-thoughts" class="headerlink" title="My thoughts"></a>My thoughts</h1><p>The thing I don’t understand is that why memory caching is not enough for those hot data. Maybe one possible explanation is that even hot data is too big to be stored in memory.</p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>LSM tree</tag>
        <tag>kv</tag>
      </tags>
  </entry>
  <entry>
    <title>first docker app</title>
    <url>/blog/first-docker-app/</url>
    <content><![CDATA[<h1 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h1><p>Create a new directory where all the files would live.</p>
<a id="more"></a>
<h2 id="Create-Node-js-app"><a href="#Create-Node-js-app" class="headerlink" title="Create Node.js app"></a>Create Node.js app</h2><figure class="highlight json"><figcaption><span>:package.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"docker_web_app"</span>,</span><br><span class="line">  <span class="attr">"version"</span>: <span class="string">"1.0.0"</span>,</span><br><span class="line">  <span class="attr">"description"</span>: <span class="string">"Node.js on Docker"</span>,</span><br><span class="line">  <span class="attr">"author"</span>: <span class="string">"First Last &lt;first.last@example.com&gt;"</span>,</span><br><span class="line">  <span class="attr">"main"</span>: <span class="string">"server.js"</span>,</span><br><span class="line">  <span class="attr">"scripts"</span>: &#123;</span><br><span class="line">    <span class="attr">"start"</span>: <span class="string">"node server.js"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="attr">"express"</span>: <span class="string">"^4.16.1"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<!-- more -->

<p>With your new <code>package.json</code> file, run <code>npm install</code>. If you are using npm version 5 or later, this will generate a <code>package-lock.json</code> file which will be copied to your Docker image.</p>
<p>Then, create a <code>server.js</code> file that defines a web app using the Express.js framework:</p>
<figure class="highlight js"><figcaption><span>:server.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">'use strict'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> express = <span class="built_in">require</span>(<span class="string">'express'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Constants</span></span><br><span class="line"><span class="keyword">const</span> PORT = <span class="number">8080</span>;</span><br><span class="line"><span class="keyword">const</span> HOST = <span class="string">'0.0.0.0'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// App</span></span><br><span class="line"><span class="keyword">const</span> app = express();</span><br><span class="line">app.get(<span class="string">'/'</span>, (req, res) =&gt; &#123;</span><br><span class="line">  res.send(<span class="string">'Hello world\n'</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">app.listen(PORT, HOST);</span><br><span class="line"><span class="built_in">console</span>.log(<span class="string">`Running on http://<span class="subst">$&#123;HOST&#125;</span>:<span class="subst">$&#123;PORT&#125;</span>`</span>);</span><br></pre></td></tr></table></figure>

<h2 id="Creating-a-Dockerfile"><a href="#Creating-a-Dockerfile" class="headerlink" title="Creating a Dockerfile"></a>Creating a Dockerfile</h2><figure class="highlight bash"><figcaption><span>:Dockerfile</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># use the latest LTS (long term support) version 8 of node available from the Docker Hub:</span></span><br><span class="line">FROM node:8</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create app directory</span></span><br><span class="line">WORKDIR /usr/src/app</span><br><span class="line"></span><br><span class="line"><span class="comment"># This image comes with Node.js and NPM already installed so the next thing we need to do </span></span><br><span class="line"><span class="comment"># is to install your app dependencies using the npm binary.</span></span><br><span class="line"><span class="comment"># Install app dependencies</span></span><br><span class="line"><span class="comment"># A wildcard is used to ensure both package.json AND package-lock.json are copied</span></span><br><span class="line"><span class="comment"># where available (npm@5+)</span></span><br><span class="line">COPY package*.json ./</span><br><span class="line"></span><br><span class="line">RUN npm install</span><br><span class="line"><span class="comment"># If you are building your code for production</span></span><br><span class="line"><span class="comment"># RUN npm install --only=production</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To bundle your app's source code inside the Docker image, use the COPY instruction:</span></span><br><span class="line"><span class="comment"># Bundle app source</span></span><br><span class="line">COPY . .</span><br><span class="line"></span><br><span class="line"><span class="comment"># Your app binds to port 8080 so you'll use the EXPOSE instruction to have it mapped </span></span><br><span class="line"><span class="comment"># by the docker daemon:</span></span><br><span class="line">EXPOSE 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># Last but not least, define the command to run your app using CMD which defines your runtime. # Here we will use the basic npm start which will run node server.js to start your server:</span></span><br><span class="line">CMD [ <span class="string">"npm"</span>, <span class="string">"start"</span> ]</span><br></pre></td></tr></table></figure>

<p>Create a <code>.dockerignore</code> file in the same directory as your <code>Dockerfile</code> with following content:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">node_modules</span><br><span class="line">npm-debug.log</span><br></pre></td></tr></table></figure>

<p>This will prevent your local modules and debug logs from being copied onto your Docker image and possibly overwriting modules installed within your image.</p>
<h1 id="Building-your-image"><a href="#Building-your-image" class="headerlink" title="Building your image"></a>Building your image</h1><p>Go to the directory that has your Dockerfile and run the following command to build the Docker image. The <code>-t</code> flag lets you tag your image so it’s easier to find later using the <code>docker images</code> command:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker build -t &lt;your username&gt;/node-web-app .</span><br></pre></td></tr></table></figure>

<p>Your image will now be listed by Docker:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code/docker/test1  docker images</span><br><span class="line">REPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">hansonzhao007/node-web-app   latest              d9a6394dc754        21 minutes ago      675MB</span><br><span class="line">node                         8                   8198006b2b57        11 hours ago        673MB</span><br><span class="line">hello-world                  latest              2cb0d9787c4d        8 weeks ago         1.85kB</span><br></pre></td></tr></table></figure>

<h1 id="How-docker-utilize-layer-cache"><a href="#How-docker-utilize-layer-cache" class="headerlink" title="How docker utilize layer cache"></a>How docker utilize layer cache</h1><p>The first time we build docker image:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code/docker/test1  docker build -t hansonzhao007/node-web-app .</span><br><span class="line">Sending build context to Docker daemon  18.94kB</span><br><span class="line">Step 1/7 : FROM node:8</span><br><span class="line">8: Pulling from library/node</span><br><span class="line">f189db1b88b3: Pull complete</span><br><span class="line">3d06cf2f1b5e: Pull complete</span><br><span class="line">687ebdda822c: Pull complete</span><br><span class="line">99119ca3f34e: Pull complete</span><br><span class="line">e771d6006054: Pull complete</span><br><span class="line">b0cc28d0be2c: Pull complete</span><br><span class="line">9bbe77ca0944: Pull complete</span><br><span class="line">75f7d70e2d07: Pull complete</span><br><span class="line">Digest: sha256:47a2131abc86d41faa910465b35987bc06b014c335309b551c876e517b5a4402</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> node:8</span><br><span class="line"> ---&gt; 8198006b2b57</span><br><span class="line">Step 2/7 : WORKDIR /usr/src/app</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 4137ea639ed8</span><br><span class="line">Removing intermediate container 4137ea639ed8</span><br><span class="line"> ---&gt; 549d3e18b855</span><br><span class="line">Step 3/7 : COPY package*.json ./</span><br><span class="line"> ---&gt; f290910d8f59</span><br><span class="line">Step 4/7 : RUN npm install</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> eb632572e423</span><br><span class="line">npm WARN docker_web_app@1.0.0 No repository field.</span><br><span class="line">npm WARN docker_web_app@1.0.0 No license field.</span><br><span class="line"></span><br><span class="line">added 50 packages <span class="keyword">in</span> 1.753s</span><br><span class="line">Removing intermediate container eb632572e423</span><br><span class="line"> ---&gt; 17748bcde097</span><br><span class="line">Step 5/7 : COPY . .</span><br><span class="line"> ---&gt; fdb83d4c8598</span><br><span class="line">Step 6/7 : EXPOSE 8080</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 69d1ada16879</span><br><span class="line">Removing intermediate container 69d1ada16879</span><br><span class="line"> ---&gt; 87924dc046d8</span><br><span class="line">Step 7/7 : CMD [ <span class="string">"npm"</span>, <span class="string">"start"</span> ]</span><br><span class="line"> ---&gt; Running <span class="keyword">in</span> 90c7d806a20b</span><br><span class="line">Removing intermediate container 90c7d806a20b</span><br><span class="line"> ---&gt; d9a6394dc754</span><br><span class="line">Successfully built d9a6394dc754</span><br><span class="line">Successfully tagged hansonzhao007/node-web-app:latest</span><br></pre></td></tr></table></figure>

<p>We see that every layer need to be rebuilt.</p>
<p>Then we try to rebuild it again:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code/docker/test1  docker build -t hansonzhao007/node-web-app .</span><br><span class="line">Sending build context to Docker daemon  18.94kB</span><br><span class="line">Step 1/7 : FROM node:8</span><br><span class="line"> ---&gt; 8198006b2b57</span><br><span class="line">Step 2/7 : WORKDIR /usr/src/app</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 549d3e18b855</span><br><span class="line">Step 3/7 : COPY package*.json ./</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; d124378627e9</span><br><span class="line">Step 4/7 : RUN npm install</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 2ce3c2c8249a</span><br><span class="line">Step 5/7 : COPY . .</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; ffca5856eb53</span><br><span class="line">Step 6/7 : EXPOSE 8080</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; bdcc9c0cc503</span><br><span class="line">Step 7/7 : CMD [ <span class="string">"npm"</span>, <span class="string">"start"</span> ]</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; d93edfeb4525</span><br><span class="line">Successfully built d93edfeb4525</span><br><span class="line">Successfully tagged hansonzhao007/node-web-app:latest</span><br></pre></td></tr></table></figure>

<p>We can see that for every CMD, there is a <code>Using cache</code>.</p>
<h2 id="What-are-the-layers"><a href="#What-are-the-layers" class="headerlink" title="What are the layers"></a>What are the layers</h2><p>Docker containers are building blocks for applications. Each container is an image with a readable/writeable layer on top of a bunch of read-only layers.</p>
<p>These layers (also called intermediate images) are generated when the commands in the Dockerfile are executed during the Docker image build.</p>
<p>When Docker builds the container from a Dockerfile, each step corresponds to a command run in the Dockerfile. And each layer is made up of the file generated from running that command.</p>
<p>For example, the layer ID for step 2 is <code>549d3e18b855</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@HansonMac  ~/Code/docker/test1  docker build -t hansonzhao007/node-web-app .</span><br><span class="line">Sending build context to Docker daemon  18.94kB</span><br><span class="line">Step 1/7 : FROM node:8</span><br><span class="line"> ---&gt; 8198006b2b57</span><br><span class="line">Step 2/7 : WORKDIR /usr/src/app</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 549d3e18b855</span><br><span class="line">Step 3/7 : COPY package*.json ./</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Once the image is built, you can view all the layers that make up the image with the docker history command. The “Image” column (i.e <code>intermediate image</code> or <code>layer</code>) shows the randomly generated UUID that correlates to that layer.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> mac@HansonMac  ~/Code/docker/test1  docker <span class="built_in">history</span> hansonzhao007/node-web-app</span><br><span class="line">IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT</span><br><span class="line">d93edfeb4525        7 minutes ago       /bin/sh -c <span class="comment">#(nop)  CMD ["npm" "start"]          0B</span></span><br><span class="line">bdcc9c0cc503        7 minutes ago       /bin/sh -c <span class="comment">#(nop)  EXPOSE 8080                  0B</span></span><br><span class="line">ffca5856eb53        7 minutes ago       /bin/sh -c <span class="comment">#(nop) COPY dir:61b10d9a51757f95e…   14kB</span></span><br><span class="line">2ce3c2c8249a        7 minutes ago       /bin/sh -c npm install                          2.91MB</span><br><span class="line">d124378627e9        7 minutes ago       /bin/sh -c <span class="comment">#(nop) COPY multi:6b51dc5ec4af055…   13.3kB</span></span><br><span class="line">549d3e18b855        31 minutes ago      /bin/sh -c <span class="comment">#(nop) WORKDIR /usr/src/app          0B</span></span><br><span class="line">8198006b2b57        11 hours ago        /bin/sh -c <span class="comment">#(nop)  CMD ["node"]                 0B</span></span><br></pre></td></tr></table></figure>

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://medium.com/@jessgreb01/digging-into-docker-layers-c22f948ed612" target="_blank" rel="noopener">Digging into Docker layers</a><br><a href="https://nodejs.org/en/docs/guides/nodejs-docker-webapp/" target="_blank" rel="noopener">Dockerizing a Node.js web app</a><br><a href="http://bitjudo.com/blog/2014/03/13/building-efficient-dockerfiles-node-dot-js/" target="_blank" rel="noopener">Building Efficient Dockerfiles - Node.js</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title>MPI configuration</title>
    <url>/blog/MPI-configuration/</url>
    <content><![CDATA[<h1 id="Install-MPI-in-each-node-server"><a href="#Install-MPI-in-each-node-server" class="headerlink" title="Install MPI in each node server"></a>Install MPI in each node server</h1><p><a href="http://mpitutorial.com/tutorials/installing-mpich2/" target="_blank" rel="noopener">http://mpitutorial.com/tutorials/installing-mpich2/</a></p>
<h1 id="Run-hello-world"><a href="#Run-hello-world" class="headerlink" title="Run hello world."></a>Run hello world.</h1><p>这里只是简单的把编译程序复制到用户的 home 目录下，然后执行。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># replace hello.c with your own source code file name</span></span><br><span class="line">filename=hello.c</span><br><span class="line">output=hello</span><br><span class="line"></span><br><span class="line">master=10.0.0.10</span><br><span class="line">slave1=10.0.0.15</span><br><span class="line">slave2=10.0.0.16</span><br><span class="line">slave3=10.0.0.18</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile the source code</span></span><br><span class="line">mpicc -o <span class="variable">$output</span> <span class="variable">$filename</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># copy the executable file to other slaves</span></span><br><span class="line">scp <span class="variable">$output</span> <span class="variable">$USER</span>@<span class="variable">$slave1</span>:~/ </span><br><span class="line">scp <span class="variable">$output</span> <span class="variable">$USER</span>@<span class="variable">$slave2</span>:~/ </span><br><span class="line">scp <span class="variable">$output</span> <span class="variable">$USER</span>@<span class="variable">$slave3</span>:~/ </span><br><span class="line"></span><br><span class="line"><span class="comment"># deploy the program to 4 nodes</span></span><br><span class="line">mpirun -n 4 -H <span class="variable">$master</span>,<span class="variable">$slave1</span>,<span class="variable">$slave2</span>,<span class="variable">$slave3</span> ./<span class="variable">$output</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Author: Wes Kendall</span></span><br><span class="line"><span class="comment">// Copyright 2011 www.mpitutorial.com</span></span><br><span class="line"><span class="comment">// This code is provided freely with the tutorials on mpitutorial.com. Feel</span></span><br><span class="line"><span class="comment">// free to modify it for your own use. Any distribution of the code must</span></span><br><span class="line"><span class="comment">// either provide a link to www.mpitutorial.com or keep this header intact.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// An intro MPI hello world program that uses MPI_Init, MPI_Comm_size,</span></span><br><span class="line"><span class="comment">// MPI_Comm_rank, MPI_Finalize, and MPI_Get_processor_name.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Initialize the MPI environment. The two arguments to MPI Init are not</span></span><br><span class="line">  <span class="comment">// currently used by MPI implementations, but are there in case future</span></span><br><span class="line">  <span class="comment">// implementations might need the arguments.</span></span><br><span class="line">  MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get the number of processes</span></span><br><span class="line">  <span class="keyword">int</span> world_size;</span><br><span class="line">  MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get the rank of the process</span></span><br><span class="line">  <span class="keyword">int</span> world_rank;</span><br><span class="line">  MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get the name of the processor</span></span><br><span class="line">  <span class="keyword">char</span> processor_name[MPI_MAX_PROCESSOR_NAME];</span><br><span class="line">  <span class="keyword">int</span> name_len;</span><br><span class="line">  MPI_Get_processor_name(processor_name, &amp;name_len);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Print off a hello world message</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Hello world from processor %s, rank %d out of %d processors\n"</span>,</span><br><span class="line">         processor_name, world_rank, world_size);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Finalize the MPI environment. No more MPI calls can be made after this</span></span><br><span class="line">  MPI_Finalize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="使用-hostfile-文件"><a href="#使用-hostfile-文件" class="headerlink" title="使用 hostfile 文件"></a>使用 hostfile 文件</h1><figure class="highlight bash"><figcaption><span>:hostfile</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># This is an example hostfile.  Comments begin with #</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The following nodes have eight processor machine, we absolutely</span></span><br><span class="line"><span class="comment"># want to disallow over-subscribing it:</span></span><br><span class="line">p10 slots=8 max-slots=8</span><br><span class="line">p15 slots=8 max-slots=8</span><br><span class="line">p16 slots=8 max-slots=8</span><br><span class="line">p18 slots=8 max-slots=8</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mpirun -np 20 --hostfile hostfile ./hello</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title>Review: SuRF: Practical Range Query Filtering with Fast Succinct Tries</title>
    <url>/blog/Review-SuRF-Practical-Range-Query-Filtering-with-Fast-Succinct-Tries/</url>
    <content><![CDATA[<h1 id="Succinct-Data-Structure"><a href="#Succinct-Data-Structure" class="headerlink" title="Succinct Data Structure"></a>Succinct Data Structure</h1><p>Succinct: expressed clearly and in a few words. 中文意思就是简洁明了的。首先看一下 wiki 引用对于 succinct data structure 的描述：</p>
<blockquote>
<p>In computer science, a succinct data structure is a data structure which uses an amount of space that is “close” to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations.</p>
</blockquote>
<p>Suppose that Z is the information-theoretical optimal number of bits needed to store some data. A representation of this data is called:</p>
<ul>
<li><code>implicit</code>: if it takes Z+O(1) bits of space,</li>
<li><code>succinct</code>: if it takes Z+o(Z) bits of space,</li>
<li><code>compact</code>: if it takes O(Z) bits of space.</li>
</ul>
<p>For example, a data structure that uses 2Z bits of storage is compact, Z + \sqrt{Z} bits is succinct,  Z+lgZ bits is also succinct, and Z+3 bits is implicit.</p>
<a id="more"></a>

<h1 id="Succinct-Tree"><a href="#Succinct-Tree" class="headerlink" title="Succinct Tree"></a>Succinct Tree</h1><p>Goal: represent the data in close to optimal space, while supporting the operations efficiently.</p>
<p>这里就要谈到 <code>Jacobson, FOCS ‘89</code> 提出来的方法。</p>
<h2 id="Heap-like-notation-for-a-binary-tree"><a href="#Heap-like-notation-for-a-binary-tree" class="headerlink" title="Heap-like notation for a binary tree"></a>Heap-like notation for a binary tree</h2><h3 id="结构扩展"><a href="#结构扩展" class="headerlink" title="结构扩展"></a>结构扩展</h3><p>首先看二叉树的示例：<br><img src="1.png" alt="1"></p>
<p>对一个二叉树，我们给每一个 leaf node 添加 external node。然后，所有 external node 赋值 0，internal node 赋值 1。按照层序遍历，将所有的值生成一个 <code>bit 序列</code>。我们就使用该序列代表整个树的结构。</p>
<p>那么有了该序列，应该怎样实现树的基本操作呢？诸如：parent, left-children, right-children。</p>
<p><img src="2.png" alt="2"><br>首先给原本的树 internal node 进行编号，编号从 1 开始，层序顺序。这样树的 8 个 node 就被依次编号为 1-8。<br>然后给所有 node，包括 external node 进行编号，编号从 1 开始，层序遍历。这样 17 个 node 就被依次编号为 1 - 17。</p>
<h3 id="rank-amp-select"><a href="#rank-amp-select" class="headerlink" title="rank &amp; select"></a>rank &amp; select</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">R  : <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>   <span class="number">5</span> <span class="number">6</span>   <span class="number">7</span>        <span class="number">8</span>                 <span class="comment">// use r to indicate the index</span></span><br><span class="line"><span class="built_in">bit</span>: <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line">S  : <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span>  <span class="comment">// use s to indicate the index</span></span><br></pre></td></tr></table></figure>

<p>这里需要介绍针对该 bit 序列，所需要的两个基本操作：</p>
<div class="note warning"><p><code>rank(s)</code>: Count the number of element in S less or equal than s.<br><code>select(i)</code>: Find the i-th smallest element in S.</p>
<ol>
<li><code>rank(s)</code>: 其实就是在 S 序列编号范围 [1,s]，bit 序列中 1 的个数。可以看到，rank(6) = 5, rank(9) = 7。这里可以理解为 S[s] 所在的 1 bit，在 R 中对应的编号。rank 是从 <code>S 到 R</code> 的映射。</li>
<li><code>select(i)</code>: 在 S 序列编号下，在 bit 序列中找到第 i 个 1 所在的编号。可以看到，select(5) = 6, select(7) = 9。这里可以理解成为 R[i] 所在的 1 bit，在 S 中对应的编号。select 是 <code>R 到 S</code> 的映射。</li>
</ol></div>

<p>这两个操作是互补的。可以看到 rank(select(5)) = 5. select(rank(9)) = 9。当然这种互补只对 internal node 成立。对于 external node 是不成立的。比如 external node 11：rank(13) = 8, select(8) = 12。</p>
<h3 id="left-right-child-and-parent"><a href="#left-right-child-and-parent" class="headerlink" title="left, right child and parent"></a>left, right child and parent</h3><p>有了 rank 和 select 这两个基本操作，我们就可以在该 <code>bit 序列</code>进行 left child，right child 和 parent 的操作了。</p>
<p>对于 internal node n:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">R  : <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>     <span class="number">5</span>  <span class="number">6</span>     <span class="number">7</span>            <span class="number">8</span>                           <span class="comment">// use r to indicate the index</span></span><br><span class="line"><span class="built_in">bit</span>: <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">0</span>  <span class="number">1</span>   <span class="number">0</span>    <span class="number">0</span>   <span class="number">1</span>   <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">S  : <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> (<span class="number">5</span>) <span class="number">6</span>  <span class="number">7</span> (<span class="number">8</span>) <span class="number">9</span> (<span class="number">10</span>) (<span class="number">11</span>) <span class="number">12</span> (<span class="number">13</span>) (<span class="number">14</span>) (<span class="number">15</span>) (<span class="number">16</span>) (<span class="number">17</span>)  <span class="comment">// use s to indicate the index, () means this number is not belonging to data set S.</span></span><br><span class="line"></span><br><span class="line">假设node是如下结构：</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span> &#123;</span></span><br><span class="line">  <span class="built_in">bit</span> val;</span><br><span class="line">  <span class="keyword">int</span> r; <span class="comment">// value in R</span></span><br><span class="line">  <span class="keyword">int</span> s; <span class="comment">// value in S</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">有如下关系：</span><br><span class="line">  rank(n.s) = n.r</span><br><span class="line">select(n.r) = n.s</span><br><span class="line"></span><br><span class="line">-  left-child(n).s = <span class="number">2</span> * rank(n.s)     = <span class="number">2</span> * n.r     <span class="comment">// 这里看出 node 的 r 编号乘二，得到其 left child 的 s 编号</span></span><br><span class="line">- right-child(n).s = <span class="number">2</span> * rank(n.s) + <span class="number">1</span> = <span class="number">2</span> * n.r + <span class="number">1</span> <span class="comment">// 这里看出 node 的 r 编号乘二加一，得到其 right child 的 s 编号</span></span><br><span class="line">-      parent(n).s = select(n.s / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">parent 可以推出来。对于 left-child，有：</span><br><span class="line">       left-child(n).s      = <span class="number">2</span> * n.r</span><br><span class="line">       left-child(n).s / <span class="number">2</span>  = n.r           <span class="comment">// 这里看出 node 的 s 编号直接除以二，得到其 parent node 的 r 编号</span></span><br><span class="line">select(left-child(n).s / <span class="number">2</span>) = select(n.r)</span><br><span class="line">select(left-child(n).s / <span class="number">2</span>) = n.s</span><br><span class="line">对 right-child 同理。</span><br><span class="line"></span><br><span class="line">简单看来就是:</span><br><span class="line">-  left-node(n).s = S[<span class="number">2</span> * n.r]</span><br><span class="line">- right-node(n).s = S[<span class="number">2</span> * n.r + <span class="number">1</span>]</span><br><span class="line">-     parent(n).r = R[n.s / <span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<div class="note success"><p><strong>Left Child:</strong></p>
<p>internal node 6 (bit 序列中，第 6 个 1 bit 所在 node, R 中编号为 6 的 node)。</p>
<ul>
<li>rank(6.s) = rank(7) = 6</li>
<li>left-node(6).s = 2 * rank(6.s) = 2 * 6 = 12</li>
<li>S 编号中，12 对应的 node 是 R 中编号为 8 的 node。</li>
</ul></div>


<div class="note success"><p><strong>Right Child</strong><br>internal node 4 (bit 序列中，第 4 个 1 bit 所在 node, R 中编号为 4 的 node)。</p>
<ul>
<li>rank(4.s) = rank(4) = 4</li>
<li>right-node(4).s = 2 * rank(4.s) + 1 = 2 * 4 = 8 + 1 = 9</li>
<li>S 编号中，9 对应的 node 是 R 中编号为 7 的 node。</li>
</ul></div>

<div class="note success"><p><strong>Parent</strong><br>internal node 8 (bit 序列中，第 8 个 1 bit 所在 node, R 中编号为 8 的 node)。</p>
<ul>
<li>select(8.r) = select(8) = 12</li>
<li>parent(8).r = select(8.r) / 2 = 12 / 2 = 6</li>
<li>R 编号中，6 对应的 node 是 S 中编号为 7 的 node。</li>
</ul></div>

<h2 id="Level-Order-Unary-Degree-Sequence-LOUDS"><a href="#Level-Order-Unary-Degree-Sequence-LOUDS" class="headerlink" title="Level Order Unary Degree Sequence (LOUDS)"></a>Level Order Unary Degree Sequence (LOUDS)</h2><p>下面对于任意的 tree 进行编码。<br>首先对已有的 tree 的 root 之上，添加一个 super root。给每个节点编码的方式也很简单：<code>该节点子节点的个数个 “1” 加上一个 “0”</code>。然后按照层序遍历的方式，将这些编码组成一个 <code>bit 序列</code>。 </p>
<p><img src="3.png" alt="3"></p>
<p><img src="6.png" alt="6"></p>
<p><img src="4.png" alt="4"></p>
<p><img src="5.png" alt="5"></p>
<p>LOUDS 是一个 bit vector。我们需要如下的基本操作</p>
<blockquote>
<p><code>rank1(i)</code> – returns number of ‘1’ in the range [0, i)<br><code>rank0(i)</code> – returns number of ‘0’ in the range [0, i). <code>rank0(i) = i - rank1(i)</code><br><code>select1(rnk)</code> – returns position of rnk-th ‘1’ in the LOUDS string, rnk = 1, 2, 3, …<br><code>select0(rnk)</code> – returns position of rnk-th ‘0’ in the LOUDS string, rnk = 1, 2, 3, …</p>
</blockquote>
<p>这四个基本操作可以在 O(1) 时间复杂度下实现，这个后面我们再聊实现。这些操作可以衍生出下面的对树的操作：</p>
<p>Different ways of tree node numbering for LOUDS are possible, Memoria uses the simplest one. Tree node positions are coded by ‘1’.</p>
<ul>
<li><code>node_num = rank1(i)</code> – gets tree node number at position i;</li>
<li><code>i = select1(node_num)</code> – finds position of a node in LOUDS given its number in the tree.</li>
</ul>
<p>Having this node numbering we can define the following tree navigation operations:</p>
<ul>
<li><code>fist_child(i) = select0(rank1(i)) + 1</code> – finds position of the first child for node at the position i;</li>
<li><code>last_child(i) = select0(rank1(i) + 1) - 1</code>– finds position of the last child for node at the position i;</li>
<li><code>parent(i) = select1(rank0(i))</code> – finds position of the parent for the node at the position i;</li>
<li><code>children(i) = last_child(i) - first_child(i)</code> – return number of children for node at the position i;</li>
<li><code>child(i, num) = first_child(i) + num</code> – returns position of num-th child for the node at the position i, num &gt;= 0;</li>
<li><code>is_node(i) = LOUDS[i] == 1 ? true : false</code> – checks if i-th position in tree node.</li>
</ul>
<p>Note that navigation operations only defined for positions i for those <code>is_leaf(i) == true</code>.</p>
<p><img src="7.png" alt="7"></p>
<p>比如我想找 <code>node_num = 8</code> 的 first child 11，last child 12，parent 4：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">       node_num = <span class="number">8</span></span><br><span class="line">i =  select1(<span class="number">8</span>) = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">first_child(<span class="number">12</span>) = select0(rank1(<span class="number">12</span>)) + <span class="number">1</span></span><br><span class="line">                = select0(<span class="number">8</span>) + <span class="number">1</span></span><br><span class="line">                = <span class="number">19</span></span><br><span class="line">      rank1(<span class="number">19</span>) = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"> last_child(<span class="number">12</span>) = select0(rank1(<span class="number">12</span>) + <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">                = select0(<span class="number">8</span> + <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">                = select0(<span class="number">9</span>) - <span class="number">1</span></span><br><span class="line">                = <span class="number">21</span> - <span class="number">1</span></span><br><span class="line">                = <span class="number">20</span></span><br><span class="line">      rank1(<span class="number">20</span>) = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">     parent(<span class="number">12</span>) = select1(rank0(<span class="number">12</span>))</span><br><span class="line">                = select1(<span class="number">4</span>)</span><br><span class="line">                = <span class="number">5</span></span><br><span class="line">       rank1(<span class="number">5</span>) = <span class="number">4</span></span><br></pre></td></tr></table></figure>


<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.computer.org/csdl/proceedings/focs/1989/1982/00/063533.pdf" target="_blank" rel="noopener">Space-efficient Static Trees and Graphs</a><br><a href="https://db.cs.cmu.edu/papers/2018/mod601-zhangA-hm.pdf" target="_blank" rel="noopener">SuRF: Practical Range Query Filtering with Fast Succinct Tries</a><br><a href="https://courses.cs.ut.ee/MTAT.03.238/2013_fall/uploads/Main/06_alg_Succinct.6up.pdf" target="_blank" rel="noopener">Advanced(Algorithmics((6EAP)</a><br><a href="https://cs.uwaterloo.ca/~imunro/cs840/Notes16/SuccinctDS.pdf" target="_blank" rel="noopener">Succinct Data Structures</a><br><a href="https://en.wikipedia.org/wiki/Succinct_data_structure#cite_note-jacobson1989space-4" target="_blank" rel="noopener">Succinct data structure</a><br><a href="https://lra.le.ac.uk/bitstream/2381/318/4/rank-select.pdf" target="_blank" rel="noopener">Rank and Select Operations on Binary Strings (1974; Elias)</a><br><a href="http://chuansong.me/n/2035229" target="_blank" rel="noopener">一种神奇的数据结构—小波树</a><br><a href="https://en.wikipedia.org/wiki/Range_minimum_query" target="_blank" rel="noopener">Range minimum query</a><br><a href="http://alexbowe.com/rrr/" target="_blank" rel="noopener">RRR – A Succinct Rank/Select Index for Bit Vectors</a></p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>leveldb</tag>
        <tag>bloomfilter</tag>
      </tags>
  </entry>
  <entry>
    <title>inode symbolic and hard link</title>
    <url>/blog/inode-symblic-and-hard-link/</url>
    <content><![CDATA[<p>一张图介绍 symbolic 和 hard link。<br><img src="1.png" alt="inode"></p>
<a id="more"></a>
<h1 id="hard-link-explain-in-bash"><a href="#hard-link-explain-in-bash" class="headerlink" title="hard link explain in bash"></a>hard link explain in bash</h1><script src="https://asciinema.org/a/aBvSNT6fDqNQ3F6Zl631XVU7S.js" id="asciicast-aBvSNT6fDqNQ3F6Zl631XVU7S" async></script>

<h1 id="symbolic-link-explain-in-bash"><a href="#symbolic-link-explain-in-bash" class="headerlink" title="symbolic link explain in bash"></a>symbolic link explain in bash</h1><script src="https://asciinema.org/a/Zd3vj73pAnRSxXDrtPR5ldKcy.js" id="asciicast-Zd3vj73pAnRSxXDrtPR5ldKcy" async></script>
]]></content>
      <categories>
        <category>database</category>
      </categories>
  </entry>
  <entry>
    <title>mit 6.828: lab1</title>
    <url>/blog/mit-6-828-lab1/</url>
    <content><![CDATA[<h1 id="安装-QEMU"><a href="#安装-QEMU" class="headerlink" title="安装 QEMU"></a>安装 QEMU</h1><p>使用 Ubuntu 16.04.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libglib2.0-dev libgcrypt20-dev zlib1g-dev gcc-multilib autoconf automake bison flex</span><br><span class="line">sudo apt install libpixman-1-dev libz-dev libtool libtool-bin libsdl1.2-dev</span><br><span class="line">git submodule update --init dtc</span><br><span class="line">./configure --<span class="built_in">disable</span>-kvm --target-list=<span class="string">"i386-softmmu x86_64-softmmu"</span> <span class="comment"># 默认安装到 /usr/local</span></span><br><span class="line">make -j8</span><br><span class="line">make -j8 install</span><br></pre></td></tr></table></figure>

<h1 id="Software-Setup"><a href="#Software-Setup" class="headerlink" title="Software Setup"></a>Software Setup</h1><p>下载实验材料：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">athena% mkdir ~/6.828</span><br><span class="line">athena% <span class="built_in">cd</span> ~/6.828</span><br><span class="line">athena% add git</span><br><span class="line">athena% git <span class="built_in">clone</span> https://pdos.csail.mit.edu/6.828/2017/jos.git lab</span><br><span class="line">Cloning into lab...</span><br><span class="line">athena% <span class="built_in">cd</span> lab</span><br><span class="line">athena%</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="Part-1-PC-Bootstrap"><a href="#Part-1-PC-Bootstrap" class="headerlink" title="Part 1: PC Bootstrap"></a>Part 1: PC Bootstrap</h1><p>这一部分不需要编码，只为了让我们了解 PC bootstrap 的过程。并且让我们熟悉 QEMU 和 QEMU/GDB 的 debug 过程。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> lab</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<p>这会 build 一个 boot loader 和 kernel : <code>kernel.img</code>。</p>
<h2 id="Simulating-the-x86"><a href="#Simulating-the-x86" class="headerlink" title="Simulating the x86"></a>Simulating the x86</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parallels@parallels-vm:~/Desktop/mit/lab$ make</span><br><span class="line">+ as kern/entry.S</span><br><span class="line">+ cc kern/entrypgdir.c</span><br><span class="line">+ cc kern/init.c</span><br><span class="line">+ cc kern/console.c</span><br><span class="line">+ cc kern/monitor.c</span><br><span class="line">+ cc kern/printf.c</span><br><span class="line">+ cc kern/kdebug.c</span><br><span class="line">+ cc lib/printfmt.c</span><br><span class="line">+ cc lib/readline.c</span><br><span class="line">+ cc lib/string.c</span><br><span class="line">+ ld obj/kern/kernel</span><br><span class="line">+ as boot/boot.S</span><br><span class="line">+ cc -Os boot/main.c</span><br><span class="line">+ ld boot/boot</span><br><span class="line">boot block is 390 bytes (max 510)</span><br><span class="line">+ mk obj/kern/kernel.img</span><br></pre></td></tr></table></figure>

<p><code>kernel.img</code> 包含了 boot loader (<code>obj/boot/boot</code>) 和 kernel (<code>obj/kernel</code>)。</p>
<p>然后使用命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make qemu</span><br><span class="line">or</span><br><span class="line">make qemu-nox</span><br></pre></td></tr></table></figure>

<p>为了退出，使用命令 <code>Ctrl+a x</code></p>
<p>在当前的 kernel 里面，可以使用如下两个命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">K&gt; <span class="built_in">help</span></span><br><span class="line"><span class="built_in">help</span> - Display this list of commands</span><br><span class="line">kerninfo - Display information about the kernel</span><br><span class="line">K&gt; kerninfo</span><br><span class="line">Special kernel symbols:</span><br><span class="line">  _start                  0010000c (phys)</span><br><span class="line">  entry  f010000c (virt)  0010000c (phys)</span><br><span class="line">  etext  f0101871 (virt)  00101871 (phys)</span><br><span class="line">  edata  f0112300 (virt)  00112300 (phys)</span><br><span class="line">  end    f0112944 (virt)  00112944 (phys)</span><br><span class="line">Kernel executable memory footprint: 75KB</span><br><span class="line">K&gt;</span><br></pre></td></tr></table></figure>

<h2 id="The-PC’s-Physical-Address-Space"><a href="#The-PC’s-Physical-Address-Space" class="headerlink" title="The PC’s Physical Address Space"></a>The PC’s Physical Address Space</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">     +------------------+  &lt;- 0xFFFFFFFF (4GB)</span><br><span class="line">     |      32-bit      |</span><br><span class="line">     |  memory mapped   |</span><br><span class="line">     |     devices      |</span><br><span class="line">     |                  |</span><br><span class="line">     /\/\/\/\/\/\/\/\/\/\</span><br><span class="line"></span><br><span class="line">     /\/\/\/\/\/\/\/\/\/\</span><br><span class="line">     |                  |</span><br><span class="line">     |      Unused      |</span><br><span class="line">     |                  |</span><br><span class="line">     +------------------+  &lt;- depends on amount of RAM</span><br><span class="line">     |                  |</span><br><span class="line">     |                  |</span><br><span class="line">     | Extended Memory  |</span><br><span class="line">     |                  |</span><br><span class="line">     |                  |</span><br><span class="line">---  +------------------+  &lt;- 0x00100000 (1MB)</span><br><span class="line"> |   |     BIOS ROM     |</span><br><span class="line"> |   +------------------+  &lt;- 0x000F0000 (960KB)</span><br><span class="line">hole |  16-bit devices, |</span><br><span class="line"> |   |  expansion ROMs  |</span><br><span class="line"> |   +------------------+  &lt;- 0x000C0000 (768KB)</span><br><span class="line"> |   |   VGA Display    |</span><br><span class="line">---  +------------------+  &lt;- 0x000A0000 (640KB)</span><br><span class="line">     |                  |</span><br><span class="line">     |    Low Memory    |</span><br><span class="line">     |                  |</span><br><span class="line">     +------------------+  &lt;- 0x00000000</span><br></pre></td></tr></table></figure>

<blockquote>
<p>The first PCs, which were based on the 16-bit Intel 8088 processor, were only capable of addressing 1MB of physical memory. The physical address space of an early PC would therefore start at 0x00000000 but end at 0x000FFFFF instead of 0xFFFFFFFF. The 640KB area marked <code>&quot;Low Memory&quot;</code> was the only random-access memory (RAM) that an early PC could use; in fact the very earliest PCs only could be configured with 16KB, 32KB, or 64KB of RAM!</p>
</blockquote>
<blockquote>
<p>The 384KB area from 0x000A0000 through 0x000FFFFF was reserved by the hardware for special uses such as video display buffers and firmware held in non-volatile memory. The most important part of this reserved area is the <code>Basic Input/Output System</code> (BIOS), which occupies the 64KB region from 0x000F0000 through 0x000FFFFF. In early PCs the BIOS was held in true read-only memory (ROM), but current PCs store the BIOS in updateable flash memory. The BIOS is responsible for <code>performing basic system initialization</code> such as activating the video card and checking the amount of memory installed. After performing this initialization, the BIOS <code>loads the operating system</code> from some appropriate location such as floppy disk, hard disk, CD-ROM, or the network, and <code>passes control</code> of the machine <code>to the operating system</code>.</p>
</blockquote>
<blockquote>
<p>When Intel finally “broke the one megabyte barrier” with the 80286 and 80386 processors, which supported 16MB and 4GB physical address spaces respectively, the PC architects nevertheless preserved the original layout for the low 1MB of physical address space in order to ensure backward compatibility with existing software. Modern PCs therefore have a <code>&quot;hole&quot;</code> in physical memory from <code>0x000A0000 to 0x00100000</code>, dividing RAM into “low” or “conventional memory” (the first 640KB) and “extended memory” (everything else). In addition, some space at the very top of the PC’s 32-bit physical address space, above all physical RAM, is now commonly reserved by the BIOS for use by 32-bit PCI devices.</p>
</blockquote>
<blockquote>
<p>Recent x86 processors can support more than 4GB of physical RAM, so RAM can extend further above 0xFFFFFFFF. In this case the BIOS must arrange to leave a <code>second hole</code> in the system’s RAM at the top of the 32-bit addressable region, to leave room for these 32-bit devices to be mapped. Because of design limitations JOS will use only the first 256MB of a PC’s physical memory anyway, so for now we will pretend that all PCs have “only” a 32-bit physical address space. But dealing with complicated physical address spaces and other aspects of hardware organization that evolved over many years is one of the important practical challenges of OS development.</p>
</blockquote>
<h2 id="The-ROM-BIOS"><a href="#The-ROM-BIOS" class="headerlink" title="The ROM BIOS"></a>The ROM BIOS</h2><p>这部分用来探索 IA-32 兼容的电脑是怎么启动的。</p>
<p>首先进入 lab 目录，打开两个 terminal。</p>
<ul>
<li>In one, enter <code>make qemu-gdb</code> (or <code>make qemu-nox-gdb</code>). This starts up QEMU, but <code>QEMU stops</code> just before the processor executes <code>the first instruction</code> and waits for a debugging connection from GDB. </li>
<li>In the second terminal, from the same directory you ran make, run <code>make gdb</code>. You should see something like this:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parallels@parallels-vm:~/Desktop/mit/lab$ make gdb</span><br><span class="line">gdb -n -x .gdbinit</span><br><span class="line">GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1</span><br><span class="line">...</span><br><span class="line">The target architecture is assumed to be i8086</span><br><span class="line">[f000:fff0]    0xffff0:	ljmp   <span class="variable">$0xf000</span>,<span class="variable">$0xe05b</span></span><br><span class="line">0x0000fff0 <span class="keyword">in</span> ?? ()</span><br><span class="line">+ symbol-file obj/kern/kernel</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure>

<p>我们来看下PC boot 时候的这个第一条指令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[f000:fff0] 0xffff0:	ljmp   <span class="variable">$0xf000</span>,<span class="variable">$0xe05b</span></span><br></pre></td></tr></table></figure>

<p>从这条指令可以看出如下几点：</p>
<ul>
<li>The IBM PC starts executing at physical address 0x000ffff0, which is at the very top of the 64KB area reserved for the ROM BIOS.</li>
<li>The PC starts executing with CS = 0xf000 and IP = 0xfff0.</li>
<li>The first instruction to be executed is a jmp instruction, which jumps to the segmented address CS = 0xf000 and IP = 0xe05b.</li>
</ul>
<p>On processor reset, the (simulated) processor enters real mode and sets CS to 0xf000 and the IP to 0xfff0, so that execution begins at that (CS:IP) segment address. </p>
<h2 id="Part-2-The-Boot-Loader"><a href="#Part-2-The-Boot-Loader" class="headerlink" title="Part 2: The Boot Loader"></a>Part 2: The Boot Loader</h2><p>After initializing the PCI bus and all the important devices the BIOS knows about, it searches for a <code>bootable device</code> such as a floppy, hard drive, or CD-ROM.</p>
<p>If the disk is <code>bootable</code>, the <code>first sector</code> of the disk is called the <code>boot sector</code>, since this is where the boot loader code resides. </p>
<p>When the BIOS finds a bootable floppy or hard disk, it loads the <code>512-byte boot sector</code> into memory at physical addresses <code>0x7c00</code> through <code>0x7dff</code>, and then uses a jmp instruction to set the CS:IP to 0000:7c00, passing control to the boot loader.</p>
<p>For 6.828, the bootload code is in <code>boot/boot.s</code> and <code>boot/main.c</code>. The bootloader perform two main functions:</p>
<ol>
<li>switch from <code>real mode</code> to <code>32bit protected mode</code>, so the software can access memory above 1MB. And the segmented addresses change from 16bit to 32bit.</li>
<li>boot loader reads kernel from hard disk.</li>
</ol>
]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>mit6.828</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Review:HOT: A Height Optimized Trie Index for Main-Memory Database Systems</title>
    <url>/blog/Paper-Review-HOT-A-Height-Optimized-Trie-Index-for-Main-MemoryDatabase-Systems/</url>
    <content><![CDATA[<h1 id="Backgroud"><a href="#Backgroud" class="headerlink" title="Backgroud"></a>Backgroud</h1><h2 id="Patricia-Tree-or-Radix-Tree-or-Compact-Prefix-Tree"><a href="#Patricia-Tree-or-Radix-Tree-or-Compact-Prefix-Tree" class="headerlink" title="Patricia Tree or Radix Tree or Compact Prefix Tree"></a><a href="https://en.wikipedia.org/wiki/Radix_tree" target="_blank" rel="noopener">Patricia Tree or Radix Tree or Compact Prefix Tree</a></h2><p><img src="1.png" alt="Radix Tree"><br>A trie that each node which is the only child is merged with its parent.</p>
<h2 id="Adaptive-Radix-Tree-ART"><a href="#Adaptive-Radix-Tree-ART" class="headerlink" title="Adaptive Radix Tree (ART)"></a>Adaptive Radix Tree (ART)</h2><p><img src="2.png" alt="ART"><br>The adaptive radix tree is a radix tree variant that integrates <code>adaptive node sizes</code> to the radix tree. One major drawback of the usual <code>radix trees</code> is the use of space, because it uses a <code>constant node</code> size in every level. The <code>major difference</code> between the radix tree and the adaptive radix tree is <code>its variable size for each node</code> based on the number of child elements, which grows while adding new entries. Hence, the adaptive radix tree leads to a better use of space without reducing its speed.</p>
<div class="note danger"><p>既然是动态增加的，node 本身的 memory 分配也应该是动态的，那么 memory 不连续，可能会导致 cache miss 问题。这个需要看原文 paper 看是否存在。<a href="https://db.in.tum.de/~leis/papers/ART.pdf" target="_blank" rel="noopener">link</a></p></div>
<a id="more"></a>

<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><img src="3.png" alt="HOT"></p>
<ul>
<li>Each trie depicted in Figure 2 stores the same 13 keys, all of which are 9 bits long.</li>
<li>The shape with the same color represent a node.</li>
<li>Compound nodes are surrounded by solid lines.</li>
<li>Dots in the figures represent either leaf values or bit positions in compound nodes which are used to distinguish between different keys.</li>
</ul>
<p><strong>Height Optimized Trie</strong></p>
<ul>
<li>HOT combines multiple nodes of a binary Patricia trie into compound nodes having a <code>maximum node fanout</code> of a predefined value k</li>
<li>Thus each node uses a custom span suitable to represent the discriminative bits of the combined nodes.</li>
<li>Adaptive node sizes (same as ART) are used to reduce memory consumption</li>
<li>Figure 2f shows a Height Optimized Trie with a maximum node fanout ofk = 4 that has 4 compound nodes and an overall height of 2 to store the same 13 keys as the other trie structures</li>
</ul>
<p>This paper want to minimize the over height of the trie such that <code>maximum number of partitions along a path</code> from the root node <code>to any leaf node</code> is minimized.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0020019003005118" target="_blank" rel="noopener">Partitioning of trees for minimizing height and cardinality</a> solves this optimization in static tree. This paper present a <code>dynamic algorithm</code>, which is able to preserve the height optimized partitioning <code>while new data is inserted</code>.</p>
<h1 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h1><p><img src="4.png" alt="Dynamic Insertion"></p>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><ul>
<li><code>BiNode</code> denote a node in a binary Patricia trie.</li>
<li>Term <code>node</code> stands for a compound node.</li>
<li>Here maximum node fanout of k = 3.</li>
</ul>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ul>
<li><p><code>normal case</code><br>Insertion is performed by locally modifying the BiNode structure of the affected node. More precisely, and as shown in <code>Figure 4b</code>, a new discriminating BiNode, which discriminates the new key from the keys contained in the subtree of the mismatching BiNode, is created and inserted into the affected node.</p>
</li>
<li><p><code>leaf-node pushdown</code>: This involves <code>creating a new node</code> instead of adding a new BiNode to an existing node.<br>If the mismatching BiNode is a leaf and the affected node is an inner node (h(n) &gt; 1), we replace the leaf with a new node.<br>This case is triggered when the key 010 is inserted into the tree shown in Figure 4b. Leaf-node pushdown does not affect the maximum tree height as can be observed in Figure 4c: Even after leaf-node pushdown, the height of the root node (and thus the tree) is still 2.</p>
</li>
<li><p><code>overflow</code>: fanout is bigger than k = 3.<br>An overflow happens when neither leaf-node pushdown nor normal insert are applicable. As Figure 4d shows, such an invalid intermediate state occurs after inserting 0011000.</p>
</li>
<li><p>Two ways to resolve <code>overflow</code>:</p>
<ul>
<li><code>Parent Pull Up</code>: Figure 4e<br>Moving the <code>root BiNode of the overflowed node</code> into its parent node. This approach is taken when growing the tree “downwards” would increase the tree height, and it is therefore better try to grow the tree “upwards”. More formally, parent pull up is triggered when the height of the overflowed node n is “almost” the height of its parent: <code>h(n) + 1 = h(parent(n))</code>. Overflow handling therefore needs to be <code>recursively</code> applied to the affected parent node. Similar to a B-tree, the overall height of HOT only increases when a new root node is created.</li>
<li><code>intermediate node creation</code>:<br><code>Root BiNode</code> of the overflowed node is moved into a <code>newly created intermediate node</code>. Intermediate node creation is only applicable if adding an additional intermediate node does not increase the overall tree height, which is the case if: <code>h(n) + 1 &lt; h(parent(n))</code>. As shown in Figure 4g.</li>
</ul>
</li>
</ul>
<p>The paper also conjecture the HOT trie has the property that that any given set of keys results in the same structure, regardless of the insertion order.</p>
<h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p><code>Key idea</code>: Linearizing a k-constrained trie to a compact bit string that can be search in parallel using <code>SIMD instructions</code>. To achieve this, this paper store the discriminative bits of each key consecutively.</p>
<p><img src="5.png" alt="desing"></p>
<ul>
<li>using <code>bit position</code> to form discriminative bits which is called <code>partial key (dense)</code></li>
<li>maximum fanout is k = 32 so as to utilizing the cache and fast update.</li>
<li>partial keys is aligned to enable SIMD operations.</li>
<li>key extraction (<code>PEXT</code> instruction: <a href="https://www.felixcloutier.com/x86/PEXT.html" target="_blank" rel="noopener">_pext_u64</a> and <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_pext_u64&expand=3893,4072" target="_blank" rel="noopener">link</a>):<br><img src="6.png" alt="PDEP"><br>key extraction is done for every node to extract bits from the search key bit-by-bit to form the comparison key (partial key).</li>
</ul>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>index</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Review: A Comparative Study of Secondary Indexing Techniques in LSM-based NoSQL Databases</title>
    <url>/blog/Paper-Review-A-Comparative-Study-of-Secondary-Indexing-Techniques-inLSM-based-NoSQL-Databases/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>NoSql databases have fast write throughput and fast lookup on primary key. And they also have good scalability and reliability.</p>
<p>However, in order to search non-primary attributes, <code>secondary index</code> should be maintained and this could hurts <code>write performance</code>.</p>
<p>This paper compares two types of secondary index:</p>
<ul>
<li><p>Embedded Indexes: lightweight filters embedded inside the primary table</p>
</li>
<li><p>Stand-Alone Indexes: separate data structure</p>
</li>
</ul>
<p>by implementing 2 <code>Embedded Indexes</code> and 3 <code>Stand-Alone Indexes</code> on top of LevelDB.</p>
<p>They conclude that : the <code>embedded indexes</code> offer superior <code>write throughput</code> and are more space efficient, whereas the <code>stand-alone secondary indexes</code> achieve <code>faster query</code> response times. So the optimal choice depends on the application workload.</p>
<a id="more"></a>
<h1 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h1><h2 id="Stand-Alone-Indexes"><a href="#Stand-Alone-Indexes" class="headerlink" title="Stand-Alone Indexes"></a>Stand-Alone Indexes</h2><p>Using tweets as example: <code>PUT(t1, {u1, text1})</code></p>
<ul>
<li>t1: tweet id</li>
<li>u1: user id</li>
<li>text1: tweet content</li>
</ul>
<p>There are <code>three types</code> of Stand-Alone Indexes update:</p>
<p><img src="1.png" alt=""></p>
<ul>
<li><code>Eager updates</code>: Earlier version of Cassandra<br>to execute <code>PUT(t4, {u1, text4})</code> on an Eager Index, we must retrieve the list for u1, add t4 and save it back.</li>
</ul>
<p><strong>Draw Back</strong>: degrades the write performance.</p>
<ul>
<li><p><code>Lazy updates</code>:<br>simply issue a PUT(u1, {t4}) on the user id index table without retrieving the existing posting list for u1. The old postings list of u1 is merged with (u1, {t4}) later, during the periodic compaction phase.</p>
</li>
<li><p><code>Composite</code>: <a href="https://asterixdb.apache.org/" target="_blank" rel="noopener">AsterixDB</a> and <a href="https://ai.google/research/pubs/pub39966" target="_blank" rel="noopener">Spanner</a><br>each entry in the secondary indexes is a composite key consisting of (secondary key + primary key). The secondary lookup is a prefix search on secondary key, which can be implemented using regular <code>range search</code> on the index table.<br>Write and compaction can be faster than <code>Lazy</code>, but the secondary attribute lookup may be slower because of range scan.</p>
</li>
</ul>
<h2 id="Embedded-Secondary-Indexes"><a href="#Embedded-Secondary-Indexes" class="headerlink" title="Embedded Secondary Indexes"></a>Embedded Secondary Indexes</h2><p>There is no separate secondary index structure,  secondary attribute information is stored inside the original (primary) data blocks. As shown in figure (b).</p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title>第一个 Dapp</title>
    <url>/blog/%E7%AC%AC%E4%B8%80%E4%B8%AA-Dapp/</url>
    <content><![CDATA[<h1 id="配置开发环境"><a href="#配置开发环境" class="headerlink" title="配置开发环境"></a>配置开发环境</h1><h2 id="安装-Nodejs"><a href="#安装-Nodejs" class="headerlink" title="安装 Nodejs"></a>安装 <a href="https://nodejs.org" target="_blank" rel="noopener">Nodejs</a></h2><p>Mac 上安装 nodejs：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></figure>

<h2 id="安装-Truffle"><a href="#安装-Truffle" class="headerlink" title="安装 Truffle"></a>安装 <a href="http://truffleframework.com/" target="_blank" rel="noopener">Truffle</a></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g truffle <span class="comment"># http://truffleframework.com/</span></span><br></pre></td></tr></table></figure>
<p>truffle 是一个用于在 Etherem 上开发 Dapp 的框架。它让我们能够用 solidity 编程语言去写 Dapp 并进行调试。</p>
<h2 id="安装-Ganache"><a href="#安装-Ganache" class="headerlink" title="安装 Ganache"></a>安装 <a href="http://truffleframework.com/ganache/" target="_blank" rel="noopener">Ganache</a></h2><p>Ganache 是一个本地的 in memory blockchain，让我们用于测试自己编写的 Dapp<br><img src="1.png" alt=""></p>
<h2 id="安装-Metamask-google-插件"><a href="#安装-Metamask-google-插件" class="headerlink" title="安装 Metamask google 插件"></a>安装 <a href="https://metamask.io/" target="_blank" rel="noopener">Metamask</a> google 插件</h2><p>为了能够使用 ethereum blockchain，我们需要安装 Google Chrome 的 METAMASK 扩展插件。使用 METAMASK 就可以让我们连接到本地的 etherum blockchain (Ganache 创建的)，并和我们的 smart contract 做交互。</p>
<p>MetaMask 能够是我们的 Chrome 浏览器变成一个 blockchain 的浏览器，一个连接到 Ethereum network 的浏览器。<br><img src="2.gif" alt=""></p>
<a id="more"></a>
<h2 id="快速部署-truffle-项目"><a href="#快速部署-truffle-项目" class="headerlink" title="快速部署 truffle 项目"></a>快速部署 truffle 项目</h2><p>truffle 给我们提供了模板（这里称作为 box），用于我们进行快速开发。所以只要使用 <code>unbox</code> 命令来解压模板，就可以了。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@macs-macbook  ~/Code/blockchain/election  truffle unbox pet-shop</span><br><span class="line">Downloading...</span><br><span class="line">Unpacking...</span><br><span class="line">Setting up...</span><br><span class="line">Unbox successful. Sweet!</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line"></span><br><span class="line">  Compile:        truffle compile</span><br><span class="line">  Migrate:        truffle migrate</span><br><span class="line">  Test contracts: truffle <span class="built_in">test</span></span><br><span class="line">  Run dev server: npm run dev</span><br></pre></td></tr></table></figure>


<h1 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h1><h2 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h2><p>分别在 contracts 和 migrations 文件夹创建如下代码：<br><img src="4.png" alt=""><br><img src="5.png" alt=""></p>
<figure class="highlight bash"><figcaption><span>:./contracts/Election.sol</span></figcaption><table><tr><td class="code"><pre><span class="line">pragma solidity ^0.4.11;</span><br><span class="line">contract Election &#123;</span><br><span class="line">    // Read cnadidate</span><br><span class="line">    string public candidate;</span><br><span class="line">    // Constructor</span><br><span class="line">    constructor() public &#123;</span><br><span class="line">        candidate = <span class="string">"Candidate 1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><figcaption><span>:./migrations/2_deploy_contracts.js</span></figcaption><table><tr><td class="code"><pre><span class="line">var Election = artifacts.require(<span class="string">"./Election.sol"</span>);</span><br><span class="line">module.exports = <span class="keyword">function</span>(deployer) &#123;</span><br><span class="line">  deployer.deploy(Election);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="获取-smart-contract-的-instance（实例）"><a href="#获取-smart-contract-的-instance（实例）" class="headerlink" title="获取 smart contract 的 instance（实例）"></a>获取 smart contract 的 instance（实例）</h3><p>Election 是我们定义的 contract 名。<br>首先使用如下命令，将我们定义的 smart contract 发布到 blockchain 中。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">truffle migrate</span><br></pre></td></tr></table></figure>
<p>需要注意的是，每次 deploy 一个 smart contract 都会消耗 ETH，这里消耗了 0,05 的 ETH<br><img src="3.png" alt=""><br>然后使用下面的命令，获取 smart contract 的一个 instance（实例）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@macs-macbook  ~/Code/blockchain/election  truffle console</span><br><span class="line">truffle(development)&gt; Election.deployed().<span class="keyword">then</span>( <span class="keyword">function</span> (instance) &#123; app = instance &#125;)</span><br><span class="line">undefined</span><br><span class="line">truffle(development)&gt; app.address</span><br><span class="line"><span class="string">'0x139c2cafabda6bd79b41e0d484e5ad440adf4bcb'</span></span><br><span class="line">truffle(development)&gt; app.candidate()</span><br><span class="line"><span class="string">'Candidate 1'</span></span><br></pre></td></tr></table></figure>

<h2 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h2><p>下面使用一个新的 smart contract 实现投票的 Dapp。</p>
<figure class="highlight bash"><figcaption><span>:./contracts/Election.sol</span></figcaption><table><tr><td class="code"><pre><span class="line">pragma solidity ^0.4.2;</span><br><span class="line"></span><br><span class="line">contract Election &#123;</span><br><span class="line">    // Model a Candidate</span><br><span class="line">    struct Candidate &#123;</span><br><span class="line">        uint id;</span><br><span class="line">        string name;</span><br><span class="line">        uint voteCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Store accounts that have voted</span><br><span class="line">    mapping(address =&gt; bool) public voters;</span><br><span class="line">    // Store Candidates</span><br><span class="line">    // Fetch Candidate</span><br><span class="line">    mapping(uint =&gt; Candidate) public candidates;</span><br><span class="line">    // Store Candidates Count</span><br><span class="line">    uint public candidatesCount;</span><br><span class="line"></span><br><span class="line">    // voted event</span><br><span class="line">    event votedEvent (</span><br><span class="line">        uint indexed _candidateId</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="keyword">function</span> Election () public &#123;</span><br><span class="line">        addCandidate(<span class="string">"Tom"</span>);</span><br><span class="line">        addCandidate(<span class="string">"Jerry"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">function</span> addCandidate (string _name) private &#123;</span><br><span class="line">        candidatesCount ++;</span><br><span class="line">        candidates[candidatesCount] = Candidate(candidatesCount, _name, 0);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">function</span> vote (uint _candidateId) public &#123;</span><br><span class="line">        // require that they havent voted before</span><br><span class="line">        require(!voters[msg.sender]);</span><br><span class="line"></span><br><span class="line">        // require a valid candidate</span><br><span class="line">        require(_candidateId &gt; 0 &amp;&amp; _candidateId &lt;= candidatesCount);</span><br><span class="line"></span><br><span class="line">        // record that voter has voted</span><br><span class="line">        voters[msg.sender] = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        // update candidate vote Count</span><br><span class="line">        candidates[_candidateId].voteCount ++;</span><br><span class="line"></span><br><span class="line">        // trigger voted event</span><br><span class="line">        votedEvent(_candidateId);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>而后在 terminal 里面重新 deploy 一下该 contract。因为 contract 的代码被修改了，所以需要 reset。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">truffle migrate --reset</span><br></pre></td></tr></table></figure>

<p>之后重新进入 console 查询状态：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@macs-macbook  ~/Code/blockchain/election  truffle console</span><br><span class="line">truffle(development)&gt; Election.deployed().<span class="keyword">then</span>( <span class="keyword">function</span> (instance) &#123; app = instance &#125;)</span><br><span class="line">undefined</span><br><span class="line">truffle(development)&gt; app.candidates(1)</span><br><span class="line">[ BigNumber &#123; s: 1, e: 0, c: [ 1 ] &#125;,</span><br><span class="line">  <span class="string">'Tom'</span>,</span><br><span class="line">  BigNumber &#123; s: 1, e: 0, c: [ 0 ] &#125; ]</span><br><span class="line">truffle(development)&gt; app.candidates(2)</span><br><span class="line">[ BigNumber &#123; s: 1, e: 0, c: [ 2 ] &#125;,</span><br><span class="line">  <span class="string">'Jerry'</span>,</span><br><span class="line">  BigNumber &#123; s: 1, e: 0, c: [ 0 ] &#125; ]</span><br><span class="line">truffle(development)&gt; app.candidates(99) <span class="comment"># 这里因为没有 key 为99的 candidate，所以返回空</span></span><br><span class="line">[ BigNumber &#123; s: 1, e: 0, c: [ 0 ] &#125;,</span><br><span class="line">  <span class="string">''</span>,</span><br><span class="line">  BigNumber &#123; s: 1, e: 0, c: [ 0 ] &#125; ]</span><br><span class="line">truffle(development)&gt; app.candidatesCount()</span><br><span class="line">BigNumber &#123; s: 1, e: 0, c: [ 2 ] &#125;</span><br></pre></td></tr></table></figure>

<h2 id="获取指定-candidate-的值"><a href="#获取指定-candidate-的值" class="headerlink" title="获取指定 candidate 的值"></a>获取指定 candidate 的值</h2><p>在 smart contract 中，value 的获取是 async 的，所以不可以将赋值写为：<code>candidate = app.candidates(1)</code>。必须在回调函数里面赋值：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">truffle(development)&gt; app.candidates(1).<span class="keyword">then</span>( <span class="keyword">function</span>(c) &#123; candidate = c;&#125;)</span><br><span class="line">undefined</span><br><span class="line">truffle(development)&gt; candidate</span><br><span class="line">[ BigNumber &#123; s: 1, e: 0, c: [ 1 ] &#125;,</span><br><span class="line">  <span class="string">'Tom'</span>,</span><br><span class="line">  BigNumber &#123; s: 1, e: 0, c: [ 0 ] &#125; ]</span><br><span class="line"><span class="comment"># 获取得到的 candidate struct 里对应的 id，name，voteCount 值</span></span><br><span class="line">truffle(development)&gt; candidate[0].toNumber()</span><br><span class="line">1</span><br><span class="line">truffle(development)&gt; candidate[1]</span><br><span class="line"><span class="string">'Tom'</span></span><br><span class="line">truffle(development)&gt; candidate[2].toNumber()</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<h2 id="查看-blockchain-中的-account"><a href="#查看-blockchain-中的-account" class="headerlink" title="查看 blockchain 中的 account"></a>查看 blockchain 中的 account</h2><p>在 Ganache 中，我们创建了 10 个 accounts<br><img src="6.png" alt=""></p>
<p>同时这些账户可以在 truffl 的 console 里面使用 web3 查找到。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">truffle(development)&gt; web3.eth.accounts</span><br><span class="line">[ <span class="string">'0xdde9664954edb28dc2afb866e668447256b15365'</span>,</span><br><span class="line">  <span class="string">'0x062f0fb7e869943884698ec2bd67d8de1e612eac'</span>,</span><br><span class="line">  <span class="string">'0x26d8d3a6e3c84775150939505b407ebef4391f8e'</span>,</span><br><span class="line">  <span class="string">'0x31de3718ca3a2d6e0df954fe585b1bee596e0642'</span>,</span><br><span class="line">  <span class="string">'0x00645cf254bdb1ff4004c1f91a17a71032eb8bbf'</span>,</span><br><span class="line">  <span class="string">'0x6c8fdd7a767e8182270b301c83974a614b6d79b3'</span>,</span><br><span class="line">  <span class="string">'0x8d29c76eb9f4d978defe7e624ca95207e926fcb8'</span>,</span><br><span class="line">  <span class="string">'0xecad6764d5eacaf402ae61726c5768866c15ee4b'</span>,</span><br><span class="line">  <span class="string">'0xa2f4ba6a56c738f968e1509851b84de27207bcc3'</span>,</span><br><span class="line">  <span class="string">'0xf654b8f4bbee0a95cab96aabf3fddb77813b493e'</span> ]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><figcaption><span>:测试 vote</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># solidity 允许给 function 传递除了参数以外的一些 metadata，这里传递了一个 &#123;from: &lt;account address&gt;&#125;。</span></span><br><span class="line"><span class="comment"># 这里 "&#123;from: ...&#125;" 就是指定了 msg.sender 参数。</span></span><br><span class="line"><span class="comment"># 最后输出的值，其实是一次 transaction 的 recipt。read blockchain 不消耗 eth，但是写 消耗，这里消耗了 49101 gas。gas 到 eth 的转化，是 gas * gas_price。这个留待解释。 TODO</span></span><br><span class="line">truffle(development)&gt; app.vote(1,&#123;from: web3.eth.accounts[4]&#125;)</span><br><span class="line">&#123; tx: <span class="string">'0x4d33e6442f55fbc5f11cafc4c4d33d6e8fec879d24c5d200c82bb0d2486e01d8'</span>,</span><br><span class="line">  receipt:</span><br><span class="line">   &#123; transactionHash: <span class="string">'0x4d33e6442f55fbc5f11cafc4c4d33d6e8fec879d24c5d200c82bb0d2486e01d8'</span>,</span><br><span class="line">     transactionIndex: 0,</span><br><span class="line">     blockHash: <span class="string">'0xf524dd8578c2ef4f0d63f24f947f0eb83c551520fa856eb02c41d6d4c8e57884'</span>,</span><br><span class="line">     blockNumber: 66,</span><br><span class="line">     gasUsed: 49101,</span><br><span class="line">     cumulativeGasUsed: 49101,</span><br><span class="line">     contractAddress: null,</span><br><span class="line">     logs: [ [Object] ],</span><br><span class="line">     status: <span class="string">'0x01'</span>,</span><br><span class="line">     logsBloom: <span class="string">'0x00000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000008000000000000000000000000000040000000000004000000000000000000000000000000000040000000000004000000000000000000000000040000000000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000040000000000000000000000000000000000000000000000000000000000000000000'</span> &#125;,</span><br><span class="line">  logs:</span><br><span class="line">   [ &#123; logIndex: 0,</span><br><span class="line">       transactionIndex: 0,</span><br><span class="line">       transactionHash: <span class="string">'0x4d33e6442f55fbc5f11cafc4c4d33d6e8fec879d24c5d200c82bb0d2486e01d8'</span>,</span><br><span class="line">       blockHash: <span class="string">'0xf524dd8578c2ef4f0d63f24f947f0eb83c551520fa856eb02c41d6d4c8e57884'</span>,</span><br><span class="line">       blockNumber: 66,</span><br><span class="line">       address: <span class="string">'0xeb7e9f45214c6b13dece3a14be570d9eaaa9144c'</span>,</span><br><span class="line">       <span class="built_in">type</span>: <span class="string">'mined'</span>,</span><br><span class="line">       event: <span class="string">'votedEvent'</span>,</span><br><span class="line">       args: [Object] &#125; ] &#125;</span><br></pre></td></tr></table></figure>


<h2 id="编写测试用例"><a href="#编写测试用例" class="headerlink" title="编写测试用例"></a>编写测试用例</h2><p>truffle 内部使用 <a href="https://mochajs.org/" target="_blank" rel="noopener">mochajs</a> 和 <a href="http://www.chaijs.com/" target="_blank" rel="noopener">chaijs</a> 进行测试</p>
<blockquote>
<p>Mocha is a feature-rich JavaScript test framework running on Node.js and in the browser, making asynchronous testing simple and fun</p>
</blockquote>
<blockquote>
<p>Chai is a BDD / TDD assertion library for node and the browser that can be delightfully paired with any javascript testing framework.</p>
</blockquote>
<figure class="highlight javascript"><figcaption><span>:./test/election.js</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> Election = artifacts.require(<span class="string">"./Election.sol"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化 contract 实例，传入的是 web3.eth.accounts 参数</span></span><br><span class="line">contract(<span class="string">"Election"</span>, <span class="function"><span class="keyword">function</span>(<span class="params">accounts</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> electionInstance;</span><br><span class="line"></span><br><span class="line">  it(<span class="string">"initializes with two candidates"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> instance.candidatesCount();</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">count</span>) </span>&#123;</span><br><span class="line">      assert.equal(count, <span class="number">2</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  it(<span class="string">"it initializes the candidates with the correct values"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      electionInstance = instance;</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(<span class="number">1</span>);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate</span>) </span>&#123;</span><br><span class="line">      assert.equal(candidate[<span class="number">0</span>], <span class="number">1</span>, <span class="string">"contains the correct id"</span>);</span><br><span class="line">      assert.equal(candidate[<span class="number">1</span>], <span class="string">"Tom"</span>, <span class="string">"contains the correct name"</span>);</span><br><span class="line">      assert.equal(candidate[<span class="number">2</span>], <span class="number">0</span>, <span class="string">"contains the correct votes count"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(<span class="number">2</span>);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate</span>) </span>&#123;</span><br><span class="line">      assert.equal(candidate[<span class="number">0</span>], <span class="number">2</span>, <span class="string">"contains the correct id"</span>);</span><br><span class="line">      assert.equal(candidate[<span class="number">1</span>], <span class="string">"Jerry"</span>, <span class="string">"contains the correct name"</span>);</span><br><span class="line">      assert.equal(candidate[<span class="number">2</span>], <span class="number">0</span>, <span class="string">"contains the correct votes count"</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  it(<span class="string">"allows a voter to cast a vote"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      electionInstance = instance;</span><br><span class="line">      candidateId = <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">return</span> electionInstance.vote(candidateId, &#123; <span class="attr">from</span>: accounts[<span class="number">0</span>] &#125;);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">receipt</span>) </span>&#123;</span><br><span class="line">      assert.equal(receipt.logs.length, <span class="number">1</span>, <span class="string">"an event was triggered"</span>);</span><br><span class="line">      assert.equal(receipt.logs[<span class="number">0</span>].event, <span class="string">"votedEvent"</span>, <span class="string">"the event type is correct"</span>);</span><br><span class="line">      assert.equal(receipt.logs[<span class="number">0</span>].args._candidateId.toNumber(), candidateId, <span class="string">"the candidate id is correct"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.voters(accounts[<span class="number">0</span>]);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">voted</span>) </span>&#123;</span><br><span class="line">      assert(voted, <span class="string">"the voter was marked as voted"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(candidateId);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> voteCount = candidate[<span class="number">2</span>];</span><br><span class="line">      assert.equal(voteCount, <span class="number">1</span>, <span class="string">"increments the candidate's vote count"</span>);</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  it(<span class="string">"throws an exception for invalid candiates"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      electionInstance = instance;</span><br><span class="line">      <span class="keyword">return</span> electionInstance.vote(<span class="number">99</span>, &#123; <span class="attr">from</span>: accounts[<span class="number">1</span>] &#125;)</span><br><span class="line">    &#125;).then(assert.fail).catch(<span class="function"><span class="keyword">function</span>(<span class="params">error</span>) </span>&#123;</span><br><span class="line">      assert(error.message.indexOf(<span class="string">'revert'</span>) &gt;= <span class="number">0</span>, <span class="string">"error message must contain revert"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(<span class="number">1</span>);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate1</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> voteCount = candidate1[<span class="number">2</span>];</span><br><span class="line">      assert.equal(voteCount, <span class="number">1</span>, <span class="string">"candidate 1 did not receive any votes"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(<span class="number">2</span>);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate2</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> voteCount = candidate2[<span class="number">2</span>];</span><br><span class="line">      assert.equal(voteCount, <span class="number">0</span>, <span class="string">"candidate 2 did not receive any votes"</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  it(<span class="string">"throws an exception for double voting"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      electionInstance = instance;</span><br><span class="line">      candidateId = <span class="number">2</span>;</span><br><span class="line">      <span class="comment">// solidity 允许给 function 传递除了参数以外的一些 metadata，这里传递了一个 &#123;from: &lt;account address&gt;&#125;。</span></span><br><span class="line">      <span class="comment">// 这里 "&#123;from: ...&#125;" 就是指定了 msg.sender 参数。</span></span><br><span class="line">      electionInstance.vote(candidateId, &#123; <span class="attr">from</span>: accounts[<span class="number">1</span>] &#125;);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(candidateId);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> voteCount = candidate[<span class="number">2</span>];</span><br><span class="line">      assert.equal(voteCount, <span class="number">1</span>, <span class="string">"accepts first vote"</span>);</span><br><span class="line">      <span class="comment">// Try to vote again</span></span><br><span class="line">      <span class="keyword">return</span> electionInstance.vote(candidateId, &#123; <span class="attr">from</span>: accounts[<span class="number">1</span>] &#125;);</span><br><span class="line">    &#125;).then(assert.fail).catch(<span class="function"><span class="keyword">function</span>(<span class="params">error</span>) </span>&#123;</span><br><span class="line">      assert(error.message.indexOf(<span class="string">'revert'</span>) &gt;= <span class="number">0</span>, <span class="string">"error message must contain revert"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(<span class="number">1</span>);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate1</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> voteCount = candidate1[<span class="number">2</span>];</span><br><span class="line">      assert.equal(voteCount, <span class="number">1</span>, <span class="string">"candidate 1 did not receive any votes"</span>);</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidates(<span class="number">2</span>);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate2</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> voteCount = candidate2[<span class="number">2</span>];</span><br><span class="line">      assert.equal(voteCount, <span class="number">1</span>, <span class="string">"candidate 2 did not receive any votes"</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><figcaption><span>:测试结果</span></figcaption><table><tr><td class="code"><pre><span class="line">mac@macs-macbook  ~/Code/blockchain/election  truffle <span class="built_in">test</span></span><br><span class="line">Using network <span class="string">'development'</span>.</span><br><span class="line"></span><br><span class="line">  Contract: Election</span><br><span class="line">    ✓ initializes with two candidates</span><br><span class="line">    ✓ it initializes the candidates with the correct values (45ms)</span><br><span class="line">    ✓ allows a voter to cast a vote (229ms)</span><br><span class="line">    ✓ throws an exception <span class="keyword">for</span> invalid candiates (117ms)</span><br><span class="line">    ✓ throws an exception <span class="keyword">for</span> double voting (194ms)</span><br><span class="line"></span><br><span class="line">  5 passing (658ms)</span><br></pre></td></tr></table></figure>

<h2 id="网页代码"><a href="#网页代码" class="headerlink" title="网页代码"></a>网页代码</h2><figure class="highlight html"><figcaption><span>:./src/index.html</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"X-UA-Compatible"</span> <span class="attr">content</span>=<span class="string">"IE=edge"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"viewport"</span> <span class="attr">content</span>=<span class="string">"width=device-width, initial-scale=1"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Election Results<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Bootstrap --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">"css/bootstrap.min.css"</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- WARNING: Respond.js doesn't work if you view the page via file:// --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--[if lt IE 9]&gt;</span></span><br><span class="line"><span class="comment">      &lt;script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="comment">      &lt;script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"&gt;&lt;/script&gt;</span></span><br><span class="line"><span class="comment">    &lt;![endif]--&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span> <span class="attr">style</span>=<span class="string">"width: 650px;"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"row"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"col-lg-12"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">h1</span> <span class="attr">class</span>=<span class="string">"text-center"</span>&gt;</span>Election Results<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">hr</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"loader"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"text-center"</span>&gt;</span>Loading...<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"content"</span> <span class="attr">style</span>=<span class="string">"display: none;"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">table</span> <span class="attr">class</span>=<span class="string">"table"</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">thead</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;<span class="name">th</span> <span class="attr">scope</span>=<span class="string">"col"</span>&gt;</span>#<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;<span class="name">th</span> <span class="attr">scope</span>=<span class="string">"col"</span>&gt;</span>Name<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;<span class="name">th</span> <span class="attr">scope</span>=<span class="string">"col"</span>&gt;</span>Votes<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">thead</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tbody</span> <span class="attr">id</span>=<span class="string">"candidatesResults"</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">tbody</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">hr</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">form</span> <span class="attr">onSubmit</span>=<span class="string">"App.castVote(); return false;"</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"candidatesSelect"</span>&gt;</span>Select Candidate<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">select</span> <span class="attr">class</span>=<span class="string">"form-control"</span> <span class="attr">id</span>=<span class="string">"candidatesSelect"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">class</span>=<span class="string">"btn btn-primary"</span>&gt;</span>Vote<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">hr</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">id</span>=<span class="string">"accountAddress"</span> <span class="attr">class</span>=<span class="string">"text-center"</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jQuery (necessary for Bootstrap's JavaScript plugins) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Include all compiled plugins (below), or include individual files as needed --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"js/bootstrap.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"js/web3.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"js/truffle-contract.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"js/app.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight javascript"><figcaption><span>:./src/js/app.js</span></figcaption><table><tr><td class="code"><pre><span class="line">App = &#123;</span><br><span class="line">  web3Provider: <span class="literal">null</span>,</span><br><span class="line">  contracts: &#123;&#125;,</span><br><span class="line">  account: <span class="string">'0x0'</span>,</span><br><span class="line">  hasVoted: <span class="literal">false</span>,</span><br><span class="line"></span><br><span class="line">  init: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> App.initWeb3();</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  initWeb3: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> refactor conditional</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">typeof</span> web3 !== <span class="string">'undefined'</span>) &#123;</span><br><span class="line">      <span class="comment">// If a web3 instance is already provided by Meta Mask.</span></span><br><span class="line">      App.web3Provider = web3.currentProvider;</span><br><span class="line">      web3 = <span class="keyword">new</span> Web3(web3.currentProvider);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Specify default instance if no web3 instance provided</span></span><br><span class="line">      App.web3Provider = <span class="keyword">new</span> Web3.providers.HttpProvider(<span class="string">'http://localhost:7545'</span>);</span><br><span class="line">      web3 = <span class="keyword">new</span> Web3(App.web3Provider);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> App.initContract();</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化 smart contract 实例</span></span><br><span class="line">  initContract: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    $.getJSON(<span class="string">"Election.json"</span>, <span class="function"><span class="keyword">function</span>(<span class="params">election</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// Instantiate a new truffle contract from the artifact</span></span><br><span class="line">      App.contracts.Election = TruffleContract(election);</span><br><span class="line">      <span class="comment">// Connect provider to interact with contract</span></span><br><span class="line">      App.contracts.Election.setProvider(App.web3Provider);</span><br><span class="line"></span><br><span class="line">      App.listenForEvents();</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> App.render();</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Listen for events emitted from the contract</span></span><br><span class="line">  listenForEvents: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    App.contracts.Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// Restart Chrome if you are unable to receive this event</span></span><br><span class="line">      <span class="comment">// This is a known issue with Metamask</span></span><br><span class="line">      <span class="comment">// https://github.com/MetaMask/metamask-extension/issues/2393</span></span><br><span class="line">      instance.votedEvent(&#123;&#125;, &#123;</span><br><span class="line">        fromBlock: <span class="number">0</span>,</span><br><span class="line">        toBlock: <span class="string">'latest'</span></span><br><span class="line">      &#125;).watch(<span class="function"><span class="keyword">function</span>(<span class="params">error, event</span>) </span>&#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">"event triggered"</span>, event)</span><br><span class="line">        <span class="comment">// Reload when a new vote is recorded</span></span><br><span class="line">        App.render();</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 渲染显示</span></span><br><span class="line">  render: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> electionInstance;</span><br><span class="line">    <span class="keyword">var</span> loader = $(<span class="string">"#loader"</span>);</span><br><span class="line">    <span class="keyword">var</span> content = $(<span class="string">"#content"</span>);</span><br><span class="line"></span><br><span class="line">    loader.show();</span><br><span class="line">    content.hide();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Load account data</span></span><br><span class="line">    web3.eth.getCoinbase(<span class="function"><span class="keyword">function</span>(<span class="params">err, account</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (err === <span class="literal">null</span>) &#123;</span><br><span class="line">        App.account = account;</span><br><span class="line">        $(<span class="string">"#accountAddress"</span>).html(<span class="string">"Your Account: "</span> + account);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Load contract data</span></span><br><span class="line">    App.contracts.Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      electionInstance = instance;</span><br><span class="line">      <span class="keyword">return</span> electionInstance.candidatesCount();</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidatesCount</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">var</span> candidatesResults = $(<span class="string">"#candidatesResults"</span>);</span><br><span class="line">      candidatesResults.empty();</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> candidatesSelect = $(<span class="string">'#candidatesSelect'</span>);</span><br><span class="line">      candidatesSelect.empty();</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">1</span>; i &lt;= candidatesCount; i++) &#123;</span><br><span class="line">        electionInstance.candidates(i).then(<span class="function"><span class="keyword">function</span>(<span class="params">candidate</span>) </span>&#123;</span><br><span class="line">          <span class="keyword">var</span> id = candidate[<span class="number">0</span>];</span><br><span class="line">          <span class="keyword">var</span> name = candidate[<span class="number">1</span>];</span><br><span class="line">          <span class="keyword">var</span> voteCount = candidate[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Render candidate Result</span></span><br><span class="line">          <span class="keyword">var</span> candidateTemplate = <span class="string">"&lt;tr&gt;&lt;th&gt;"</span> + id + <span class="string">"&lt;/th&gt;&lt;td&gt;"</span> + name + <span class="string">"&lt;/td&gt;&lt;td&gt;"</span> + voteCount + <span class="string">"&lt;/td&gt;&lt;/tr&gt;"</span></span><br><span class="line">          candidatesResults.append(candidateTemplate);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Render candidate ballot option</span></span><br><span class="line">          <span class="keyword">var</span> candidateOption = <span class="string">"&lt;option value='"</span> + id + <span class="string">"' &gt;"</span> + name + <span class="string">"&lt;/ option&gt;"</span></span><br><span class="line">          candidatesSelect.append(candidateOption);</span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> electionInstance.voters(App.account);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">hasVoted</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// Do not allow a user to vote</span></span><br><span class="line">      <span class="keyword">if</span>(hasVoted) &#123;</span><br><span class="line">        $(<span class="string">'form'</span>).hide();</span><br><span class="line">      &#125;</span><br><span class="line">      loader.hide();</span><br><span class="line">      content.show();</span><br><span class="line">    &#125;).catch(<span class="function"><span class="keyword">function</span>(<span class="params">error</span>) </span>&#123;</span><br><span class="line">      <span class="built_in">console</span>.warn(error);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  castVote: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> candidateId = $(<span class="string">'#candidatesSelect'</span>).val();</span><br><span class="line">    App.contracts.Election.deployed().then(<span class="function"><span class="keyword">function</span>(<span class="params">instance</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> instance.vote(candidateId, &#123; <span class="attr">from</span>: App.account &#125;);</span><br><span class="line">    &#125;).then(<span class="function"><span class="keyword">function</span>(<span class="params">result</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// Wait for votes to update</span></span><br><span class="line">      $(<span class="string">"#content"</span>).hide();</span><br><span class="line">      $(<span class="string">"#loader"</span>).show();</span><br><span class="line">    &#125;).catch(<span class="function"><span class="keyword">function</span>(<span class="params">err</span>) </span>&#123;</span><br><span class="line">      <span class="built_in">console</span>.error(err);</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">$(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  $(<span class="built_in">window</span>).load(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    App.init();</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h2 id="运行实例"><a href="#运行实例" class="headerlink" title="运行实例"></a>运行实例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">truffle migrate --reset</span><br><span class="line">npm run dev</span><br><span class="line"></span><br><span class="line">&gt; pet-shop@1.0.0 dev /Users/mac/Code/blockchain/election</span><br><span class="line">&gt; lite-server</span><br><span class="line"></span><br><span class="line">** browser-sync config **</span><br><span class="line">&#123; injectChanges: <span class="literal">false</span>,</span><br><span class="line">  files: [ <span class="string">'./**/*.&#123;html,htm,css,js&#125;'</span> ],</span><br><span class="line">  watchOptions: &#123; ignored: <span class="string">'node_modules'</span> &#125;,</span><br><span class="line">  server:</span><br><span class="line">   &#123; baseDir: [ <span class="string">'./src'</span>, <span class="string">'./build/contracts'</span> ],</span><br><span class="line">     middleware: [ [Function], [Function] ] &#125; &#125;</span><br><span class="line">[Browsersync] Access URLs:</span><br><span class="line"> --------------------------------------</span><br><span class="line">       Local: http://localhost:3000</span><br><span class="line">    External: http://10.181.60.166:3000</span><br><span class="line"> --------------------------------------</span><br><span class="line">          UI: http://localhost:3001</span><br><span class="line"> UI External: http://10.181.60.166:3001</span><br><span class="line"> --------------------------------------</span><br><span class="line">[Browsersync] Serving files from: ./src</span><br><span class="line">[Browsersync] Serving files from: ./build/contracts</span><br><span class="line">[Browsersync] Watching files...</span><br><span class="line">18.05.03 21:38:06 200 GET /index.html</span><br><span class="line">18.05.03 21:38:06 200 GET /js/bootstrap.min.js</span><br><span class="line">18.05.03 21:38:06 200 GET /css/bootstrap.min.css</span><br><span class="line">18.05.03 21:38:06 200 GET /js/app.js</span><br><span class="line">18.05.03 21:38:06 200 GET /js/web3.min.js</span><br><span class="line">18.05.03 21:38:06 200 GET /js/truffle-contract.js</span><br><span class="line">18.05.03 21:38:08 200 GET /Election.json</span><br><span class="line">18.05.03 21:38:08 404 GET /favicon.ico</span><br></pre></td></tr></table></figure>

<p>运行之后会弹出网页，但是一直显示 <code>loading</code>，看不到任何从 smart contract 返回的数据。<br><img src="7.png" alt=""></p>
<p>这是因为我们的客户端程序虽然在运行，但是还没有链接到我们创建的 blockchain instance 上。我们需要打开 Ganache，找到本地 in memory blockchain 的 RPC server url address：<br><img src="8.png" alt=""></p>
<p>然后在 Google Chrome(或者 Firefox) 浏览器中 MetaMask 插件里自定义 RPC 链接：<br><img src="9.png" alt=""></p>
<p>之后就可以看到如下界面：<br><img src="10.png" alt=""></p>
<p>点击 vote 以后，可以看到如下结果：<br><img src="11.png" alt=""></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.youtube.com/watch?v=3681ZYbDSSk&list=PLS5SEs8ZftgXXPYBH6rDk4TKnDOvinwJr" target="_blank" rel="noopener">How to Build Ethereum Dapp</a></li>
<li><a href="https://github.com/dappuniversity/election" target="_blank" rel="noopener">dappuniversity/election</a></li>
</ol>
]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>dapp</tag>
        <tag>ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop Installation</title>
    <url>/blog/Hadoop-Installation/</url>
    <content><![CDATA[<h1 id="Hadoop-Single-Node-Installation"><a href="#Hadoop-Single-Node-Installation" class="headerlink" title="Hadoop Single Node Installation"></a>Hadoop Single Node Installation</h1><h2 id="Java-environment-setup"><a href="#Java-environment-setup" class="headerlink" title="Java environment setup"></a>Java environment setup</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install default-jdk</span><br></pre></td></tr></table></figure>
<h2 id="Adding-a-dedicated-Hadoop-system-user"><a href="#Adding-a-dedicated-Hadoop-system-user" class="headerlink" title="Adding a dedicated Hadoop system user"></a>Adding a dedicated Hadoop system user</h2><p>This will add the user hduser and the group hadoop to your local machine.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo addgroup hadoop</span><br><span class="line">sudo adduser --ingroup hadoop hduser</span><br></pre></td></tr></table></figure>

<h2 id="Configuring-SSH"><a href="#Configuring-SSH" class="headerlink" title="Configuring SSH"></a>Configuring SSH</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su hduser</span><br><span class="line">ssh-keygen -t rsa -P <span class="string">""</span></span><br><span class="line">cat <span class="variable">$HOME</span>/.ssh/id_rsa.pub &gt;&gt; <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>The output looks like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hduser@hanson:~$ ssh-keygen -t rsa -P &quot;&quot;</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hduser/.ssh/id_rsa):</span><br><span class="line">Created directory &apos;/home/hduser/.ssh&apos;.</span><br><span class="line">Your identification has been saved in /home/hduser/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hduser/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:VJjkPSKe0LsRjcS+avfbA/kolHR5iPP6OVDp1QaT2EM hduser@hanson</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|     ....*E.     |</span><br><span class="line">|     o.++o*      |</span><br><span class="line">|    ..=.=+o=     |</span><br><span class="line">|     o=**.o.o    |</span><br><span class="line">|     .=OS+ .     |</span><br><span class="line">|      =o=        |</span><br><span class="line">|     o.o +       |</span><br><span class="line">|    o + ooo      |</span><br><span class="line">|   . . +=o..     |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure>

<p><strong>enable SSH access to your local machine with this newly created key.</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hduser@hanson:~$ s</span><br></pre></td></tr></table></figure>
<p><strong>test the SSH setup by connecting to your local machine with the <code>hduser</code> user</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hduser@hanson:~$ ssh localhost</span><br></pre></td></tr></table></figure>
<p>You should install <code>ssh</code> first ( sudo apt-get install ssh)</p>
<p>Output should be like this:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">The authenticity of host <span class="string">'localhost (127.0.0.1)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>localhost<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* Documentation:  https://help.ubuntu.com</span></span><br><span class="line"><span class="string">* Management:     https://landscape.canonical.com</span></span><br><span class="line"><span class="string">* Support:        https://ubuntu.com/advantage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">0 packages can be updated.</span></span><br><span class="line"><span class="string">0 updates are security updates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The programs included with the Ubuntu system are free software;</span></span><br><span class="line"><span class="string">the exact distribution terms for each program are described in the</span></span><br><span class="line"><span class="string">individual files in /usr/share/doc/*/copyright.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by</span></span><br><span class="line"><span class="string">applicable law.</span></span><br></pre></td></tr></table></figure>

<h2 id="Disabling-IPv6"><a href="#Disabling-IPv6" class="headerlink" title="Disabling IPv6"></a>Disabling IPv6</h2><p>One problem with IPv6 on Ubuntu is that using <code>0.0.0.0</code> for the various networking-related Hadoop configuration options will result in Hadoop binding to the IPv6 addresses of my Ubuntu box. In my case, I realized that there’s no practical point in enabling IPv6 on a box when you are not connected to any IPv6 network. Hence, I simply disabled IPv6 on my Ubuntu machine. Your mileage may vary.</p>
<p>To disable IPv6 on Ubuntu 10.04 LTS, open <code>/etc/sysctl.conf</code> in the editor of your choice and add the following lines to the end of the file:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># disable ipv6</span></span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br></pre></td></tr></table></figure>
<p>You have to reboot your machine in order to make the changes take effect.</p>
<p>You can check whether IPv6 is enabled on your machine with the following command:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/sys/net/ipv6/conf/all/disable_ipv6</span><br></pre></td></tr></table></figure>
<p>If output is 1, then ipv6 is disabled.</p>
<h3 id="Alternative"><a href="#Alternative" class="headerlink" title="Alternative"></a>Alternative</h3><p>You can also disable IPv6 only for Hadoop as documented in <a href="https://issues.apache.org/jira/browse/HADOOP-3437" target="_blank" rel="noopener">HADOOP-3437</a>. You can do so by adding the following line to <code>hadoop/etc/hadoop/hadoop-env.sh</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_OPTS=-Djava.net.preferIPv4Stack=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="Hadoop-installation"><a href="#Hadoop-installation" class="headerlink" title="Hadoop installation"></a>Hadoop installation</h2><p>Download Hadoop from the <code>Apache Download Mirrors</code> and extract the contents of the Hadoop package to a location of your choice. I picked $HADOOP_HOME. Make sure to change the owner of all the files to the hduser user and hadoop group, for example:</p>
<ul>
<li>wget <a href="http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz" target="_blank" rel="noopener">http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz</a> (download hadoop)</li>
<li>tar -xzvf hadoop-2.7.4.tar.gz (extract compressed file)</li>
<li>sudo mv hadoop-2.7.4 $HADOOP_HOME</li>
<li>sudo chown -R hduser:hadoop hadoop</li>
</ul>
<h2 id="Update-HOME-bashrc"><a href="#Update-HOME-bashrc" class="headerlink" title="Update $HOME/.bashrc"></a>Update $HOME/.bashrc</h2><p>Add the following lines to the <code>end</code> of the <code>$HOME/.bashrc</code> file of user hduser. If you use a shell other than bash, you should of course update its appropriate configuration files instead of <code>.bashrc</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set Hadoop-related environment variables</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_PREFIX=~/Program/hadoop <span class="comment">#This is where your put your hadoop program</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on)</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=$(readlink -f /usr/bin/java | sed <span class="string">"s:bin/java::"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some convenient aliases and functions for running Hadoop-related commands</span></span><br><span class="line"><span class="built_in">unalias</span> fs &amp;&gt; /dev/null</span><br><span class="line"><span class="built_in">alias</span> fs=<span class="string">"hadoop fs"</span></span><br><span class="line"><span class="built_in">unalias</span> hls &amp;&gt; /dev/null</span><br><span class="line"><span class="built_in">alias</span> hls=<span class="string">"fs -ls"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If you have LZO compression enabled in your Hadoop cluster and</span></span><br><span class="line"><span class="comment"># compress job outputs with LZOP (not covered in this tutorial):</span></span><br><span class="line"><span class="comment"># Conveniently inspect an LZOP compressed file from the command</span></span><br><span class="line"><span class="comment"># line; run via:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># $ lzohead /hdfs/path/to/lzop/compressed/file.lzo</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Requires installed 'lzop' command.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="function"><span class="title">lzohead</span></span> () &#123;</span><br><span class="line">hadoop fs -cat <span class="variable">$1</span> | lzop -dc | head -1000 | less</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add Hadoop bin/ directory to PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<p>Then you should run ‘source .bashrc’ to enable the new configuration.</p>
<h2 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h2><p>The only required environment variable we have to configure for Hadoop in this tutorial is <code>JAVA_HOME</code>. Open <code>hadoop/etc/hadoop/hadoop-env.sh</code></p>
<ul>
<li><p>readlink -f /usr/bin/java | sed “s:bin/java::” (find the default Java path)</p>
</li>
<li><p>sudo vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $HADOOP_HOME/etc/hadoop/hadoop-env.sh</span></span><br><span class="line"><span class="comment">#export JAVA_HOME=$&#123;JAVA_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=$(readlink -f /usr/bin/java | sed <span class="string">"s:bin/java::"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>$HADOOP_HOME/bin/hadoop</code> or <code>hadoop</code>(run hadoop, following appears, then hadoop is installed correctly)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">CLASSNAME            run the class named CLASSNAME</span><br><span class="line">or</span><br><span class="line"><span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">fs                   run a generic filesystem user client</span><br><span class="line">version              <span class="built_in">print</span> the version</span><br><span class="line">jar &lt;jar&gt;            run a jar file</span><br><span class="line">note: please use <span class="string">"yarn jar"</span> to launch</span><br><span class="line">YARN applications, not this <span class="built_in">command</span>.</span><br><span class="line">checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">classpath            prints the class path needed to get the</span><br><span class="line">credential           interact with credential providers</span><br><span class="line">Hadoop jar and the required libraries</span><br><span class="line">daemonlog            get/<span class="built_in">set</span> the <span class="built_in">log</span> level <span class="keyword">for</span> each daemon</span><br></pre></td></tr></table></figure>
<p>You can repeat this exercise also for other users who want to use Hadoop.</p>
</li>
</ul>
<p>Create a directory called input in our home directory and copy Hadoop’s configuration files into it to use those files as our data.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir ~/input</span><br><span class="line">cp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/*.xml ~/input</span><br></pre></td></tr></table></figure>

<p>run cmd:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar grep ~/input ~/grep_example <span class="string">'principal[.]*'</span></span><br></pre></td></tr></table></figure>
<p>You can see output like following:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Output</span><br><span class="line">. . .</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes <span class="built_in">read</span>=1247674</span><br><span class="line">FILE: Number of bytes written=2324248</span><br><span class="line">FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=2</span><br><span class="line">Map output records=2</span><br><span class="line">Map output bytes=37</span><br><span class="line">Map output materialized bytes=47</span><br><span class="line">Input split bytes=114</span><br><span class="line">Combine input records=0</span><br><span class="line">Combine output records=0</span><br><span class="line">Reduce input groups=2</span><br><span class="line">Reduce shuffle bytes=47</span><br><span class="line">Reduce input records=2</span><br><span class="line">Reduce output records=2</span><br><span class="line">Spilled Records=4</span><br><span class="line">Shuffled Maps =1</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=1</span><br><span class="line">GC time elapsed (ms)=61</span><br><span class="line">Total committed heap usage (bytes)=263520256</span><br><span class="line">Shuffle Errors</span><br><span class="line">BAD_ID=0</span><br><span class="line">CONNECTION=0</span><br><span class="line">IO_ERROR=0</span><br><span class="line">WRONG_LENGTH=0</span><br><span class="line">WRONG_MAP=0</span><br><span class="line">WRONG_REDUCE=0</span><br><span class="line">File Input Format Counters</span><br><span class="line">Bytes Read=151</span><br><span class="line">File Output Format Counters</span><br><span class="line">Bytes Written=37</span><br></pre></td></tr></table></figure>

<p>If you run cmd:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ~/grep_example/*</span><br></pre></td></tr></table></figure>

<p>You will see:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Output</span><br><span class="line">6       principal</span><br><span class="line">1       principal.</span><br></pre></td></tr></table></figure>
<p>But if you run this test again, it will give some error. Don’t worry, just delete the <code>grep_example</code> folder then everything will work fine.</p>
<h2 id="Hadoop-Pseudo-distributed-Mode"><a href="#Hadoop-Pseudo-distributed-Mode" class="headerlink" title="Hadoop Pseudo-distributed Mode"></a>Hadoop Pseudo-distributed Mode</h2><h3 id="HDFS-Configuration"><a href="#HDFS-Configuration" class="headerlink" title="HDFS Configuration"></a>HDFS Configuration</h3><p>In this section, we will configure the directory where Hadoop will store its data files, the network ports it listens to, etc. Our setup will use Hadoop’s Distributed File System, <code>HDFS</code>, even though our little “cluster” only contains our single local machine.</p>
<p>You can leave the settings below “as is” with the exception of the <code>hadoop.tmp.dir</code> parameter – this parameter you must change to a directory of your choice. We will use the directory <code>/app/hadoop/tmp</code> in this tutorial. Hadoop’s default configurations use <code>hadoop.tmp.dir</code> as the base temporary directory both for the local file system and HDFS, so don’t be surprised if you see Hadoop creating the specified directory automatically on HDFS at some later point.</p>
<p>Now we create the directory and set the required ownerships and permissions:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p ~/Program/hadoop/tmp</span><br><span class="line">sudo chown hduser:hadoop ~/Program/hadoop/tmp</span><br><span class="line"><span class="comment"># ...and if you want to tighten up security, chmod from 755 to 750...</span></span><br><span class="line">sudo chmod 750 ~/Program/hadoop/tmp</span><br></pre></td></tr></table></figure>

<p>Add the following snippets between the <configuration> … </configuration> tags in the respective configuration XML file.</p>
<p>HDFS is the distributed file system used by Hadoop to store data in the cluster, capable of hosting very very (very) large files, splitting them over the nodes of the cluster. Theoretically, you don’t need to have it running and files could instead be stored elsewhere like S3 or even the local file system (if using a purely local Hadoop installation). However, some applications require interactions with HDFS so you may have to set it up sooner or later if you’re using third party modules. HDFS is composed of a<code>NameNode</code> which holds all the metadata regarding the stored files, and <code>DataNodes</code> (one per node in the cluster) which hold the actual data.</p>
<p>The main HDFS configuration file is located at <code>$HADOOP_PREFIX/etc/hadoop/hdfs-site.xml</code>. If you’ve been following since the beginning, this file should be empty so it will use the default configurations outlined in <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">this page</a>. For a single-node installation of HDFS you’ll want to change <code>hdfs-site.xml</code> to have, at the very least, the following:</p>
<p>First you should create <code>dfs</code> folder, <code>datanode</code> &amp; <code>namenode</code> folder under <code>tmp</code> folder.<br>tmp<br> |– dfs<br>   |– datanode<br>   |– namenode</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/osboxes/Program/hadoop/tmp/dfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/osboxes/Program/hadoop/tmp/dfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Path on the local filesystem where the NameNode stores the namespace and transaction logs persistently.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Default block replication.</span><br><span class="line">The actual number of replications can be specified when the file is created.</span><br><span class="line">The default is used if replication is not specified in create time.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>In addition, add the following to <code>$HADOOP_PREFIX/etc/hadoop/core-site.xml</code> to let the Hadoop modules know where the <code>HDFS NameNode</code> is located.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/osboxes/Program/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>NameNode URI<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><code>Note</code>:<br>If you configured <code>core-site.xml</code>, then the bellow example testing will fail.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This test will fail in to connection refuse</span></span><br><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar grep ~/input ~/grep_example <span class="string">'principal[.]*'</span></span><br></pre></td></tr></table></figure>

<h3 id="YARN-on-a-Single-Node"><a href="#YARN-on-a-Single-Node" class="headerlink" title="YARN on a Single Node"></a>YARN on a Single Node</h3><p>YARN is the component responsible for allocating containers to run tasks, coordinating the execution of said tasks, restart them in case of failure, among other housekeeping. Just like HDFS, it also has 2 main components: a ResourceManager which keeps track of the cluster resources and NodeManagers in each of the nodes which communicates with the ResourceManager and sets up containers for execution of tasks.</p>
<p>You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.</p>
<p>Configure parameters as follows: etc/hadoop/<code>mapred-site.xml:</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p> <code>$HADOOP_PREFIX/etc/hadoop/yarn-site.xml</code>. The file should currently be empty which means it’s using the default configurations you can find <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">here</a>. For a single-node installation of YARN you’ll want to add the following to that file:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;128&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Minimum <span class="built_in">limit</span> of memory to allocate to each container request at the Resource Manager.&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Maximum <span class="built_in">limit</span> of memory to allocate to each container request at the Resource Manager.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;The minimum allocation <span class="keyword">for</span> every container request at the RM, <span class="keyword">in</span> terms of virtual CPU cores. Requests lower than this won<span class="string">'t take effect, and the specified value will get allocated the minimum.&lt;/description&gt;</span></span><br><span class="line"><span class="string">    &lt;/property&gt;</span></span><br><span class="line"><span class="string">    &lt;property&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span></span><br><span class="line"><span class="string">        &lt;value&gt;2&lt;/value&gt;</span></span><br><span class="line"><span class="string">        &lt;description&gt;The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this won'</span>t take effect, and will get capped to this value.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Physical memory, <span class="keyword">in</span> MB, to be made available to running containers&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Number of CPU cores that can be allocated <span class="keyword">for</span> containers.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Starting"><a href="#Starting" class="headerlink" title="Starting"></a>Starting</h1><p>Now that we’ve finished configuring everything, it’s time to setup the folders and start the daemons:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Start HDFS daemons</span></span><br><span class="line"><span class="comment"># Format the namenode directory (DO THIS ONLY ONCE, THE FIRST TIME)</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format</span><br><span class="line"><span class="comment"># Start the namenode daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="comment"># Start the datanode daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh start datanode</span><br><span class="line"><span class="comment">## Start YARN daemons</span></span><br><span class="line"><span class="comment"># Start the resourcemanager daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="comment"># Start the nodemanager daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Format the namenode directory (DO THIS ONLY ONCE, THE FIRST TIME)</span><br><span class="line">$HADOOP_PREFIX/bin/hdfs namenode -format</span><br><span class="line"># Start NameNode daemon and DataNode daemon, The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs).</span><br><span class="line">$HADOOP_PREFIX/sbin/start-dfs.sh</span><br><span class="line">$HADOOP_PREFIX/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>Hopefully, everything should be running. Use the command <code>jps</code> to see if all daemons are launched. If one is missing, check <code>$HADOOP_PREFIX/logs/&lt;daemon with problems&gt;.log</code> for any errors.</p>
<p>The output looks like this:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hduser@hanson:<span class="variable">$HADOOP_HOME</span>/etc/hadoop$ jps</span><br><span class="line">7890 DataNode</span><br><span class="line">16585 SecondaryNameNode</span><br><span class="line">12722 ResourceManager</span><br><span class="line">13013 NodeManager</span><br><span class="line">13126 Jps</span><br><span class="line">7703 NameNode</span><br></pre></td></tr></table></figure>

<p>Make the HDFS directories required to execute MapReduce jobs:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bin/hdfs dfs -mkdir /user</span><br><span class="line">$ bin/hdfs dfs -mkdir /user/&lt;username&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><p>To test if everything is working ok, lets run one of the example applications shipped with Hadoop called DistributedShell. This application spawns a specified number of containers and runs a shell command in each of them. Lets run DistributedShell with the ‘date’ command which outputs the current time:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Run Distributed shell with 2 containers and executing the script `date`.</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hadoop jar <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar org.apache.hadoop.yarn.applications.distributedshell.Client --jar <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar --shell_command date --num_containers 2 --master_memory 1024</span><br></pre></td></tr></table></figure>

<p>The output looks like this:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Initializing Client</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Running Client</span><br><span class="line">17/11/26 11:20:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Got Cluster metric info from ASM, numNodeManagers=1</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Got Cluster node info from ASM</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Got node report from ASM <span class="keyword">for</span>, nodeId=hanson:35765, nodeAddresshanson:8042, nodeRackName/default-rack, nodeNumContainers0</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Queue info, queueName=default, queueCurrentCapacity=0.0, queueMaxCapacity=1.0, queueApplicationCount=0, queueChildQueueCount=0</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: User ACL Info <span class="keyword">for</span> Queue, queueName=root, userAcl=SUBMIT_APPLICATIONS</span><br><span class="line">...</span><br><span class="line">17/11/26 11:20:32 INFO distributedshell.Client: Got application report from ASM <span class="keyword">for</span>, appId=1, clientToAMToken=null, appDiagnostics=, appMasterHost=hanson/10.211.55.3, appQueue=default, appMasterRpcPort=-1, appStartTime=1511716821975, yarnAppState=FINISHED, distributedFinalState=SUCCEEDED, appTrackingUrl=http://hanson:8088/proxy/application_1511716475099_0001/, appUser=hduser</span><br><span class="line">17/11/26 11:20:32 INFO distributedshell.Client: Application has completed successfully. Breaking monitoring loop</span><br><span class="line">17/11/26 11:20:32 INFO distributedshell.Client: Application completed successfully</span><br></pre></td></tr></table></figure>
<p>With this command we are telling hadoop to run the <code>Client class</code> in the <code>hadoop-yarn-applications-distributedshell-2.7.4.jar</code>, passing it the jar containing the definition of the ApplicationMaster (the same jar), the shell command to run in each of the <code>hosts (date)</code>, the number of containers to spawn (2) and the memory used by the ApplicationMaster (1024MB). The value of 1024 was set empirically by trying to run the program several times until it stopped failing due to the ApplicationMaster using more memory than that which had been allocated to it. You can <code>check the entire set of parameters</code> you can pass to DistributedShell by using the same command <code>without any arguments</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check the parameters for the DistributedShell client.</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hadoop jar <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar org.apache.hadoop.yarn.applications.distributedshell.Client</span><br></pre></td></tr></table></figure>
<p>The output should look like this:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">17/11/26 11:29:46 INFO distributedshell.Client: Initializing Client</span><br><span class="line">No jar file specified <span class="keyword">for</span> application master</span><br><span class="line">usage: Client</span><br><span class="line">-appname &lt;arg&gt;                                 Application Name. Default</span><br><span class="line">value - DistributedShell</span><br><span class="line">-attempt_failures_validity_interval &lt;arg&gt;      when</span><br><span class="line">attempt_failures_validity_</span><br><span class="line">interval <span class="keyword">in</span> milliseconds</span><br><span class="line">is <span class="built_in">set</span> to &gt; 0,the failure</span><br><span class="line">number will not take</span><br><span class="line">failures <span class="built_in">which</span> happen out</span><br><span class="line">of the validityInterval</span><br><span class="line">into failure count. If</span><br><span class="line">failure count reaches to</span><br><span class="line">maxAppAttempts, the</span><br><span class="line">application will be</span><br><span class="line">failed.</span><br><span class="line">...</span><br></pre></td></tr></table></figure>


<h1 id="Hadoop-Web-Interfaces"><a href="#Hadoop-Web-Interfaces" class="headerlink" title="Hadoop Web Interfaces"></a>Hadoop Web Interfaces</h1><h2 id="Web-UIs-for-the-Common-User"><a href="#Web-UIs-for-the-Common-User" class="headerlink" title="Web UIs for the Common User"></a>Web UIs for the Common User</h2><hr>
<p>The default Hadoop ports are as follows:</p>
<table>
<thead>
<tr>
<th>Daemon</th>
<th>Default Port</th>
<th>Configuration Parameter</th>
</tr>
</thead>
<tbody><tr>
<td>——HDFS——-</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Namenode</td>
<td>50070</td>
<td>dfs.http.address</td>
</tr>
<tr>
<td>Datanodes</td>
<td>50075</td>
<td>dfs.datanode.http.address</td>
</tr>
<tr>
<td>Secondarynamenode</td>
<td>50090</td>
<td>dfs.secondary.http.address</td>
</tr>
<tr>
<td>Backup/Checkpoint node?</td>
<td>50105</td>
<td>dfs.backup.http.address</td>
</tr>
<tr>
<td>—-MapReduce—-</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Jobracker</td>
<td>50030</td>
<td>mapred.job.tracker.http.address</td>
</tr>
<tr>
<td>Tasktrackers</td>
<td>50060</td>
<td>mapred.task.tracker.http.address</td>
</tr>
</tbody></table>
<p>Hadoop daemons expose some information over HTTP. All Hadoop daemons expose the following:</p>
<ul>
<li><code>/logs</code>:<br>Exposes, for downloading, log files in the Java system property <strong>hadoop.log.dir</strong>.</li>
<li><code>/logLevel</code>:<br>Allows you to dial up or down <strong>log4j</strong> logging levels. This is similar to <strong>hadoop daemonlog</strong> on the command line.</li>
<li><code>/stacks</code>:<br>Stack traces for all threads. Useful for debugging.</li>
<li><code>/metrics</code>:<br>Metrics for the server. Use /metrics?format=json to retrieve the data in a structured form. <code>Available in 0.21</code>.</li>
</ul>
<p>Individual daemons expose extra daemon-specific endpoints as well. Note that these are not necessarily part of Hadoop’s public API, so they tend to change over time.</p>
<p>The <code>Namenode</code> exposes:</p>
<ul>
<li><code>/</code>:<br>Shows information about the namenode as well as the HDFS. There’s a link from here to browse the filesystem, as well.</li>
<li><code>/dfsnodelist.jsp?whatNodes=(DEAD|LIVE)</code>:<br>Shows lists of nodes that are disconnected from (DEAD) or connected to (LIVE) the namenode.</li>
<li><code>/fsck</code>:<br>Runs the “fsck” command. Not recommended on a busy cluster.</li>
<li><code>/listPaths</code>:<br>Returns an XML-formatted directory listing. This is useful if you wish (for example) to poll HDFS to see if a file exists. The URL can include a path (e.g., /listPaths/user/philip) and can take optional GET arguments: /listPaths?recursive=yes will return all files on the file system; /listPaths/user/philip?filter=s.* will return all files in the home directory that start with s; and /listPaths/user/philip?exclude=.txt will return all files except text files in the home directory. Beware that filter and exclude operate on the directory listed in the URL, and they ignore the recursive flag.</li>
<li><code>/data</code> and <code>/fileChecksum</code><br>These forward your HTTP request to an appropriate datanode, which in turn returns the data or the checksum.</li>
</ul>
<p><code>Datanodes</code> expose the following:</p>
<ul>
<li><code>/browseBlock.jsp</code>, <code>/browseDirectory.jsp, tail.jsp</code>, <code>/streamFile</code>, <code>/getFileChecksum</code><br>These are the endpoints that the namenode redirects to when you are browsing filesystem content. You probably wouldn’t use these directly, but this is what’s going on underneath.</li>
<li><code>/blockScannerReport</code><br>Every datanode verifies its blocks at configurable intervals. This endpoint provides a listing of that check.</li>
</ul>
<p>The <code>secondarynamenode</code> exposes a simple status page with information including which namenode it’s talking to, when the last checkpoint was, how big it was, and which directories it’s using.</p>
<p>The <code>jobtracker</code>‘s UI is commonly used to look at running jobs, and, especially, to find the causes of failed jobs. The UI is best browsed starting at <code>/jobtracker.jsp</code>. There are over a dozen related pages providing details on tasks, history, scheduling queues, jobs, etc.</p>
<p><code>Tasktrackers</code> have a simple page (<code>/tasktracker.jsp</code>), which shows running tasks. They also expose <code>/taskLog?taskid=</code>to query logs for a specific task. They use <code>/mapOutput</code> to serve the output of map tasks to reducers, but this is an internal API.</p>
<h2 id="Under-the-Covers-for-the-Developer-and-the-System-Administrator"><a href="#Under-the-Covers-for-the-Developer-and-the-System-Administrator" class="headerlink" title="Under the Covers for the Developer and the System Administrator"></a>Under the Covers for the Developer and the System Administrator</h2><hr>
<p>Internally, Hadoop mostly uses Hadoop IPC to communicate amongst servers. (Part of the goal of the Apache Avro project is to replace Hadoop IPC with something that is easier to evolve and more language-agnostic; HADOOP-6170 is the relevant ticket.) Hadoop also uses HTTP (for the secondarynamenode communicating with the namenode and for the tasktrackers serving map outputs to the reducers) and a raw network socket protocol (for datanodes copying around data).</p>
<p>The following table presents the ports and protocols (including the relevant Java class) that Hadoop uses. This table does not include the HTTP ports mentioned above.</p>
<table>
<thead>
<tr>
<th>Daemon</th>
<th>Default Port</th>
<th>Configuration Parameter</th>
<th>Protocol</th>
<th>Used for</th>
</tr>
</thead>
<tbody><tr>
<td>Namenode</td>
<td>8020</td>
<td>fs.default.name</td>
<td>IPC: ClientProtocol</td>
<td>Filesystem metadata operations</td>
</tr>
<tr>
<td>Datanode</td>
<td>50010</td>
<td>dfs.datanode.address</td>
<td>Custom Hadoop Xceiver: DataNode and DFSClient</td>
<td>DFS data transfer</td>
</tr>
<tr>
<td>Datanode</td>
<td>50020</td>
<td>dfs.datanode.ipc.address</td>
<td>IPC: InterDatanodeProtocol, ClientDatanodeProtocolClientProtocol</td>
<td>Block metadata operations and recovery</td>
</tr>
<tr>
<td>Backupnode</td>
<td>50100</td>
<td>dfs.backup.address</td>
<td>Same as namenode</td>
<td>HDFS Metadata Operations</td>
</tr>
<tr>
<td>&gt; This is the port part of hdfs://host:8020/.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Default is not well-defined. Common values are 8021, 9001, or 8012. See <a href="http://issues.apache.org/jira/browse/MAPREDUCE-566" target="_blank" rel="noopener">MAPREDUCE-566</a>.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Binds to an unused local port.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h1 id="Multinode-configuration"><a href="#Multinode-configuration" class="headerlink" title="Multinode configuration"></a>Multinode configuration</h1><h2 id="Node-configure"><a href="#Node-configure" class="headerlink" title="Node configure"></a>Node configure</h2><p>First get every machine’s ip address. Assuming we have 3 machine, 1 master and 2 slaves.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">master: 10.211.55.3</span><br><span class="line">slave1: 10.211.55.6</span><br><span class="line">slave2: 10.211.55.7</span><br></pre></td></tr></table></figure>

<p>Then go to every machine’s <code>/etc/hostname</code> file, change them:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hostname</span><br><span class="line"># change hostname to master, slave1, slave2</span><br></pre></td></tr></table></figure>

<p>Then go to every machine’s <code>/etc/hosts</code> file, add the same content:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.211.55.3 master</span><br><span class="line">10.211.55.6 slave1</span><br><span class="line">10.211.55.7 slave2</span><br></pre></td></tr></table></figure>

<p>Then reboot 3 machine and conform every machine’s hostname:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osboxes@master:~$ hostname</span><br><span class="line">master</span><br><span class="line">osboxes@master:~$ ping slave1</span><br><span class="line">PING slave1 (10.211.55.6) 56(84) bytes of data.</span><br><span class="line">64 bytes from slave1 (10.211.55.6): icmp_seq=1 ttl=64 time=0.466 ms</span><br><span class="line">64 bytes from slave1 (10.211.55.6): icmp_seq=2 ttl=64 time=0.417 ms</span><br><span class="line">64 bytes from slave1 (10.211.55.6): icmp_seq=3 ttl=64 time=0.402 ms</span><br><span class="line"></span><br><span class="line">osboxes@master:~$ ping slave2</span><br><span class="line">PING slave2 (10.211.55.7) 56(84) bytes of data.</span><br><span class="line">64 bytes from slave2 (10.211.55.7): icmp_seq=1 ttl=64 time=0.491 ms</span><br><span class="line">64 bytes from slave2 (10.211.55.7): icmp_seq=2 ttl=64 time=0.336 ms</span><br><span class="line">64 bytes from slave2 (10.211.55.7): icmp_seq=3 ttl=64 time=0.392 ms</span><br></pre></td></tr></table></figure>

<p>Then in every machine, use <code>ssh</code> command to link every other machine: ( This is used to enable ssh without password)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osboxes@master:~$ ssh slave1</span><br><span class="line">The authenticity of host <span class="string">'slave1 (10.211.55.6)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>slave1,10.211.55.6<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* Documentation:  https://help.ubuntu.com</span></span><br><span class="line"><span class="string">* Management:     https://landscape.canonical.com</span></span><br><span class="line"><span class="string">* Support:        https://ubuntu.com/advantage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">21 packages can be updated.</span></span><br><span class="line"><span class="string">0 updates are security updates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Last login: Wed Nov 29 17:08:56 2017 from 127.0.0.1</span></span><br><span class="line"><span class="string">osboxes@slave1:~$ exit</span></span><br><span class="line"><span class="string">logout</span></span><br><span class="line"><span class="string">Connection to slave1 closed.</span></span><br><span class="line"><span class="string">osboxes@master:~$ ssh slave2</span></span><br><span class="line"><span class="string">The authenticity of host '</span>slave2 (10.211.55.7)<span class="string">' can'</span>t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span><br><span class="line">Are you sure you want to <span class="built_in">continue</span> connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added <span class="string">'slave2,10.211.55.7'</span> (ECDSA) to the list of known hosts.</span><br><span class="line">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span><br><span class="line"></span><br><span class="line">* Documentation:  https://help.ubuntu.com</span><br><span class="line">* Management:     https://landscape.canonical.com</span><br><span class="line">* Support:        https://ubuntu.com/advantage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">21 packages can be updated.</span><br><span class="line">0 updates are security updates.</span><br><span class="line"></span><br><span class="line">Last login: Wed Nov 29 17:08:56 2017 from 127.0.0.1</span><br><span class="line">osboxes@slave2:~$ <span class="built_in">exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave2 closed.</span><br><span class="line">osboxes@master:~$ ssh master</span><br><span class="line">The authenticity of host <span class="string">'master (10.211.55.3)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>master,10.211.55.3<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* Documentation:  https://help.ubuntu.com</span></span><br><span class="line"><span class="string">* Management:     https://landscape.canonical.com</span></span><br><span class="line"><span class="string">* Support:        https://ubuntu.com/advantage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">21 packages can be updated.</span></span><br><span class="line"><span class="string">0 updates are security updates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Last login: Wed Nov 29 17:08:56 2017 from 127.0.0.1</span></span><br><span class="line"><span class="string">osboxes@master:~$ exit</span></span><br><span class="line"><span class="string">logout</span></span><br><span class="line"><span class="string">Connection to master closed.</span></span><br><span class="line"><span class="string">osboxes@master:~$</span></span><br></pre></td></tr></table></figure>

<h2 id="Update-core-site-xml-in-all-machine"><a href="#Update-core-site-xml-in-all-machine" class="headerlink" title="Update core-site.xml in all machine"></a>Update core-site.xml in all machine</h2><p>Delete this configure:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:///home/osboxes/Program/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>And change all <code>fs.default</code> property to:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000/&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;NameNode URI&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Update-hdfs-site-xml-in-all-machine"><a href="#Update-hdfs-site-xml-in-all-machine" class="headerlink" title="Update hdfs-site.xml in all machine"></a>Update hdfs-site.xml in all machine</h2><p>in master node, delete <code>datanode</code> property and change <code>dfs.replication</code> to 2:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides <span class="keyword">in</span> this file. --&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file:///home/osboxes/Program/hadoop/tmp/dfs/namenode&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Path on the <span class="built_in">local</span> filesystem <span class="built_in">where</span> the NameNode stores the namespace and transaction logs persistently.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Default block replication.</span><br><span class="line">The actual number of replications can be specified when the file is created.</span><br><span class="line">The default is used <span class="keyword">if</span> replication is not specified <span class="keyword">in</span> create time.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>In the slave node, delete <code>namenode</code> property and change <code>dfs.replication</code> to 2</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file:///home/osboxes/Program/hadoop/tmp/dfs/datanode&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Comma separated list of paths on the <span class="built_in">local</span> filesystem of a DataNode <span class="built_in">where</span> it should store its blocks.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Default block replication.</span><br><span class="line">The actual number of replications can be specified when the file is created.</span><br><span class="line">The default is used <span class="keyword">if</span> replication is not specified <span class="keyword">in</span> create time.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Update-yarn-site-xml-in-all-machine"><a href="#Update-yarn-site-xml-in-all-machine" class="headerlink" title="Update yarn-site.xml in all machine"></a>Update yarn-site.xml in all machine</h2><p>insert the following to all 3 machines.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:8025&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:8030&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:8050&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Update-mapred-site-xml"><a href="#Update-mapred-site-xml" class="headerlink" title="Update mapred-site.xml"></a>Update mapred-site.xml</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Master-or-Slaves-only-configuration"><a href="#Master-or-Slaves-only-configuration" class="headerlink" title="Master or Slaves only configuration"></a>Master or Slaves only configuration</h2><p>edit <code>~/Program/hadoop/etc/hadoop/slaves</code> file (Master only)<br>change to this value:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<p>edit <code>~/Program/hadoop/etc/hadoop/masters</code> file (Master only)<br>change to this value:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">master</span><br></pre></td></tr></table></figure>

<p>Recreate Namenode folder (Master Only):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p ~/Program/hadoop/tmp/dfs/namenode</span><br><span class="line">chown osboxes:osboxes -R ~/Program/hadoop/tmp/</span><br><span class="line">chmod 777 ~/Program/hadoop/tmp/dfs/namenode</span><br></pre></td></tr></table></figure>

<p>Recreate Datanode folder (All slave Nodes only):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p ~/Program/hadoop/tmp/dfs/datanode</span><br><span class="line">chown osboxes:osboxes -R ~/Program/hadoop/tmp/</span><br><span class="line">chmod 777 ~/Program/hadoop/tmp/dfs/datanode</span><br></pre></td></tr></table></figure>

<p>Format the Namenode （Master only）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h2 id="Start-the-DFS-amp-Yarn（Master-only）"><a href="#Start-the-DFS-amp-Yarn（Master-only）" class="headerlink" title="Start the DFS &amp; Yarn（Master only）"></a>Start the DFS &amp; Yarn（Master only）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">osboxes@master:~/Program/hadoop/sbin$ ./start-dfs.sh</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">master: starting namenode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-namenode-master.out</span><br><span class="line">slave2: starting datanode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-datanode-slave2.out</span><br><span class="line">slave1: starting datanode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-datanode-slave1.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-secondarynamenode-master.out</span><br><span class="line">osboxes@master:~/Program/hadoop/sbin$ jps</span><br><span class="line">7316 Jps</span><br><span class="line">6904 NameNode</span><br><span class="line">7180 SecondaryNameNode</span><br><span class="line">osboxes@master:~/Program/hadoop/sbin$ ./start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/osboxes/Program/hadoop/logs/yarn-osboxes-resourcemanager-master.out</span><br><span class="line">slave2: starting nodemanager, logging to /home/osboxes/Program/hadoop/logs/yarn-osboxes-nodemanager-slave2.out</span><br><span class="line">slave1: starting nodemanager, logging to /home/osboxes/Program/hadoop/logs/yarn-osboxes-nodemanager-slave1.out</span><br><span class="line">osboxes@master:~/Program/hadoop/sbin$ jps</span><br><span class="line">7665 Jps</span><br><span class="line">7382 ResourceManager</span><br><span class="line">6904 NameNode</span><br><span class="line">7180 SecondaryNameNode</span><br></pre></td></tr></table></figure>

<p>If you go to your slave machine, then you will see：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osboxes@slave1:~/Program/hadoop/tmp/dfs$ jps</span><br><span class="line">2769 NodeManager</span><br><span class="line">2538 DataNode</span><br><span class="line">2891 Jps</span><br></pre></td></tr></table></figure>

<h2 id="Review-Yarn-console"><a href="#Review-Yarn-console" class="headerlink" title="Review Yarn console"></a>Review Yarn console</h2><p>If all the services started successfully on all nodes, then you should see all of your nodes listed under Yarn nodes. You can hit the following url on your browser and verify that:</p>
<p><a href="http://master:8088/cluster/nodes" target="_blank" rel="noopener">http://master:8088/cluster/nodes</a><br><a href="http://master:50070" target="_blank" rel="noopener">http://master:50070</a></p>
<p>You can also get the report of your cluster by issuing the below commands:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfsadmin -report</span><br></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#installation" target="_blank" rel="noopener">Running Hadoop on Ubuntu Linux (Single-Node Cluster)</a></li>
<li><a href="https://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide/" target="_blank" rel="noopener">Hadoop YARN Installation: The definitive guide</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-16-04" target="_blank" rel="noopener">How to Install Hadoop in Stand-Alone Mode on Ubuntu 16.04</a></li>
<li><a href="http://www.powerxing.com/install-hadoop/" target="_blank" rel="noopener">Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04</a></li>
<li><a href="http://blog.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/" target="_blank" rel="noopener">Hadoop Default Ports Quick Reference</a></li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Hadoop: Setting up a Single Node Cluster.</a></li>
</ul>
]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>mapreudce</tag>
      </tags>
  </entry>
  <entry>
    <title>Shared Libraries 概述</title>
    <url>/blog/shared-libraries-%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<p>最近使用 <code>make</code> 编译代码的时候，遇到了和 Shared Libraries 相关的一些问题，做一下总结。</p>
<h1 id="什么是-Shared-Libraries"><a href="#什么是-Shared-Libraries" class="headerlink" title="什么是 Shared Libraries"></a>什么是 Shared Libraries</h1><p>Shared Libraries 是程序在运行的时候才装载的 libraries，换句话说，就是这些代码并没有编译链接到可执行文件中。</p>
<h1 id="Shared-Libraries-name"><a href="#Shared-Libraries-name" class="headerlink" title="Shared Libraries name"></a>Shared Libraries name</h1><p>Shared Libraries 有下面三种名字：</p>
<h2 id="soname"><a href="#soname" class="headerlink" title="soname"></a>soname</h2><p><code>lib</code>+ <code>library name</code> + <code>.so</code> + <code>period</code> + <code>version number</code><br>比如：libleveldb.so.1.20。在 linux 系统中，soname 是 shared libraries 的 real name 的 symlink。</p>
<figure class="highlight bash"><figcaption><span>:soname示例</span></figcaption><table><tr><td class="code"><pre><span class="line">$ ls -al | grep level</span><br><span class="line">-rw-r--r--  1 root root 789K 2018-04-03 01:42 libleveldb.a</span><br><span class="line">lrwxrwxrwx  1 root root   18 2018-04-03 01:42 libleveldb.so -&gt; libleveldb.so.1.20* <span class="comment"># linker name</span></span><br><span class="line">lrwxrwxrwx  1 root root   18 2018-04-03 01:42 libleveldb.so.1 -&gt; libleveldb.so.1.20* <span class="comment"># soname</span></span><br><span class="line">-rwxr-xr-x  1 root root 459K 2018-04-03 01:42 libleveldb.so.1.20* <span class="comment"># real name</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p>可以看到 <code>libleveldb.so</code> 和 <code>libleveldb.so.1</code>都是 <code>libleveldb.so.1.20*</code> 的 symlink。从文件大小就能看出不同。</p>
<h2 id="real-name"><a href="#real-name" class="headerlink" title="real name"></a>real name</h2><p>每一个 shared library 都有一个 <code>real name</code>，real name 下的文件才是真正包含源代码的文件。real name 会在 soname 后面添加 period，minor number，[another period，release number]，后面两个是 optional 的。</p>
<h2 id="linker-name"><a href="#linker-name" class="headerlink" title="linker name"></a>linker name</h2><p>就是不包含任何 version number 的 soname。这个就是我们在 compile 的时候用到到的 name。</p>
<p>管理 shared library 的关键在于区分好这些 name。程序应该只列出它们想要用到的 soname。当我们创建一个 shared library 后，应该给它命名一个包含 version information 的 name。然后将创建的 shared library 安装（复制）到指定目录（比如 <code>/usr/local/lib</code> <code>/usr/lib</code> ），然后运行 <code>ldconfig</code> 命令。 ldconfig 会检测目录中已存在的files，并给他们的 real name 创建对应的 soname。并设置 cache file: <code>/etc/ld.so.chache</code>。</p>
<p><code>ldconfig</code> 并不会设置 linker name。linker name 的创建一般在 library installation 的时候，并且 linker name 只是对最新的 soname 或者 real name 创建一个 symlink。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://www.tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html" target="_blank" rel="noopener">Program Library HOWTO</a></li>
</ol>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title>google s2 库安装配置</title>
    <url>/blog/google-s2-%E5%BA%93%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>安装 openssl1.0.2 版本。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --prefix 用来指定 openssl 要被安装在哪里</span></span><br><span class="line">./config shared --prefix=/home/hanson/program/usr</span><br><span class="line">make -j32</span><br></pre></td></tr></table></figure>

<p><a href="https://github.com/google/s2geometry" target="_blank" rel="noopener">安装 s2</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake ..</span><br><span class="line"><span class="comment"># 如果想手动指定 openssl 的路径，使用如下命令。这是因为当前版本(2018/04/01)的 s2 </span></span><br><span class="line"><span class="comment"># openssl 只支持到 1.0.2。如果使用最新1.1.0版本会编译出错。</span></span><br><span class="line">OPENSSL_ROOT_DIR=/home/hanson/program/usr/  cmake ..</span><br><span class="line">sudo make install <span class="comment"># 会将 libs2.so share library 安装到 /usr/local/lib 中，头文件放入 /usr/local/include/s2</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
]]></content>
      <categories>
        <category>database</category>
      </categories>
  </entry>
  <entry>
    <title>python在一张图中使用两组y坐标</title>
    <url>/blog/python%E5%9C%A8%E4%B8%80%E5%BC%A0%E5%9B%BE%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%B8%A4%E7%BB%84y%E5%9D%90%E6%A0%87/</url>
    <content><![CDATA[<p>少说闲话直接上代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">csv_file = file_path_name</span><br><span class="line">df = pd.read_csv(csv_file,header=<span class="literal">None</span>)</span><br><span class="line">df.columns = [[<span class="string">'max cell'</span>,<span class="string">'avg cell'</span>,<span class="string">'time'</span>]]</span><br><span class="line">df[<span class="string">'index'</span>] = range(len(df))</span><br><span class="line">fig, ax0 = plt.subplots()</span><br><span class="line">ax1 = ax0.twinx()</span><br><span class="line"></span><br><span class="line">df.plot(x=[<span class="string">'index'</span>], <span class="comment"># 设置 x 轴值域</span></span><br><span class="line">        y=[<span class="string">'max cell'</span>,<span class="string">'avg cell'</span>], <span class="comment"># 设置 y 轴值域</span></span><br><span class="line">        ax=ax0, <span class="comment"># 指定在哪套 y 坐标画</span></span><br><span class="line">        color=[<span class="string">'b'</span>,<span class="string">'g'</span>], <span class="comment"># 指定颜色</span></span><br><span class="line">       )</span><br><span class="line">df.plot(x=[<span class="string">'index'</span>],</span><br><span class="line">        y=[<span class="string">'time'</span>],</span><br><span class="line">        ax=ax1,</span><br><span class="line">        style=<span class="string">'r--'</span> <span class="comment"># 指定 style</span></span><br><span class="line">       )</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------ 下面的操作将两个图的 legend 整合成一个 -------------</span></span><br><span class="line"><span class="comment"># 提取出图中的 lines 和 labels</span></span><br><span class="line">lines0, labels0 = ax0.get_legend_handles_labels()</span><br><span class="line">lines1, labels1 = ax1.get_legend_handles_labels()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 ax0 上的 legend </span></span><br><span class="line">ax0.legend_.remove()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 重命名 legend，并添加整合以后的 legend 到图中</span></span><br><span class="line">labels = [<span class="string">"Max cell #"</span>, <span class="string">"Avg cell #"</span>, <span class="string">"Time"</span>]</span><br><span class="line">ax1.legend(lines0 + lines1, <span class="comment"># 整合所有的 line</span></span><br><span class="line">           labels, <span class="comment"># 设置新的 labels</span></span><br><span class="line">           loc=<span class="number">4</span>,  <span class="comment"># 设置 legend 的位置，1 从右上角开始，逆时针算位置</span></span><br><span class="line">          )</span><br><span class="line">          </span><br><span class="line"><span class="comment"># 设置图表的两套 y 轴标题和公用的 x 轴标题</span></span><br><span class="line">ax0.set_xlabel(<span class="string">"Max cell setting"</span>)</span><br><span class="line">ax0.set_ylabel(<span class="string">"Cell number"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cover time(ms)"</span>)</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p><img src="1.png" alt=""></p>
<p><code>loc</code> 位置参考如下：</p>
<table>
<thead>
<tr>
<th>Location String</th>
<th align="center">Location Code</th>
</tr>
</thead>
<tbody><tr>
<td>‘best’</td>
<td align="center">0</td>
</tr>
<tr>
<td>‘upper right’</td>
<td align="center">1</td>
</tr>
<tr>
<td>‘upper left’</td>
<td align="center">2</td>
</tr>
<tr>
<td>‘lower left’</td>
<td align="center">3</td>
</tr>
<tr>
<td>‘lower right’</td>
<td align="center">4</td>
</tr>
<tr>
<td>‘right’</td>
<td align="center">5</td>
</tr>
<tr>
<td>‘center left’</td>
<td align="center">6</td>
</tr>
<tr>
<td>‘center right’</td>
<td align="center">7</td>
</tr>
<tr>
<td>‘lower center’</td>
<td align="center">8</td>
</tr>
<tr>
<td>‘upper center’</td>
<td align="center">9</td>
</tr>
<tr>
<td>‘center’</td>
<td align="center">10</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pyplot</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux批量重命名文件</title>
    <url>/blog/Linux%E6%89%B9%E9%87%8F%E9%87%8D%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>在当 TA 时候，要给本科生改 program 作业，批量下载的文件名又长又臭，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-rw-r--r--  1 mac  staff   4.0K Mar 27 13:36 Program 1_and7697_attempt_2018-03-22-16-56-45_and7697_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   1.3K Mar 27 13:36 Program 1_axg6672_attempt_2018-03-20-14-13-01_axg6672_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   799B Mar 27 13:36 Program 1_axk5863_attempt_2018-03-21-23-31-29_axk5863_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   797B Mar 27 13:36 Program 1_axr8361_attempt_2018-03-21-22-27-28_axr8361_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   1.1K Mar 27 13:36 Program 1_bxk5485_attempt_2018-03-21-21-19-30_bxk5485_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   1.7K Mar 27 13:36 Program 1_cra5824_attempt_2018-03-19-09-43-14_cra5824_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   1.7K Mar 27 13:36 Program 1_cxx4741_attempt_2018-03-21-21-13-51_cxx4741_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   1.5K Mar 27 13:36 Program 1_daa5782_attempt_2018-03-21-16-23-26_daa5782_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   4.8K Mar 27 13:36 Program 1_dbp4110_attempt_2018-03-16-23-09-09_dbp4110_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   1.9K Mar 27 13:36 Program 1_dtn5102_attempt_2018-03-20-20-29-06_dtn5102-p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   2.1K Mar 27 13:36 Program 1_eep5180_attempt_2018-03-21-23-23-13_eep5180_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   3.6K Mar 27 13:36 Program 1_egc8644_attempt_2018-03-23-22-02-19_egc8644_p1.s</span><br><span class="line">-rw-r--r--  1 mac  staff   969B Mar 27 13:36 Program 1_exg6686_attempt_2018-03-21-22-30-05_exgg6686_p1.s</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>所以就想把他们重命名一下，变成只有 <code>xxx1111.s</code> 的形式。查找了一些 Linux 下批量重命名的方法，总结如下：</p>
<a id="more"></a>

<h1 id="cut-命令"><a href="#cut-命令" class="headerlink" title="cut 命令"></a>cut 命令</h1><p>这里使用 <code>cut</code> 命令截取文件名中想要的一段字符，然后再尾部添加 <code>.s</code> 共同构成新的文件名。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> *.s</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    newname=<span class="string">"<span class="variable">$(echo "$name" | cut -c 11-17)</span>"</span>.s</span><br><span class="line">    mv <span class="string">"<span class="variable">$name</span>"</span>  <span class="string">"<span class="variable">$newname</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>怎样快速读取文件</title>
    <url>/blog/%E6%80%8E%E6%A0%B7%E5%BF%AB%E9%80%9F%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>首先使用 mmap 映射文件到内存中，然后返回该文件对应的 memory 首地址。 char* data。</p>
<p>然后对该 char* 类型 构造一个 ifstream（不复制 data 内容到 ifstream 中）。</p>
<p>代码如下：</p>
<a id="more"></a>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdint.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/mman.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;streambuf&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;istream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;numeric&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">membuf</span>:</span> <span class="built_in">std</span>::streambuf &#123;</span><br><span class="line">    membuf(<span class="keyword">char</span> <span class="keyword">const</span>* base, <span class="keyword">size_t</span> size) &#123;</span><br><span class="line">        <span class="function"><span class="keyword">char</span>* <span class="title">p</span><span class="params">(<span class="keyword">const_cast</span>&lt;<span class="keyword">char</span>*&gt;(base))</span></span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;setg(p, p, p + size);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">imemstream</span>:</span> <span class="keyword">virtual</span> membuf, <span class="built_in">std</span>::istream &#123;</span><br><span class="line">    imemstream(<span class="keyword">char</span> <span class="keyword">const</span>* base, <span class="keyword">size_t</span> size)</span><br><span class="line">        : membuf(base, size)</span><br><span class="line">        , <span class="built_in">std</span>::istream(<span class="keyword">static_cast</span>&lt;<span class="built_in">std</span>::streambuf*&gt;(<span class="keyword">this</span>)) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> fd = open(<span class="string">"../XXXX"</span>, O_RDONLY, (<span class="keyword">mode_t</span>)<span class="number">0600</span>);</span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Error opening file for writing"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;        </span><br><span class="line">    </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">stat</span> <span class="title">fileInfo</span> = &#123;</span><span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">if</span> (fstat(fd, &amp;fileInfo) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        perror(<span class="string">"Error getting the file size"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (fileInfo.st_size == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Error: File is empty, nothing to do\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"File size is %ji\n"</span>, (<span class="keyword">intmax_t</span>)fileInfo.st_size);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">char</span> *<span class="built_in">map</span> = (<span class="keyword">char</span>*)mmap(<span class="number">0</span>, fileInfo.st_size, PROT_READ, MAP_SHARED, fd, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">map</span> == MAP_FAILED)</span><br><span class="line">    &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        perror(<span class="string">"Error mmapping the file"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> inserted = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取数据的开始</span></span><br><span class="line">    <span class="built_in">string</span> line;</span><br><span class="line">    <span class="function">imemstream <span class="title">in</span><span class="params">(<span class="built_in">map</span>, fileInfo.st_size)</span></span>;</span><br><span class="line">    <span class="keyword">auto</span> started = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</span><br><span class="line">    <span class="comment">// 这里遇到空格和换行都会当做一个 string 的结束</span></span><br><span class="line">    <span class="keyword">while</span>(in &gt;&gt; line &amp;&amp; inserted &lt; <span class="number">100000</span>) &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; line &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">auto</span> done = <span class="built_in">std</span>::chrono::high_resolution_clock::now();</span><br><span class="line">    <span class="keyword">double</span> time_in_ms = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::milliseconds&gt;(done-started).count();</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Total "</span> &lt;&lt; time_in_ms &lt;&lt;<span class="string">" ms"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Don't forget to free the mmapped memory</span></span><br><span class="line">    <span class="keyword">if</span> (munmap(<span class="built_in">map</span>, fileInfo.st_size) == <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        perror(<span class="string">"Error un-mmapping the file"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Un-mmaping doesn't close the file, so we still need to do that.</span></span><br><span class="line">    close(fd);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://insanecoding.blogspot.com/2011/11/how-to-read-in-file-in-c.html" target="_blank" rel="noopener">How to read in a file in C++</a></li>
<li><a href="https://www.byvoid.com/zhs/blog/fast-readfile" target="_blank" rel="noopener">探寻C++最快的读取文件的方案</a></li>
<li><a href="https://stackoverflow.com/questions/13059091/creating-an-input-stream-from-constant-memory" target="_blank" rel="noopener">Creating an input stream from constant memory</a></li>
<li><a href="https://gist.github.com/marcetcheverry/991042" target="_blank" rel="noopener">mapread.c</a></li>
</ol>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Pymongo 试验</title>
    <url>/blog/Pymongo-%E8%AF%95%E9%AA%8C/</url>
    <content><![CDATA[<p>首先将 geojson 数据导入到 MongoDB 中，具体操作<a href="https://stackoverflow.com/questions/22029114/how-to-import-geojson-file-to-mongodb" target="_blank" rel="noopener">参考如下</a>。</p>
<p>需要使用两条命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jq --compact-output <span class="string">".features"</span> input.geojson &gt; output.geojson</span><br><span class="line">mongoimport --db dbname -c collectionname --file <span class="string">"output.geojson"</span> --jsonArray</span><br></pre></td></tr></table></figure>
<p>通过上述命令，我导入了 nyu yellow taxi 的 json 数据。该 json 数据是经过后期处理的，格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;type&quot;: &quot;FeatureCollection&quot;,</span><br><span class="line">    &quot;features&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;Feature&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;place&quot;: &quot;nyu&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;geometry&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;Point&quot;,</span><br><span class="line">                &quot;coordinates&quot;: [</span><br><span class="line">                    -73.98268127441406,</span><br><span class="line">                    40.731311798095696</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;Feature&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;place&quot;: &quot;nyu&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;geometry&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;Point&quot;,</span><br><span class="line">                &quot;coordinates&quot;: [</span><br><span class="line">                    -73.99224853515625,</span><br><span class="line">                    40.74908065795898</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p>比如我想要获得 nyu 南边一个圆里面的所有 taxi 信息。<br><img src="1.png" alt=""></p>
<p>find 请求如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;geometry: &#123;$geoWithin: &#123; $centerSphere: [ [ -74.0093912556225, 40.7094698471372 ], 0.00008092516322998259 ]&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>而改写到 pymongo 里面则是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> pprint <span class="keyword">as</span> pp</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># Replace XXXX with your connection URI from the Atlas UI</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mongodb:// is the protocol definition</span></span><br><span class="line"><span class="comment"># localhost:27017 is the server we are connecting to</span></span><br><span class="line"><span class="comment"># /myproject is the database we wish to connect to</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">'mongodb://localhost:27017/m201'</span></span><br><span class="line">client = pymongo.MongoClient(url)</span><br><span class="line">taxi = client[<span class="string">'m201'</span>][<span class="string">'nyu_taxi'</span>]</span><br><span class="line"></span><br><span class="line">query = &#123;</span><br><span class="line">    <span class="string">"geometry"</span>: &#123;</span><br><span class="line">        <span class="string">"$geoWithin"</span>: &#123;</span><br><span class="line">            <span class="string">"$centerSphere"</span>: [</span><br><span class="line">                [ <span class="number">-74.0093912556225</span>, <span class="number">40.7094698471372</span> ],</span><br><span class="line">                <span class="number">0.00008092516322998259</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">projection = &#123;<span class="string">"_id"</span>:<span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">points = &#123;&#125;</span><br><span class="line">points[<span class="string">'type'</span>] = <span class="string">'FeatureCollection'</span></span><br><span class="line">points[<span class="string">'features'</span>] = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> taxi.find(query,projection):</span><br><span class="line"><span class="comment">#     print(t)</span></span><br><span class="line">    points[<span class="string">'features'</span>].append(t)</span><br><span class="line"></span><br><span class="line">pp.pprint(points)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save as geojson format</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./filter_nyu_taxi.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> outfile:  </span><br><span class="line">    json.dump(points,outfile, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>S2 计算给定点到哪条边最近</title>
    <url>/blog/S2-%E8%AE%A1%E7%AE%97%E7%BB%99%E5%AE%9A%E7%82%B9%E5%88%B0%E5%93%AA%E6%9D%A1%E8%BE%B9%E6%9C%80%E8%BF%91/</url>
    <content><![CDATA[<p>在 GIS 系统里，我们可能会遇到如下的场景，给定一个坐标点，计算其到哪条边最近。而生活中对应的实例是：根据你的 GPS 位置信号，找到离你最近的一条公路是哪个。</p>
<p>在 google s2 的库中，提供了如下类，用来进行该类型的空间索引：<code>S2ClosestEdgeQuery</code>。</p>
<p>首先我们将已有的 geospatial objects 存到一个 index 中，也就是对这些 objects 建立了一个空间索引，我们暂且称这个索引为 <code>index</code>。</p>
<p>之后我们给定一个查询目标，<code>Target</code>，从 <code>index</code> 里面查找到在 <code>Target</code> 周围，符合我们要求的 spatial objects，这就是 query 以后返回的 <code>results</code>。</p>
<p>下面看一段简单的查询代码，给定 Target 点，找到距离它最近的边是哪些：</p>
<a id="more"></a>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2closest_edge_query.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;gtest/gtest.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/third_party/absl/memory/memory.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/mutable_s2shape_index.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s1angle.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2cap.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2cell.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2cell_id.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2edge_distances.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2loop.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2metrics.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2edge_vector_shape.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2point_vector_shape.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2polygon.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2predicates.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2testing.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"s2/s2text_format.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> absl::make_unique;</span><br><span class="line"><span class="keyword">using</span> s2textformat::MakeIndexOrDie;</span><br><span class="line"><span class="keyword">using</span> s2textformat::MakePointOrDie;</span><br><span class="line"><span class="keyword">using</span> s2textformat::MakePolygonOrDie;</span><br><span class="line"><span class="keyword">using</span> s2textformat::ParsePointsOrDie;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">fabs</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::make_pair;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::min;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::ostream;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::pair;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> absl::make_unique;</span><br><span class="line"><span class="keyword">using</span> s2textformat::MakeIndexOrDie;</span><br><span class="line"><span class="keyword">using</span> s2textformat::MakePointOrDie;</span><br><span class="line"><span class="keyword">using</span> s2textformat::MakePolygonOrDie;</span><br><span class="line"><span class="keyword">using</span> s2textformat::ParsePointsOrDie;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">fabs</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::make_pair;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::min;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::ostream;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::pair;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::numeric_limits;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Tests a target point in the interior of an indexed polygon.</span></span><br><span class="line">  <span class="comment">// (The index also includes a polyline loop with no interior.)</span></span><br><span class="line">  <span class="comment">// 这里定义了三条线段，(0,0)-(0,5) (5,5)-(5,0)，第三条由三个小段组成：(0,6)-(0,7)-(0,9)-(0,10)</span></span><br><span class="line">  <span class="keyword">auto</span> index = MakeIndexOrDie(<span class="string">"# 0:0, 0:5 | 5:5, 5:0| 0:6, 0:7, 0:9, 0:10 #"</span>);</span><br><span class="line">  S2ClosestEdgeQuery::Options options;</span><br><span class="line">  <span class="comment">// KNN 算法，设置 K = 2</span></span><br><span class="line">  options.set_max_edges(<span class="number">2</span>);</span><br><span class="line">  <span class="function">S2ClosestEdgeQuery <span class="title">query</span><span class="params">(index.get(), options)</span></span>;</span><br><span class="line">  <span class="comment">// 设定查询的目标点</span></span><br><span class="line">  S2ClosestEdgeQuery::<span class="function">PointTarget <span class="title">target</span><span class="params">(MakePointOrDie(<span class="string">"2:12"</span>))</span></span>;</span><br><span class="line">  <span class="comment">// 查找距离该点最近的两条线段</span></span><br><span class="line">  <span class="keyword">auto</span> results = query.FindClosestEdges(&amp;target);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 最终返回结果，一定是小于等于 2</span></span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">"should be 2:"</span> &lt;&lt; results.size() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">  <span class="comment">// 最近的 edge id 是哪个，也就是上面通过 MakeIndexOrDie 中的第几个 edge。这里应该是是 2，代表第三条线段</span></span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; results[<span class="number">0</span>].shape_id &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">  <span class="comment">// 因为可能一条线段由多个小段组成，所以再定位到是哪个小段，这里是2，表示最后一小段 (0,9)-(0,10)</span></span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; results[<span class="number">0</span>].edge_id &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">  <span class="comment">// 输出最近的距离值</span></span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; results[<span class="number">0</span>].distance &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以从下面的示例图中看出来：<br><img src="1.png" alt=""></p>
<p>当然，s2 的该类不仅仅只能计算点到哪条边最近，抽象一下它能够实现的功能为：</p>
<ol>
<li>计算距离给定 Target（可以是 point，edge，或者复杂的 polygon）最近的 spatial objects （这些 object 也可以是 point, edge, polygon）是哪些。</li>
<li>给定一个范围 r，是否在距离 Target 为 r 的周围，有 spatial object 存在。</li>
</ol>
<p>具体的例子可以到下面去查阅：<br><a href="http://s2geometry.io/devguide/s2closestedgequery" target="_blank" rel="noopener">http://s2geometry.io/devguide/s2closestedgequery</a><br><a href="https://github.com/google/s2geometry/blob/master/src/s2/s2closest_edge_query_test.cc" target="_blank" rel="noopener">https://github.com/google/s2geometry/blob/master/src/s2/s2closest_edge_query_test.cc</a></p>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>s2</tag>
      </tags>
  </entry>
  <entry>
    <title>Review:The bright side of sitting in traffic: Crowdsourcing road congestion data</title>
    <url>/blog/Review-The-bright-side-of-sitting-in-traffic-Crowdsourcing-road-congestion-data/</url>
    <content><![CDATA[<p><a href="https://googleblog.blogspot.com/2009/08/bright-side-of-sitting-in-traffic.html" target="_blank" rel="noopener">This blog</a> is written on Aug. 25, 2009, it discusses how to use cell phone GPS signal to identify crowded road in the U.S.</p>
<blockquote>
<p>If you use Google Maps for mobile with GPS enabled on your phone, that’s exactly what you can do. When you choose to enable Google Maps with My Location, your phone sends anonymous bits of data back to Google describing how fast you’re moving. When we combine your speed with the speed of other phones on the road, across thousands of phones moving around a city at any given time, we can get a pretty good picture of live traffic conditions. We continuously combine this data and send it back to you for free in the Google Maps <code>traffic layers</code>.</p>
</blockquote>
<p>The more users participant in this process, the more precise the report is. And this system was online to cover all U.S. highways and arterials in that week.</p>
<a id="more"></a>
<p>An issue that is addressed in the blog is <code>privacy</code>. People may not want to share their location and their destination data with others.</p>
<blockquote>
<p>We understand that many people would be concerned about telling the world how fast their car was moving if they also had to tell the world where they were going, so we built privacy protections in from the start. We only use anonymous speed and location information to calculate traffic conditions, and only do so when you have chosen to enable location services on your phone.</p>
</blockquote>
<p>Google solve the <code>privacy</code> issue by using <code>anonymous speed and location</code> information. Also they <code>aggregate</code> the data to make it safer:</p>
<blockquote>
<p>When a lot of people are reporting data from the same area, we <code>combine their data together</code> to make it hard to tell one phone from another.</p>
</blockquote>
<p>Google says they delete the <code>start and end point</code> of every trip. So I guess these data could only be stored in the cell phone temporarily.</p>
<blockquote>
<p>Even though the vehicle carrying a phone is anonymous, we don’t want anybody to be able to find out where that anonymous vehicle came from or where it went — so we find the start and end points of every trip and permanently delete that data so that even Google ceases to have access to it.</p>
</blockquote>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>google map</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB M201 - Query Plan</title>
    <url>/blog/MongoDB-M201-Query-Plan/</url>
    <content><![CDATA[<h1 id="什么是-Quary-Plan"><a href="#什么是-Quary-Plan" class="headerlink" title="什么是 Quary Plan"></a>什么是 Quary Plan</h1><p>当发起一个 query 请求时，当有多个约束条件，就会形成一个 query plan，本质上是怎么样去组织 pipeline 的 stage，使得 query 更有效率。</p>
<p>比如有下面的一个 query 请求：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">// 找出 zip code 大于 50000，并且`cuisine` field 包含 `Sushi`的 documents `stars` 降序排序</span><br><span class="line">db.restaurants.find(&#123;<span class="string">"address.zipcode"</span>: &#123;<span class="variable">$gt</span>:<span class="string">'50000'</span>&#125;, cuisine: <span class="string">'Sushi'</span>&#125;).sort(&#123;<span class="string">"stars"</span>: -1&#125;)</span><br></pre></td></tr></table></figure>

<p>那么我们应该怎样组织 query 过程呢？是先找到 documents 然后排序，还是先排序，然后再找 document？</p>
<p>使用什么样的 query plan 是跟我们建立的 index 相关的。使用不同的 index，则会得到不同的 query plan。</p>
<p>比如当index分别为如下两个时候：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;address.zipcode: 1, cuisine: 1&#125;</span><br><span class="line">&#123;cuisine: 1, stars: 1&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>1)的 index 下，query plan 是：IXSCAN -》FETCH -》SORT<br>2)的 index 下，query plan 是：IXSCAN -》FETCH。因为已经按照 stars 升序排序了，MongoDB 会反向scan，得到 stars 降序的结果。节省了一次 sort 过程。</p>
<h1 id="怎样选择-Query-Plan"><a href="#怎样选择-Query-Plan" class="headerlink" title="怎样选择 Query Plan"></a>怎样选择 Query Plan</h1><p>还是上面的 query 请求：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.restaurants.find(&#123;<span class="string">"address.zipcode"</span>: &#123;<span class="variable">$gt</span>:<span class="string">'50000'</span>&#125;, cuisine: <span class="string">'Sushi'</span>&#125;).sort(&#123;<span class="string">"stars"</span>: -1&#125;)</span><br></pre></td></tr></table></figure>

<p>但是当同时存在多个 index，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;_id: 1&#125;</span><br><span class="line">&#123;name: 1, cuisine: 1, stars: 1&#125;</span><br><span class="line">&#123;<span class="string">"address.zipcode"</span>: 1, cuisine: 1&#125;</span><br><span class="line">&#123;<span class="string">"address.zipcode"</span>: 1, stars: 1&#125;</span><br><span class="line">&#123;cuisine: 1, stars: 1&#125;</span><br></pre></td></tr></table></figure>

<p>那么 query plan 应该怎么安排才能够更有效利用这些 index，是的 query 的耗时更短呢？</p>
<p><code>第一次调用</code> 该 query 时候，MongoDB 首先会扫描已有的 index，并从中找出能够利用的 index（which indexes are viable to satisfy the query），作为 <code>candidate indexes</code>。在这里，candidate index 是这几个：</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line">    &#123;_id: 1&#125;</span><br><span class="line">    &#123;name: 1, cuisine: 1, stars: 1&#125;</span><br><span class="line"><span class="addition">+   &#123;"address.zipcode": 1, cuisine: 1&#125;</span></span><br><span class="line"><span class="addition">+   &#123;"address.zipcode": 1, stars: 1&#125;</span></span><br><span class="line"><span class="addition">+   &#123;cuisine: 1, stars: 1&#125;</span></span><br></pre></td></tr></table></figure>

<p>然后根据 <code>candidate indexed</code>， MongoDB 的 <code>query optimizer</code> 会生成 <code>candidate plans</code>。同时，MongoDB 有一个 <code>empirical query planer</code>，它会对每个 candidate plan，运行一小段时间（trial run），然后从这些测试中，找出 performance 最好的一个。</p>
<p>对本 query 而言，最好的 plan 如下：<br><img src="1.png" alt=""></p>
<p>之后，MongoDB 会 <code>cache</code> 该 query 下的 best plan，我们把这个 cache 称为 <code>plan cache</code>。<br>我们形容相同的 query 具有相同的 <code>query shape</code>。那么下次，具有相同的 query shape 的 query 进来以后，就会直接从 <code>plan cache</code> 里面调用对应的 query plan。</p>
<h1 id="Evict-a-plan"><a href="#Evict-a-plan" class="headerlink" title="Evict a plan"></a>Evict a plan</h1><p>随着时间，数据库的 collection 数据会发生变化，index 也是如此。所以有时候，plan cache 需要剔除某些 query plan，这个过程叫做 <code>evict</code>。<br>下面这些情况下，会 evict：<br><img src="2.png" alt=""></p>
<ul>
<li>Server restart</li>
<li>Rebuild index</li>
<li>Create/Drop indexes</li>
</ul>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB M201 - Building Indexes </title>
    <url>/blog/MongoDB-M201-Building-Indexes/</url>
    <content><![CDATA[<h1 id="Building-Indexes"><a href="#Building-Indexes" class="headerlink" title="Building Indexes"></a>Building Indexes</h1><p>两种方式：</p>
<ul>
<li>forground indexes: 建立时候会 block user input，导致这段时间 DB 不可用。这在产品中不可接受。</li>
<li>background indexes: 后台建立 index，不会 block operation，代价是build 的时间更长。<a id="more"></a>

</li>
</ul>
<h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongoimport -d m201 -c restaurants --drop restaurants.json</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mac@macs-MacBook  ~/Desktop  mongoimport -d m201 -c restaurants --drop restaurants.json</span><br><span class="line">2018-03-01T14:45:48.743-0600	connected to: localhost</span><br><span class="line">2018-03-01T14:45:48.745-0600	dropping: m201.restaurants</span><br><span class="line">2018-03-01T14:45:51.732-0600	[<span class="comment">######..................] m201.restaurants	40.0MB/144MB (27.9%)</span></span><br><span class="line">2018-03-01T14:45:54.731-0600	[<span class="comment">#############...........] m201.restaurants	78.9MB/144MB (54.9%)</span></span><br><span class="line">2018-03-01T14:45:57.731-0600	[<span class="comment">###################.....] m201.restaurants	115MB/144MB (80.1%)</span></span><br><span class="line">2018-03-01T14:45:59.862-0600	[<span class="comment">########################] m201.restaurants	144MB/144MB (100.0%)</span></span><br><span class="line">2018-03-01T14:45:59.862-0600	imported 1000000 documents</span><br></pre></td></tr></table></figure>

<h1 id="Build-background-index"><a href="#Build-background-index" class="headerlink" title="Build background index"></a>Build background index</h1><p>默认建立的 index 都是 forground 的，所以为了不 block operation，这里使用 mongo shell 创建 index：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.restaurants.createIndex(&#123;<span class="string">"cuisine"</span>:1,<span class="string">"name"</span>:1,<span class="string">"address.zipcode"</span>:1&#125;,&#123;<span class="string">"background"</span>:<span class="literal">true</span>&#125;)</span><br></pre></td></tr></table></figure>

<p>这个建立过程虽然不会 block db operation，但是会 block 当前的 mongo shell。我们可以打开另一个 mongo shell 使用 <code>db.currentOp()</code> 去查看当前状态。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">"opid"</span> : 2381,</span><br><span class="line"><span class="string">"secs_running"</span> : NumberLong(2),</span><br><span class="line"><span class="string">"microsecs_running"</span> : NumberLong(2389863),</span><br><span class="line"><span class="string">"op"</span> : <span class="string">"command"</span>,</span><br><span class="line"><span class="string">"ns"</span> : <span class="string">"m201.<span class="variable">$cmd</span>"</span>,</span><br><span class="line"><span class="string">"command"</span> : &#123;</span><br><span class="line">	<span class="string">"createIndexes"</span> : <span class="string">"restaurants"</span>,</span><br><span class="line">	<span class="string">"indexes"</span> : [</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="string">"key"</span> : &#123;</span><br><span class="line">				<span class="string">"cuisine"</span> : 1,</span><br><span class="line">				<span class="string">"name"</span> : 1,</span><br><span class="line">				<span class="string">"address.zipcode"</span> : 1</span><br><span class="line">			&#125;,</span><br><span class="line">			<span class="string">"name"</span> : <span class="string">"cuisine_1_name_1_address.zipcode_1"</span>,</span><br><span class="line">			<span class="string">"background"</span> : <span class="literal">true</span></span><br><span class="line">		&#125;</span><br><span class="line">	],</span><br><span class="line">	<span class="string">"<span class="variable">$db</span>"</span> : <span class="string">"m201"</span></span><br><span class="line">&#125;,</span><br><span class="line"><span class="string">"msg"</span> : <span class="string">"Index Build (background) Index Build (background): 280957/1000000 28%"</span>,</span><br><span class="line"><span class="string">"progress"</span> : &#123;</span><br><span class="line">	<span class="string">"done"</span> : 280958,</span><br><span class="line">	<span class="string">"total"</span> : 1000000</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure>

<p>可以看到当前状态是 db 正在 background 的方式建立 index，进行了 28%。<br>记住 opid，我们可以使用 <code>db.killOp(opid)</code> 命令来 kill operation before it finish。</p>
<p>index 建立结束以后的状态如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.restaurants.createIndex(&#123;<span class="string">"cuisine"</span>:1,<span class="string">"name"</span>:1,<span class="string">"address.zipcode"</span>:1&#125;,&#123;<span class="string">"background"</span>:<span class="literal">true</span>&#125;)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="string">"createdCollectionAutomatically"</span> : <span class="literal">false</span>,</span><br><span class="line">	<span class="string">"numIndexesBefore"</span> : 1,</span><br><span class="line">	<span class="string">"numIndexesAfter"</span> : 2,</span><br><span class="line">	<span class="string">"ok"</span> : 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB in Python</title>
    <url>/blog/MongoDB-in-Python/</url>
    <content><![CDATA[<h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><h2 id="导入一个-collection"><a href="#导入一个-collection" class="headerlink" title="导入一个 collection"></a>导入一个 collection</h2><p><a href="https://s3.amazonaws.com/edu-static.mongodb.com/lessons/coursera/building-an-app/getting-data-into-mongodb/movies_initial.csv" target="_blank" rel="noopener">https://s3.amazonaws.com/edu-static.mongodb.com/lessons/coursera/building-an-app/getting-data-into-mongodb/movies_initial.csv</a></p>
<p>after create MongoDB Altas cluster, import movies_initial.csv file to the cluster.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongoimport --<span class="built_in">type</span> csv --headerline --db mflix --c movies_initial --host <span class="string">"mflix-shard-00-00-1cvum.mongodb.net:27017,mflix-shard-00-01-1cvum.mongodb.net:27017,mflix-shard-00-02-1cvum.mongodb.net:27017"</span>--authenticationDatabase admin --ssl --username hansonzhao007 --password Zxsh3017568 --file movies_initial.csv</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<h1 id="导入所有-collection"><a href="#导入所有-collection" class="headerlink" title="导入所有 collection"></a>导入所有 collection</h1><p>首先从这里 <a href="https://s3.amazonaws.com/edu-static.mongodb.com/lessons/coursera/building-an-app/mflix.zip" target="_blank" rel="noopener">https://s3.amazonaws.com/edu-static.mongodb.com/lessons/coursera/building-an-app/mflix.zip</a> 下载压缩文件并解压。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 进入mflix文件夹，并安装依赖环境</span></span><br><span class="line">pip3 install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p>After installing all the dependencies you can import the data required by mflix into your MongoDB Atlas cluster.<br>To import this data you’ll first need to paste your connection URI (from the Atlas UI) into env.sh (or env.bat on Windows).<br>After you’ve update env.sh (or env.bat on Windows) with your Atlas connection URI you can run init.sh (or init.bat on Windows) to import all the required data:<br>On Windows your env.bat should look like this:<br><img src="1.png" alt=""></p>
<p>导入过程如下：<br><img src="2.png" alt=""></p>
<p>最终数据如下：<br><img src="3.png" alt=""></p>
<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><p>除了安装 MongoDB ，要使用 python 来操作 mongoDB，需要使用 PyMongo plugin。安装过程见<a href="https://api.mongodb.com/python/current/installation.html" target="_blank" rel="noopener">这里</a>。</p>
<h1 id="链接-MongoDB-atlas"><a href="#链接-MongoDB-atlas" class="headerlink" title="链接 MongoDB atlas"></a>链接 MongoDB atlas</h1><p>因为创建的 atlas cluster 是基于 mongoDB 3.4 的，所以链接的 URI 如下：<br><img src="4.png" alt=""></p>
<p>然后使用如下 code 测试链接效果：<br><img src="5.png" alt=""></p>
<p>可以看到没有 error 输出，表示链接成功。</p>
<h1 id="MongoDB-aggregation"><a href="#MongoDB-aggregation" class="headerlink" title="MongoDB aggregation"></a>MongoDB aggregation</h1><p>下面测试 mongodb 的 pipeline stage。在连接 mongodb cluster 以后：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pprint</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</span><br><span class="line"><span class="comment"># 这里是 MongoDB pipeline stage 的语法写法。</span></span><br><span class="line">pipeline = [</span><br><span class="line">    <span class="comment"># group stage</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'$group'</span>:&#123;</span><br><span class="line">            <span class="string">'_id'</span>:&#123;<span class="string">"language"</span>:<span class="string">"$language"</span>&#125;,</span><br><span class="line">            <span class="string">'count'</span>:&#123;<span class="string">'$sum'</span>:<span class="number">1</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># sort stage</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'$sort'</span>:&#123;<span class="string">'count'</span>:<span class="number">-1</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 上面等同于这里：作用是根据 language 聚类，统计数量，并按照由多到少的顺序排列</span></span><br><span class="line"><span class="comment"># pipeline = [</span></span><br><span class="line"><span class="comment">#     &#123;</span></span><br><span class="line"><span class="comment">#         '$sortByCount': "$language"</span></span><br><span class="line"><span class="comment">#     &#125;</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line">pprint.pprint(list(client.mflix.movies_initial.aggregate(pipeline)))</span><br></pre></td></tr></table></figure>

<h1 id="Facet"><a href="#Facet" class="headerlink" title="Facet"></a>Facet</h1><p>这里facet 的作用是接受上一个 stage 的输入，将当前 stage 拆分成并列的 pipeline，输出多个结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pipeline = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'$sortByCount'</span>:<span class="string">'$language'</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'$facet'</span>:&#123;</span><br><span class="line">            <span class="string">'top language combination'</span>:[&#123;<span class="string">'$limit'</span>:<span class="number">100</span>&#125;],</span><br><span class="line">            <span class="string">'unusual combinations shared by'</span>:[</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">'$skip'</span>:<span class="number">100</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">'$bucketAuto'</span>:&#123;</span><br><span class="line">                        <span class="string">'groupBy'</span>: <span class="string">'$count'</span>,</span><br><span class="line">                        <span class="string">'buckets'</span>: <span class="number">5</span>,</span><br><span class="line">                        <span class="string">'output'</span>: &#123;</span><br><span class="line">                            <span class="string">'language combinations'</span>:&#123;<span class="string">'$sum'</span>:<span class="number">1</span>&#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">pprint.pprint(list(client.mflix.movies_initial.aggregate(pipeline)))</span><br></pre></td></tr></table></figure>
<p><img src="6.png" alt=""></p>
<h1 id="Filter-on-Scalar-Field"><a href="#Filter-on-Scalar-Field" class="headerlink" title="Filter on Scalar Field"></a>Filter on Scalar Field</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filter = &#123;<span class="string">'language'</span>:<span class="string">'Korean, English'</span>&#125;</span><br><span class="line">projection = &#123;<span class="string">'language'</span>:<span class="number">1</span>,<span class="string">'title'</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">pprint.pprint(list(client.mflix.movies_initial.find(filter,projection)))</span><br></pre></td></tr></table></figure>

<h1 id="Geospatial-queries"><a href="#Geospatial-queries" class="headerlink" title="Geospatial queries"></a>Geospatial queries</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace XXXX with your connection URI from the Atlas UI</span></span><br><span class="line">course_cluster_uri = <span class="string">"mongodb://hansonzhao007:&lt;PASSWORD&gt;@mflix-shard-00-00-1cvum.mongodb.net:27017,mflix-shard-00-01-1cvum.mongodb.net:27017,mflix-shard-00-02-1cvum.mongodb.net:27017/test?ssl=true&amp;replicaSet=mflix-shard-0&amp;authSource=admin"</span></span><br><span class="line"></span><br><span class="line">course_client = pymongo.MongoClient(course_cluster_uri)</span><br><span class="line">theaters = course_client[<span class="string">'mflix'</span>][<span class="string">'theaters'</span>]</span><br><span class="line"></span><br><span class="line">theater = theaters.find_one(&#123;&#125;)</span><br><span class="line"></span><br><span class="line">pprint.pprint(theater)</span><br><span class="line"></span><br><span class="line">pprint.pprint(theater[<span class="string">'location'</span>][<span class="string">'geo'</span>])</span><br><span class="line"></span><br><span class="line">EARTH_RADIUS_MILES = <span class="number">3963.2</span></span><br><span class="line">EARTH_RADIUS_KILOMETERS = <span class="number">6378.1</span></span><br><span class="line">example_radius = <span class="number">0.1747728893434987</span></span><br><span class="line"></span><br><span class="line">radius_in_miles = example_radius * EARTH_RADIUS_MILES</span><br><span class="line"></span><br><span class="line">print(radius_in_miles)</span><br><span class="line"></span><br><span class="line">query = &#123;</span><br><span class="line">  <span class="string">"location.geo"</span>: &#123;</span><br><span class="line">    <span class="string">"$nearSphere"</span>: &#123;</span><br><span class="line">      <span class="string">"$geometry"</span>: &#123;</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"Point"</span>,</span><br><span class="line">        <span class="string">"coordinates"</span>: [<span class="number">-73.9899604</span>, <span class="number">40.7575067</span>]</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"$minDistance"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="string">"$maxDistance"</span>: <span class="number">10000</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">projection = &#123;<span class="string">"location.address"</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> theater <span class="keyword">in</span> theaters.find(query,projection).limit(<span class="number">5</span>):</span><br><span class="line">    pprint.pprint(theater)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB M201 - Sorting with indexes</title>
    <url>/blog/MongoDB-M201-Sorting-with-indexes/</url>
    <content><![CDATA[<p>任何一次 query 的结果都可以被排序，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">exp.find(&#123;first_name:<span class="string">"James"</span>&#125;).sort(&#123;first_name:1&#125;)</span><br></pre></td></tr></table></figure>

<p>有两种排序方式：<br>    1. 从 disk 里面读取 documents，然后做 in memory 排序。<br>    2. 根据 field 生成 index，index 是排序的。</p>
<p>当我们对没有建立 index 的 field query 后（以上面的first_name 查找排序为例）进行排序。mongoDB 会首先从 collection 里面遍历，完成 query 操作，这会 touch 到所有的 documents。然后 filter 以后的结果，在 memory 中进行排序。如果 query 得到 documents 很多，那么该过程会很费时(mongoDB 会避免超过 32MB 的 in memory 排序)。</p>
<a id="more"></a>
<h1 id="Compound-index-sorting"><a href="#Compound-index-sorting" class="headerlink" title="Compound index sorting"></a>Compound index sorting</h1><p>index prefix: 当使用 compound index 的时候，如果 query 的 field 组成了一个 compound index 的 prefix，那么就可以利用 index 输出 sorting 的结果。</p>
<p><img src="1.png" alt=""><br>因为选中的三项都可以构成 compound index 的 prefix，所以可以用来同时做 filter 和 sorting。</p>
<ul>
<li>第一项不是 prefix，只是中间的某个子集，所以不行。</li>
<li>第四项因为跳过了 address.state ，所以也不是子集，用不了 compound index 去 filter 和 sorting。只能做 memory sorting。</li>
</ul>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB M201 - Single Field Indexes </title>
    <url>/blog/MongoDB-M201-Single-Field-Indexes/</url>
    <content><![CDATA[<h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><p>首先到 <a href="https://s3.amazonaws.com/m312/people.json" target="_blank" rel="noopener">https://s3.amazonaws.com/m312/people.json</a> 下载数据。然后导入本地 MongoDB 数据库里。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongoimport -d m201 -c people --drop people.json</span><br></pre></td></tr></table></figure>
<p>MongoDB 启动以后（即 mongod 运行），使用 MongoDB compass 链接本地数据库，查看：<br><img src="1.png" alt=""></p>
<a id="more"></a>
<p>然后可以进入 mongo shell 使用如下命令查看 m201 的数据。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.people.count(&#123; <span class="string">"email"</span> : &#123;<span class="string">"<span class="variable">$exists</span>"</span>: 1&#125; &#125;)</span><br></pre></td></tr></table></figure>

<h1 id="查询分析"><a href="#查询分析" class="headerlink" title="查询分析"></a>查询分析</h1><p>首先 在 mongo shell 中使用一条查询命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.people.find(&#123;<span class="string">"ssn"</span>:<span class="string">"720-38-5636"</span>&#125;).explain(<span class="string">"executionStats"</span>)</span><br></pre></td></tr></table></figure>
<p>这里 explain 是用于显示查询的过程的，也可以使用 compass 的 explain 窗口显示：<br><img src="2.png" alt=""></p>
<p>可以看到这次查询耗费 21ms。为了返回一个结果，检查了所有的 document。效率很低。</p>
<h1 id="创建-index"><a href="#创建-index" class="headerlink" title="创建 index"></a>创建 index</h1><p>使用命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.people.createIndex(&#123;ssn:1&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="3.png" alt=""></p>
<p>可以看到 index 创建成功。下面重新重复上次的查询。<br><img src="4.png" alt=""></p>
<p>可以看到因为 index 的加速，只 touch 了一个 document，并且查询时间为 0ms。</p>
<p>我们可以创建一个 explain 的查询器，以方便观察呢每次 query 的过程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">exp = db.people.explain(<span class="string">"executionStats"</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB M201 - How Data is stored on disk</title>
    <url>/blog/M201-How-Data-is-stored-on-disk/</url>
    <content><![CDATA[<h1 id="配置本地环境"><a href="#配置本地环境" class="headerlink" title="配置本地环境"></a>配置本地环境</h1><p>默认情况下，MongoDB 数据库目录的组织结构如下：<br><img src="1.png" alt=""></p>
<ul>
<li>collection-*.wt: 数据库的 collection 文件</li>
<li>index-*.wt: 数据库建立的 index 文件</li>
</ul>
<p><img src="2.png" alt=""></p>
<a id="more"></a>
<p>现在的数据库组织形式还是 flat 的，也就是所有数据库 collection 存储在一个目录下。</p>
<h1 id="创建数据库子目录"><a href="#创建数据库子目录" class="headerlink" title="创建数据库子目录"></a>创建数据库子目录</h1><p>我们可以使用 –directoryperdb 命令让 MongoDB 将数据库存储时候按照文件夹区分开：<br>首先使用命令，关闭 mongodb server。然后删除 db 目录并重新建立一个 db 目录。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">• mongo admin --<span class="built_in">eval</span> <span class="string">'db.shutdownServer()'</span></span><br><span class="line">• rm -rf /data/db</span><br><span class="line">• mkdir -p /data/db</span><br></pre></td></tr></table></figure>


<p>下面对每一个 database 都创建自己的目录。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">• mongod --dbpath /data/db --fork --logpath /data/db/mongodb.log --directoryperdb</span><br></pre></td></tr></table></figure>
<p><img src="3.png" alt=""></p>
<p>可以看到初级目录里面已经没有 collection 和 index 文件了。如果进入 local 子目录，可以看到：<br><img src="4.png" alt=""></p>
<h1 id="创建-collection-和-index-子目录"><a href="#创建-collection-和-index-子目录" class="headerlink" title="创建 collection 和 index 子目录"></a>创建 collection 和 index 子目录</h1><p>上面我们已经把不同的 database 放到了不同目录下，现在我们还可以将 collection 和 index 文件放在各自的目录下。</p>
<p>首先使用命令，关闭 mongodb server。然后删除 db 目录并重新建立一个 db 目录。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">• mongo admin --<span class="built_in">eval</span> <span class="string">'db.shutdownServer()'</span></span><br><span class="line">• rm -rf /data/db</span><br><span class="line">• mkdir -p /data/db</span><br></pre></td></tr></table></figure>
<p>使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">• mongod --dbpath /data/db --fork --logpath /data/db/mongodb.log --directoryperdb --wiredTigerDirectoryForIndexes</span><br></pre></td></tr></table></figure>

<p><img src="5.png" alt=""></p>
<p>可以看到 local 数据库目录下多了两个子目录，一个是 collection ，一个是 index。之所以这么做，是为了提升数据库的性能。我们可以将 collection 和 index 放在两个不同的 disk 上。这样利用两个 disk 的 parallel 读写（parallel I/O），提升 mongodb 的数据吞吐量。</p>
<p><img src="6.png" alt=""></p>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>lock</title>
    <url>/blog/lock/</url>
    <content><![CDATA[<h1 id="What-a-lock-do"><a href="#What-a-lock-do" class="headerlink" title="What a lock do?"></a>What a lock do?</h1><p>Lock is used to make sure code in critical section been executed atomically.</p>
<p><code>Atomic</code>: An operation (or set of operations) is atomic, linearizable, indivisible or uninterruptible if it appears to the rest of the system to occur at once without being interrupted. Atomicity is a guarantee of isolation from interrupts, signals, concurrent processes and threads.</p>
<h1 id="build-a-Lock"><a href="#build-a-Lock" class="headerlink" title="build a Lock"></a>build a Lock</h1><p>with the help of hardware and os, some primitive instructions can help to build a lock.</p>
<a id="more"></a>
<h1 id="Evaluating-a-Lock"><a href="#Evaluating-a-Lock" class="headerlink" title="Evaluating a Lock"></a>Evaluating a Lock</h1><ol>
<li>provide mutual exclusion</li>
<li>fairness, no starvation</li>
<li>how is the performance</li>
</ol>
<h1 id="Spin-lock-with-Test-and-set"><a href="#Spin-lock-with-Test-and-set" class="headerlink" title="Spin lock with Test-and-set"></a>Spin lock with Test-and-set</h1><p>We can test the old value while simultaneously setting the memory location to new value.<br>atomic test-and-set instruction(in C)</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">TestAndSet</span><span class="params">(<span class="keyword">int</span> *old_ptr, <span class="keyword">int</span> <span class="keyword">new</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> old = *old_ptr; <span class="comment">// fetch old value at old_ptr</span></span><br><span class="line">  *old_ptr = <span class="keyword">new</span>; <span class="comment">// store ’new’ into old_ptr</span></span><br><span class="line">  <span class="keyword">return</span> old; <span class="comment">// return the old value</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>spin lock:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> __<span class="title">lock_t</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> flag;</span><br><span class="line">&#125; <span class="keyword">lock_t</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(<span class="keyword">lock_t</span> *lock)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 0 indicates that lock is available, 1 that it is held</span></span><br><span class="line">  lock-&gt;flag = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">(<span class="keyword">lock_t</span> *lock)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">while</span> (TestAndSet(&amp;lock-&gt;flag, <span class="number">1</span>) == <span class="number">1</span>)</span><br><span class="line"> ; <span class="comment">// spin-wait (do nothing)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">unlock</span><span class="params">(<span class="keyword">lock_t</span> *lock)</span> </span>&#123;</span><br><span class="line"> lock-&gt;flag = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>初始化，没有锁。假如有一个 thread 调用 lock() 函数；那么因为返回值是0，不会进入 spin lock。但是这时候，lock-&gt;flag 被置为 new value 1；假如该 thread 并没有释放lock，那么 lock-&gt;flag 就一直为 1；当另一个 thread 调用 lock() 时候，test-and-set 返回 1，并继续设置为 1。进入 spin lock。</p>
<ol>
<li>提供 mutual exclusion</li>
<li>不提供 fairness，可能出现 starvation</li>
<li>performance：对于单核来说，很不好。假如有 N 个 thread 共用一个 lock，一旦一个 thread 锁住 lock，其他 thread 因为会一直 spin，所以会占用 (N-1) 个 time period 做无用的循环。</li>
</ol>
<h1 id="Spin-lock-with-compare-and-swap"><a href="#Spin-lock-with-compare-and-swap" class="headerlink" title="Spin lock with compare-and-swap"></a>Spin lock with compare-and-swap</h1><h1 id="Spin-lock-with-Load-Linked-and-Store-Conditional"><a href="#Spin-lock-with-Load-Linked-and-Store-Conditional" class="headerlink" title="Spin lock with Load-Linked and Store-Conditional"></a>Spin lock with Load-Linked and Store-Conditional</h1><h1 id="Spin-lock-with-Fetch-And-Add"><a href="#Spin-lock-with-Fetch-And-Add" class="headerlink" title="Spin lock with Fetch-And-Add"></a>Spin lock with Fetch-And-Add</h1>]]></content>
      <categories>
        <category>program</category>
      </categories>
      <tags>
        <tag>multi-thread</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Atomic</title>
    <url>/blog/cplusAtomic/</url>
    <content><![CDATA[<p><a href="https://stackoverflow.com/questions/6319146/c11-introduced-a-standardized-memory-model-what-does-it-mean-and-how-is-it-g" target="_blank" rel="noopener">What exactly is std::atomic</a><br><a href="https://bartoszmilewski.com/2008/12/01/c-atomics-and-memory-ordering/" target="_blank" rel="noopener">C++ atomics and memory ordering</a></p>
<p>This is used to prevent the CPU from doing the instruction reordering.</p>
<p>An operation (or set of operations) is atomic, linearizable, indivisible or uninterruptible if it appears to the rest of the system to occur at once without being interrupted. Atomicity is a guarantee of isolation from interrupts, signals, concurrent processes and threads.</p>
<a id="more"></a>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">atomic&lt;<span class="keyword">bool</span>&gt; ready = <span class="literal">false</span>;</span><br><span class="line">atomic&lt;<span class="keyword">int</span>&gt; data = <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<p>Thread 0:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">data.store(<span class="number">1</span>, memory_order_release);</span><br><span class="line">ready.store(<span class="literal">true</span>, memory_order_release);</span><br></pre></td></tr></table></figure>
<p>Thread 1:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (ready.load(memory_order_acquire))</span><br><span class="line">  assert (data.load(memory_order_acquire) == <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>The code below will not create a memory fence.</p>
<p><code>[memory fence](https://en.wikipedia.org/wiki/Memory_barrier)</code>:CPUs often execute instructions <a href="https://en.wikipedia.org/wiki/Out-of-order_execution" target="_blank" rel="noopener">out of order</a> to make maximum use of the available silicon (including memory read/writes). Because the hardware enforces instructions integrity you never notice this in a single thread of execution. However for multiple threads or environments with volatile memory (memory mapped I/O for example) this can lead to unpredictable behavior.</p>
<p>A memory fence/barrier is a class of instructions that mean memory read/writes occur in the order you expect. For example a ‘full fence’ means all read/writes before the fence are comitted before those after the fence.</p>
<ul>
<li><code>memory_order_acquire</code>: guarantees that <code>subsequent loads</code> are not moved before the current load or any preceding loads.</li>
<li><code>memory_order_release</code>: <code>preceding stores</code> are not moved past the current store or any subsequent stores.</li>
<li><code>memory_order_acq_rel</code>: combines the two previous guarantees.</li>
<li><code>memory_order_consume</code>: potentially weaker form of memory_order_acquire that enforces ordering of the current load before other operations that are data-dependent on it (for instance, when a load of a pointer is marked memory_order_consume, subsequent operations that dereference this pointer won’t be moved before it (yes, even that is not guaranteed on all platforms!).</li>
<li><code>memory_order_relaxed</code>: all reorderings are okay.</li>
</ul>
]]></content>
      <categories>
        <category>program</category>
      </categories>
      <tags>
        <tag>multi-thread</tag>
        <tag>levelDB</tag>
      </tags>
  </entry>
  <entry>
    <title>what is mutual exclusion 互斥</title>
    <url>/blog/mutex/</url>
    <content><![CDATA[<p>it is the requirement that one thread of execution never enter its critical section at the same time that another concurrent thread of execution enters its own critical section</p>
<p>不允许多线程对 critical section （可以是 share variables）的同时访问。</p>
<p>下面是用 linked list 的删除操作来说明。</p>
<p><img src="1.png" alt="1"></p>
<p>两个线程同时进行删除操作，分别想删除 i 和 i+1，同时执行了</p>
<ul>
<li>node(i-1)-&gt;next = node(i)-&gt;next (为了删除 node(i）)</li>
<li>node(i)-&gt;next = node(i+1)-&gt;next (为了删除 node(i+1))<a id="more"></a>
最后结果就像第三行所示， node(i+1)没有被删除。</li>
</ul>
<p>但是这之后，因为还有释放空间操作，结果就是 node(i) 和 node(i+1) 同时被 delete。所以最后最后的 list 结构变成：</p>
<ul>
<li>node(i-1) ——————&gt; unknown — node(i+2)</li>
</ul>
<p>list 结构被破坏。</p>
<p>所以需要对删除 linked list 的函数，delete 加 mutex 锁。</p>
]]></content>
      <categories>
        <category>program</category>
      </categories>
      <tags>
        <tag>multi-thread</tag>
        <tag>levelDB</tag>
      </tags>
  </entry>
  <entry>
    <title>review on Geospatial Performance Improvements in MongoDB 3.2</title>
    <url>/blog/mongodb-geoindex-improvement/</url>
    <content><![CDATA[<p><a href="https://www.mongodb.com/blog/post/geospatial-performance-improvements-in-mongodb-3-2" target="_blank" rel="noopener">link</a></p>
<p>In mongoDB version before v3.2, <code>$geoNear</code> operation was sometimes slow for queries on dense datasets.</p>
<p>After version v3.2, this algorithm had been upgrade. This is a brief explanation about the upgrade.</p>
<p>The <code>$geoNear</code> algorithm iteratively expands its search in distance <code>intervals</code> (the red annulus shown below), aiming to have a few hundred documents per interval. Searching all documents in an interval is accomplished by finding an index <code>cell covering</code>. This covering ensures that all of the documents in the interval can be found using an index scan. The documents in the covering but not in the interval are filtered out afterwards. After all of the documents in an interval are found, they are sorted and returned.</p>
<a id="more"></a>
<p><img src="1.png" alt=""></p>
<p>Problem is that the cell covering may have overlaps in different iterations (Also called stages).</p>
<p><img src="2.png" alt=""></p>
<p><img src="3.png" alt=""></p>
<p>The original $geoNear algorithm did not account for this problem. At every interval, it would create a covering independent of what was already covered during searches of previous intervals. The <code>repeated index scans</code> could become very expensive in dense datasets where the search intervals were thin.</p>
<p><code>new $geoNear algorithm</code></p>
<ul>
<li>buffer the documents in current covering but not in current iteration’s interval, and provide them for next iteration.</li>
<li>current iteration(or stage) only fetch the documents in current coverings that has no overlap with previous iteration. (the new fetched documents with the buffered documents from previous iteration will be used)</li>
</ul>
<p><img src="4.png" alt=""></p>
<p><img src="5.png" alt=""></p>
<p>Since the algorithm now only needs to visit the cells that have not been visited before, the number of index scans required in large queries drops dramatically</p>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper review: A Scalable Distributed Spatial Index for the Internet-of-Things</title>
    <url>/blog/socc17-iot/</url>
    <content><![CDATA[<p>This is an article about partitioning geo-index at SOCC17 from UCB by Anand.</p>
<h1 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h1><p>The era of the Internet of things is imminent. Big data of IoT is inherently geospatial in nature and has unpredictable <code>skews in space and time</code>.</p>
<p>In order to overcome the skews and give the best performance, partition and re-balance are critical to the database and key-value store. However, frequent re-balance operation will harm insertion performance.</p>
<a id="more"></a>
<p>This paper proposes a method called <code>sift</code> which claims can migrate skew at ingest time. By utilizing additional dimension, sift can distribute spatial data which originally be put in one node now be put in several nodes. This is how it overcomes spatial data skew.</p>
<h1 id="Future-relevance"><a href="#Future-relevance" class="headerlink" title="Future relevance"></a>Future relevance</h1><p>I have some doubts about this paper.</p>
<ol>
<li><p>MongoDB does not support spatial index partition, and this paper says in order to query a spatial index, MongoDB has to query all partition based on other index and aggregate all results together to serve one spatial query.<br>Yes, MongoDB does not support spatial index sharding, however, we can use another field to reduce the range of spatial query. For example, use state, city and then go to that partition to do a spatial query.</p>
</li>
<li><p>Sift does not limit the number of objects that one node can hold (only limited by its memory) and never move objects once they are inserted. I really doubt this can work, even with its hyper-dimension mechanism to migrate the data skew. Because I think if data in one place explode, the data could overflow even though they can be stored in several nodes instead of one node.</p>
</li>
</ol>
<p>Following is an illustration of its hyper-dimension mechanism.</p>
<p><img src="1.png" alt=""></p>
<p>Actually, the <code>main idea</code> is that both leaf node and inter node can store object data.</p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamo: Amazon's Highly Available Key-value Store</title>
    <url>/blog/Casandra/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This is the paper presented at 2007’s SOSP by Amazon.</p>
<p>In order to <code>provide high availability</code> Dynamo <code>sacrifices consistency</code> under certain failure scenarios ().</p>
<p>Amazon’s primary usecase for Dynamo is its shopping cart, where it’s really important to be highly available, or customers should be able to view and add items to shopping cart dispite of datacenter failure.</p>
<p>In a cluster consists of millions of components, it always have small but significant number of server or network components failing at any giving time. Dynamo should be able to treats these as normal case without impacting availability or performance.</p>
<a id="more"></a>
<p>For many service, such as shopping carts, customer preference, sales rank, they only need a primary-key to retrieve data and not require complex querying, and this is how Dynamo works. It provides <code>primary-key only</code> interface.</p>
<ul>
<li>consistent hashing(no need to redistribution when storage nodes added or removed). Differ from traditional consistent hashing, it assigns key to multiple point in the ring.(<code>virtual nodes</code>)</li>
<li>Lamport algorithm for synchronization (clock sync)</li>
<li>quorum-based protocol (Replicated-write protocol)</li>
<li>gossip failure detection in which each node regularly announces to its neighbors that it is still up and running.</li>
</ul>
<p>Dynamo provides eventual consistency. Some replica servers may fail before all updates come. So, for example, the shopping cart may have different version. Dynamo merges different version shopping cart to avoid any “add to cart” operation lost, however, some deleted items can resurface. Dynamo uses <code>vector clock</code> to decide whether to implement a merge operation.</p>
<p>#</p>
]]></content>
      <categories>
        <category>review</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title>mongoDB index 与 performance</title>
    <url>/blog/mongo-index/</url>
    <content><![CDATA[<ul>
<li><a href="https://www.youtube.com/watch?v=qI_g07C_Q5I" target="_blank" rel="noopener">Introduce to NoSql</a></li>
</ul>
<h1 id="Single-Field-indexes"><a href="#Single-Field-indexes" class="headerlink" title="Single Field indexes"></a>Single Field indexes</h1><p>对 collection 里的某一个 field 创建的 index 就是 sigle  field indexes。它的特点：</p>
<ul>
<li>keys from only one Field</li>
<li>can find a single value for the indexes field</li>
<li>can find a range of values</li>
<li>can use dot notation to index field in subdocuments</li>
<li>can be used to find several distinct values in a single query<a id="more"></a>
首先下载 <a href="https://university.mongodb.com/static/MongoDB_2018_M201_January/handouts/people.json" target="_blank" rel="noopener">people</a> 文件。然后利用下面的命令导入数据库。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongoimport -d m201 -c people --drop people.json</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>然后执行查询命令(进入 mongo shell 中)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.people.find(&#123;<span class="string">"ssn"</span>:<span class="string">"720-38-5636"</span>&#125;).explain(<span class="string">"executionStats"</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到总共检查了 50474 个（people collection 的总 document 数量就是 50474，也就是遍历了整个数据库） documents，返回了 1 个结果。效率很低。<br><img src="result1.png" alt="result1"></p>
<p>接下来我们给 people collection 的 ssn field 创建 indexes。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; db.people.createIndex(&#123;ssn:1&#125;)</span><br><span class="line">&#123;</span><br><span class="line">	&quot;createdCollectionAutomatically&quot; : false,</span><br><span class="line">	&quot;numIndexesBefore&quot; : 1,</span><br><span class="line">	&quot;numIndexesAfter&quot; : 2,</span><br><span class="line">	&quot;ok&quot; : 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以创建一个 object 用于观察查询结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; exp = db.people.explain(&quot;executionStats&quot;)</span><br><span class="line">Explainable(m201.people)</span><br><span class="line">&gt; exp.find(&#123;&quot;ssn&quot;:&quot;720-38-5636&quot;&#125;)</span><br></pre></td></tr></table></figure>

<p>可以看到有了 index 以后，检索的 document 变成了 1。并且 query 时间从 32ms 变成了 4ms。</p>
<p><img src="result2.png" alt="res2"></p>
<h1 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h1><p>任何一次 query 的结果都可以被排序，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找到所有 James 并按照 first name 排序</span></span><br><span class="line">exp.find(&#123;first_name:<span class="string">"James"</span>&#125;).sort(&#123;first_name:1&#125;)</span><br></pre></td></tr></table></figure>

<p>有两种排序方式：</p>
<ol>
<li>从 disk 里面读取 documents，然后做 in memory 排序。</li>
<li>根据 field 生成 index，index 是排序的。</li>
</ol>
<p>1.当我们对没有建立 index 的 field query 后（以上面的first_name 查找排序为例）进行排序。mongoDB 会首先从 collection 里面遍历，完成 query 操作，这会 touch 到所有的 documents。然后 filter 以后的结果，在 memory 中进行排序。如果 query 得到 documents 很多，那么该过程会很费时(mongoDB 会避免超过 32GB 的 in memory 排序)。</p>
<p><img src="sort-in-memory.png" alt="sm"></p>
<p>可以看到这组语句 touch 了所有的 document，并且耗时33ms。</p>
<p>2.如果是 query 建立了 index 的 field 并且再进行排序，比如 ssn。为了保证对比结果在同样的条件，我们限制其 query 结果只用 751条，和上面的相同：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">exp.find(&#123;ssn:&#123;<span class="variable">$gte</span>:<span class="string">"200-00-1000"</span>&#125;&#125;).<span class="built_in">limit</span>(751).sort(&#123;ssn:-1&#125;)</span><br></pre></td></tr></table></figure>

<p><img src="sort-index.png" alt=""></p>
<p>可以看到在同样 751 结果排序，有 index 的只耗费 2ms。这是因为 mongoDB 在对有 index 的 field 进行 query 过程中，是按照已经建立的 index 去 fetch 的，而不是按照 document 的原始顺序。所以即使我们不对 query 结果进行排序，得到的结果也是排好序的（按照当初建立 ssn index 的顺序）。如果我们按照和当初建立 index 的顺序的相反顺序去 sort 结果（建立时候 accending，sort 时候 decending），那么mongoDB 自动就会从后往前去查找 index，这样 query 后的顺序仍然是满足要求的，不需要额外排序（query 时候从 forward 变成了 backward 顺序）。</p>
<h1 id="Compound-Index"><a href="#Compound-Index" class="headerlink" title="Compound Index"></a>Compound Index</h1><p>对多个 field 建立 index 并组合在一起使用，有点像多级排序。比如第一级按照 first name accending，第二级按照 last name accending。</p>
<p>比如在没有建立任何 index 时候，进行如下 query：<br><img src="noindex.png" alt="noindex"></p>
<p>可以看到为了查到 Jasmine Frazier，遍历了整个 document，耗时62ms。</p>
<p>然后建立 last_name 的 index，重复查询：<br><img src="1index.png" alt="1index"></p>
<p>可以看到只检查了 31 个 document，耗时 0ms。效率明显提升。</p>
<p>接下来我们建立 compound index：<br><img src="cpindex.png" alt="cpindex"></p>
<p>然后重复查询：<br><img src="cpresult.png" alt="cpresult"></p>
<p>可以看到在 compound index 下，只 touch 了一个 document。效率更高。</p>
<h1 id="Multikey-index"><a href="#Multikey-index" class="headerlink" title="Multikey index"></a>Multikey index</h1><h1 id="Partial-index"><a href="#Partial-index" class="headerlink" title="Partial index"></a>Partial index</h1><h1 id="Text-index"><a href="#Text-index" class="headerlink" title="Text index"></a>Text index</h1>]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>mongoDB 基础</title>
    <url>/blog/mongo-beginner/</url>
    <content><![CDATA[<h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><ul>
<li>NoSQL 数据库</li>
<li>compass app: 连接 mongodb atlas 的官方 app</li>
<li>mongod: mongoDB 的 database application</li>
<li>mongo: 提供和 database 进行交互的 shell 接口 application</li>
<li>向 mongodb atlas 上传数据<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongoimport --<span class="built_in">type</span> csv --headerline --db mflix --collection movies_initial --host <span class="string">"cluster0-shard-00-00-1cvum.mongodb.net:27017,cluster0-shard-00-01-1cvum.mongodb.net:27017,cluster0-shard-00-02-1cvum.mongodb.net:27017"</span> --authenticationDatabase admin --ssl --username hansonzhao007 --password XXXXXX --file movies_initial.csv</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="Aggregation-Framework"><a href="#Aggregation-Framework" class="headerlink" title="Aggregation Framework"></a>Aggregation Framework</h1><h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><img src="pipeline.png" alt="pipeline"><br>从 collection 里面获得数据，并输入到后面的 stages，每个都进行不同的操作。这些输入输出都是 documents。</li>
</ul>
<h2 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h2><p><img src="stages.png" alt="stages"><br>stage 里面可以定义自己对 document 的操作。可以是 reshape，accumulation 等等。</p>
<p><img src="stage-function.png" alt="stage-function"><br>这里的 stage1 可以用来过滤输入数据，不用对每个数据都处理。stage2 可以用来自定义一些数据操作；stage3 可以再次过滤一下。</p>
<h2 id="write-a-pipeline"><a href="#write-a-pipeline" class="headerlink" title="write a pipeline"></a>write a pipeline</h2><p>下图是python 写 mongodb pipeline 的图例：<br><img src="pipeline-python.png" alt="python-pipeline"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="keyword">import</span> pprint</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</span><br><span class="line"></span><br><span class="line">client = MongoClient(<span class="string">"mongodb+srv://hansonzhao007:yourpassword@cluster0-1cvum.mongodb.net"</span>)</span><br><span class="line"></span><br><span class="line">pipeline = [</span><br><span class="line">    <span class="comment"># group stage</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'$group'</span>:&#123;</span><br><span class="line">            <span class="string">'_id'</span>:&#123;<span class="string">"language"</span>:<span class="string">"$language"</span>&#125;,</span><br><span class="line">            <span class="string">'count'</span>:&#123;<span class="string">'$sum'</span>: <span class="number">1</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># sort stage</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">'$sort'</span>:&#123;<span class="string">'count'</span>: <span class="number">-1</span>&#125; <span class="comment"># decending order, 1 is accending order</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># # a simplify version is:</span></span><br><span class="line"><span class="comment"># pipeline = [</span></span><br><span class="line"><span class="comment">#   &#123;</span></span><br><span class="line"><span class="comment">#     '$sortByCount':"$language"</span></span><br><span class="line"><span class="comment">#   &#125;</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line"></span><br><span class="line">clear_output()</span><br><span class="line">pprint.pprint(list(client.mflix.movies_initial.aggregate(pipeline)))</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[&#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">'English'</span>&#125;, <span class="string">'count'</span>: 25325&#125;,</span><br><span class="line"> &#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">'French'</span>&#125;, <span class="string">'count'</span>: 1784&#125;,</span><br><span class="line"> &#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">'Italian'</span>&#125;, <span class="string">'count'</span>: 1480&#125;,</span><br><span class="line"> &#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">'Japanese'</span>&#125;, <span class="string">'count'</span>: 1290&#125;,</span><br><span class="line"> &#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">''</span>&#125;, <span class="string">'count'</span>: 1115&#125;,</span><br><span class="line"> &#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">'Spanish'</span>&#125;, <span class="string">'count'</span>: 875&#125;,</span><br><span class="line"> &#123;<span class="string">'_id'</span>: &#123;<span class="string">'language'</span>: <span class="string">'Russian'</span>&#125;, <span class="string">'count'</span>: 777&#125;,</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure>

<h1 id="indexes"><a href="#indexes" class="headerlink" title="indexes"></a>indexes</h1><h2 id="加速"><a href="#加速" class="headerlink" title="加速"></a>加速</h2><p>使用 index 来加速 mongodb 的查找。</p>
<p><img src="indexes.png" alt="indexes"></p>
<p>collection 就像是一本书，document 就是书里面的章节内容，而 indexes 的作用就是章节目录。如果没有章节目录，我们想要搜索想要的内容，就只能把书从头找到尾，耗时O(N)。如果有目录，目录按照字母顺序排序，那么想要某一章节，就可以对目录进行 binary search，耗时 O(N)，然后直接定位到对应章节。</p>
<p>同时为了满足针对不同 filed 的索引需求，mongodb 的一个 collection 可以有多个 index，用来进行索引。</p>
<p><img src="multi-indexes.png" alt="multi-indexes"></p>
<p>mongodb 使用 B tree 来对 index 进行索引。</p>
<p><img src="btree-index.png" alt="btree-index"></p>
<p>下面是使用 index 和不使用 index 进行搜索时，所检测的 document 数量对比。</p>
<p><img src="wovsw.png" alt="wovsw"></p>
<h2 id="overhead-额外开销"><a href="#overhead-额外开销" class="headerlink" title="overhead 额外开销"></a>overhead 额外开销</h2><p>虽然使用 index 对查找进行加速，但是这会对插入引入额外的 overhead（开销），也就是增加了插入数据的时间开销。</p>
<p>这是因为每个 index 的组织结构是 b-tree。在 collection 中插入 document 的时候，会更新 b-tree 的数据，必要时还需要做 rebalance 操作。</p>
<h1 id="组织结构"><a href="#组织结构" class="headerlink" title="组织结构"></a>组织结构</h1><p>数据是怎么存储的。</p>
<p><img src="organization.png" alt="org"></p>
<p><img src="org2.png" alt="org2"></p>
<p>在 mongodb 存储数据库的文件目录下，有这些文件：<br><img src="files.png" alt="file"></p>
<ul>
<li><code>collection-*.wt</code>: 数据库的 collection 文件</li>
<li><code>index-*.wt</code>: 数据库建立的 index 文件</li>
</ul>
<p>现在的数据库组织形式还是 flat 的，也就是所有数据库文件存储在一个目录下。</p>
<p>我们可以使用 <code>--directoryperdb</code> 命令让 mongodb 将数据库存储时候按照文件夹区分开：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongod --dbpath --/data/db --fork --logpath /data/db/mongodb.log --directoryperdb</span><br></pre></td></tr></table></figure>

<p>这样再看目录：<br><img src="files2.png" alt="file2"><br>可以看到每个数据库都有自己的文件夹目录了。但是在文件目录里面，比如 local 文件里面，collection 和 index 文件是 flat 的平铺在里面的，并没有区分文件夹。<br><img src="dir.png" alt="dir"></p>
<p>我们还可以使用 <code>--wiredTigerDirectoryForIndexes</code> 命令，在数据库文件夹里，给 collection 和 index 各自也建立文件夹。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongod --dbpath --/data/db --fork --logpath /data/db/mongodb.log --directoryperdb --wiredTigerDirectoryForIndexes</span><br></pre></td></tr></table></figure>

<p>这时候再看 local 文件夹里：<br><img src="subdir.png" alt="subdir"></p>
<p>已经有了 collection 和 index 两个文件夹了。</p>
<p>之所以这么做，是为了提升数据库的性能。我们可以将 collection 和 index 放在两个不同的 disk 上。这样利用两个 disk 的 parallel 读写（parallel I/O），提升 mongodb 的数据吞吐量。<br><img src="parallel.png" alt="parallel"></p>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>【WEP】CPU Caching</title>
    <url>/blog/wep-cache/</url>
    <content><![CDATA[<p>hierarchy<br><img src="introduction.png" alt=""><br>Program code and data has <code>temporal</code> and <code>spatial locality</code>. This means that, over short periods of time, there is a good chance that the same code or data gets reused.</p>
<p>For code this means that there are most likely loops in the code so that the same code gets executed over and over again (the perfect case for <code>spatial locality</code>对于刚被访问的数据，其相邻的数据在将来被访问的概率高). Data accesses are also ideally limited to small regions. Even if the memory used over short time periods is not close together there is a high chance that the same data will be reused before long (<code>temporal locality</code>对于刚被访问的数据，其本身在将来被访问的概率高).</p>
<a id="more"></a>
<p>Assume access to main memory takes 200 cycles and access to the cache memory take 15 cycles. Then code using 100 data elements 100 times each will spend 2,000,000 cycles on memory operations if there is no cache and only 168,500 cycles if all data can be cached. That is an improvement of 91.5%.</p>
<h1 id="3-1-CPU-Caches-in-the-Big-Picture"><a href="#3-1-CPU-Caches-in-the-Big-Picture" class="headerlink" title="3.1 CPU Caches in the Big Picture"></a>3.1 CPU Caches in the Big Picture</h1><p><img src="Selection_001.png" alt="Figure 3.1"><br>Figure 3.1 shows the minimum cache configuration. It corresponds to the architecture which could be found in early systems which deployed CPU caches. The CPU core is no longer directly connected to the main memory.</p>
<p><img src="Selection_002.png" alt="Figure 3.2"><br>Figure 3.2 shows three levels of cache and introduces the nomenclature we will use in the remainder of the document. L1d is the level 1 data cache, L1i is the level 1 instruction cache, etc.</p>
<p> cpu cache<br><img src="Selection_003.png" alt="Figure 3.3"><br><img src="mac_cpu_cache_info.png" alt="Figure cache ex"><br>In this figure we have two processors, each with two cores, each of which has two threads. The threads share the Level 1 caches. The cores (shaded in the darker gray) have individual Level 1 caches. All cores of the CPU share the higher-level caches. The two processors (the two big boxes shaded in the lighter gray) of course do not share any caches. All this will be important, especially when we are discussing the cache effects on multiprocess and multi-thread applications.</p>
<h1 id="3-2-Cache-Operation-at-High-Level"><a href="#3-2-Cache-Operation-at-High-Level" class="headerlink" title="3.2 Cache Operation at High Level"></a>3.2 Cache Operation at High Level</h1><p><em>By default all data read or written by the CPU cores is stored in the cache</em>. There are memory regions which cannot be cached but this is something only the OS implementers have to be concerned about; it is not visible to the application programmer. There are also instructions which allow the programmer to deliberately <code>bypass certain caches</code>. 默认情况从CPU cache中读写数据，内存中也有不能cache的部分，但这是由操作系统关心的，无关 programmer。也可以通过特定的指令，绕过 cache。</p>
<p>If the CPU needs a data word the caches are searched first. Obviously, the cache cannot contain the content of the entire main memory (otherwise we would need no cache), but since all memory addresses are cacheable, each cache entry is <code>tagged</code> using the address of the data word in the main memory. This way a request to read or write to an address can search the caches for a matching tag. The address in this context can be either the virtual or physical address, varying based on the cache implementation.</p>
<p>Since for the tag, in addition to the actual memory, additional space is required, it is inefficient to chose a word as the granularity of the cache. For a 32-bit word on an x86 machine the tag itself might need 32 bits or more. Furthermore, since spatial locality is one of the principles on which caches are based, it would be bad to not take this into account. Since neighboring memory is likely to be used together it should also be loaded into the cache together. In early caches these lines were 32 bytes long; nowadays the norm is 64 bytes. <code>If the memory bus is 64 bits wide this means 8 transfers per cache line</code>. DDR supports this transport mode efficiently.</p>
<p>When memory content is needed by the processor the entire cache line is loaded into the L1d. The memory address for each cache line is computed by masking the address value according to the cache line size. For a 64 byte cache line this means the low 6 bits are zeroed. The discarded bits are used as the offset into the cache line. The remaining bits are in some cases used to locate the line in the cache and as the tag. In practice an address value is split into three parts. For a 32-bit address it might look as follows:</p>
<p><img src="Selection_004.png" alt=""></p>
<p>With a cache line size of $2^O$ the low $O$ bits are used as the offset into the cache line. The next $S$ bits select the “cache set”.</p>
<p>For now it is sufficient to understand there are $2^S$ sets of cache lines. This leaves the top $32−S−O = T$ bits which form the tag. These $T$ bits are the value associated with each cache line to distinguish all the aliases(<code>All cache lines with the same S part of the address are known by the same alias</code>) which are cached in the same cache set. The $S$ bits used to address the cache set do not have to be stored since they are the same for all cache lines in the same set. The $S$ bits used to address the cache set do not have to be stored since they are the same for all cache lines in the same set.</p>
<p>When an instruction modifies memory the processor still has to load a cache line first because no instruction modifies an entire cache line at once (exception to the rule: write-combining as explained in section 6.1). The content of the cache line before the write operation therefore has to be loaded. It is not possible for a cache to hold partial cache lines. A cache line which has been written to and which has not been written back to main memory is said to be “dirty”. Once it is written the dirty flag is cleared. 当instruction修改memory时候，仍然需要load cache line。已经被written但是并没有被写会 memory 的 cache line，会被成为 <code>“dirty”</code>。一旦被写回 memory，那么 dirty flag 就会被 cleard。</p>
<p><code>cache coherency</code>:<br>MESI</p>
<ul>
<li>A dirty cache line is not present in any other processor’s cache.</li>
<li>Clean copies of the same cache line can reside in arbitrarily many caches.</li>
</ul>
<p>Costs associated with cache hits and misses</p>
<p><img src="cache_cost.png" alt=""><br>For the on-die L2 cache a large part (probably even the majority) of the access time is caused by wire delays. This is a physical limitation which can only get worse with increasing cache sizes. Only process shrinking (for instance, going from 60nm for Merom to 45nm for Penryn in Intel’s lineup) can improve those numbers. 片上 L2 cache 的瓶颈在wire delay上，只有缩小尺寸才能改善，比如从 60nm 工艺变成 45nm 工艺。</p>
<h1 id="CPU-Cache-Implementation-Details"><a href="#CPU-Cache-Implementation-Details" class="headerlink" title="CPU Cache Implementation Details"></a>CPU Cache Implementation Details</h1><h2 id="Associativity"><a href="#Associativity" class="headerlink" title="Associativity"></a>Associativity</h2><ul>
<li>Fully Associative Cache：<br>数据 A 可存放在 cache 的任意位置。 设计复杂，特别当 cache size 很大时，硬件成本非常之高。<br>It would be possible to implement a cache where each cache line can hold a copy of any memory location.</li>
</ul>
<p><img src="Selection_005.png" alt="Figure3.5"> <img src="fully.png" alt=""><br>To access a cache line the processor core would have to compare the tags of each and every cache line with the tag for the requested address. The tag would be comprised of the entire part of the address which is not the offset into the cache line (that means, S is zero).<br>Given a <code>4MB cache with 64B cache lines</code>the cache would have <code>65,536 entries</code>. To achieve adequate performance the cache logic would have to be able to pick from all these entries the one matching a given tag in just a few cycles.<br>Fully associative caches are practical for small caches (for instance, the TLB caches on some Intel processors are fully associative) but those caches are small, really small. We are talking about a few dozen entries at most.</p>
<ul>
<li>Direct-Mapped Cache<br>数据 A 在 cache 的存放位置只有固定一处。从设计结构看，数据所在cache line，直接由 Set 决定，这样对应的Tag就只能放置在Set所决定的位置（拥有相同Set的data只能放置在相同的位置）。而不能如 Fully 那样放在任意cache line。这是由设计结构决定的。直接后果就是当 program 使用的 memory 并不 evenly 分布时候，某些 cache line 会被过度使用的同时，另一些可能一直空着，所以容易产生碰撞，最终降低 cache 的命中率，影响性能。</li>
</ul>
<p><img src="Selection_006.png" alt=""> <img src="direct.png" alt=""></p>
<p>It requires exactly one comparator, one multiplexer (two in this diagram where tag and data are separated, but this is not a hard requirement on the design), and some logic to select only valid cache line content).<br>The number of transistors in a simple multiplexer grows with O(log N), where N is the number of cache lines.<br>This is tolerable but might get slow, in which case speed can be increased by spending more real estate on transistors in the multiplexers to parallelize some of the work and to increase the speed.</p>
<p>It has a <code>drawback</code>: it only works well if the addresses used by the program are evenly distributed with respect to the bits used for the direct mapping. If they are not, and this is usually the case, some cache entries are heavily used and therefore repeated evicted while others are hardly used at all or remain empty.</p>
<ul>
<li>N-Way Set-Associative Cache:<br>数据 A 在 cache 的存放位置可以有 N 处。同样的Set可以同时放多个Tag的data，由N决定。</li>
</ul>
<p><img src="Selection_007.png" alt=""> <img src="set_associative.png" alt=""></p>
<p>结构图中，同一Set对应图中同一列，并且此结构图是4 way 的（way是行）。我PC的cache配置，L2 cache 256K, 8-way Set-associative, 512个set，cache line 64B。每个方块对应 cache Line 大小 64，所以总 cache 大小是 row<em>col</em>size_of_one_cell = 8（way） × 512（Set size）× 64B = 256KB。符合我的配置参数。这样的话，由 Set 和 Offset 组成的 (9+6) 15bit ，决定了 地址会每隔 $2^15$，即32768 Byte 数据，就落在cache 的同一个 set 中。当一个 Set 被选中，那么 N-way 指向的 N 个 cache line address 需要同时被比较，以确定data 是否在 cache 中。<br>A set-associative cache combines the good features of the full associative and direct-mapped caches to largely avoid the weaknesses of those designs.cd<br>The tag and data storage are divided into sets, one of which is selected by the address of a cache line. This is similar to the direct-mapped cache. But <code>instead of only having one element for each set value in the cache, a small number of values is cached for the same set value.</code> The tags for all the set members are compared in parallel, which is similarr to the functioning of the fully associative cache.</p>
<p>Today processors are using associativity levels of up to 24 for L2 caches or higher. L1 caches usually get by with 8 sets.<br>we can use <code>sudo dmidecode -t cache</code> cmd to see configuration of cache.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Handle <span class="number">0x0004</span>, DMI type <span class="number">7</span>, <span class="number">19</span> bytes</span><br><span class="line">Cache Information</span><br><span class="line">	Socket Designation: L2-Cache</span><br><span class="line">	Configuration: Enabled, Not Socketed, Level <span class="number">2</span></span><br><span class="line">	Operational Mode: Write Through</span><br><span class="line">	Location: Internal</span><br><span class="line">	Installed Size: <span class="number">256</span> kB</span><br><span class="line">	Maximum Size: <span class="number">256</span> kB</span><br><span class="line">	Supported SRAM Types:</span><br><span class="line">		Unknown</span><br><span class="line">	Installed SRAM Type: Unknown</span><br><span class="line">	Speed: Unknown</span><br><span class="line">	Error Correction Type: Multi-<span class="built_in">bit</span> ECC</span><br><span class="line">	System Type: Unified</span><br><span class="line">	Associativity: <span class="number">8</span>-way Set-associative</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>cache <code>命中率</code>：</p>
<p><img src="Selection_008.png" alt=""><br>we can see that associativity can indeed help to reduce the number of cache misses significantly.<br>In general, increasing the associativity of a cache <code>above 8 seems to have little effects</code> for a single-threaded workload. With the introduction of <code>hyper-threaded</code> processors where the first level cache is shared and multi-core processors which use a shared L2 cache the situation changes. Now you basically have two programs hitting on the same cache which causes the associativity in practice to be halved (or quartered for quad-core processors). So it can be expected that, with increasing numbers of cores, the associativity of the shared caches should grow. Once this is not possible anymore (16-way set associativity is already hard) processor designers have to start using shared L3 caches and beyond, while L2 caches are potentially shared by a subset of the cores.</p>
<h2 id="Measurements-of-Cache-Effects"><a href="#Measurements-of-Cache-Effects" class="headerlink" title="Measurements of Cache Effects"></a>Measurements of Cache Effects</h2><p>使用一个链表来测试 Cache 影响。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">l</span> &#123;</span></span><br><span class="line">   <span class="class"><span class="keyword">struct</span> <span class="title">l</span> *<span class="title">n</span>;</span></span><br><span class="line">   <span class="keyword">long</span> <span class="keyword">int</span> pad[NPAD];</span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure>
<p>如果 NPAD = 7。在 32bit 机器上，每个 element 大小 32byte，在 64bit 机器上，element 大小 64byte。</p>
<h3 id="Single-Threaded-Sequential-Access"><a href="#Single-Threaded-Sequential-Access" class="headerlink" title="Single Threaded Sequential Access"></a>Single Threaded Sequential Access</h3><p>测试环境：64bit机器，16KB L1d，1MB L2<br>纵轴是 CPU Cycle。Y–axis shows the average number of CPU cycles it takes to process one element;</p>
<p><img src="Selection_010.png" alt=""><br>可以看到图上有三个段：</p>
<ul>
<li>Up to a working set size of $ 2^{14}$ bytes. （L1d影响）</li>
<li>From $2^{15}$ bytes to $2^{20}$ bytes.（L2影响）</li>
<li>From $2^{21}$ bytes and up.</li>
</ul>
<p>We do not see sharp edges in the transition from one level to the other because the caches are used by other parts of the system as well and so the cache is not exclusively available for the program data. Specifically the L2 cache is a unified cache and also used for the instructions.<br>第二段，当L1d的大小不足够 hold all data 的时候，需要 L2 存储。但是从之前我们可以看到 L2 的 access time 需要约 14 cycle，这里却只需要 9 cycle 左右。这是因为 CPU 有一个 <code>prefetch</code> 的操作，即当开始使用一个新的 cache line 的时候，它已经被 cache 一部分了。<br>而 prefetch 的作用，在 working set 超过 L2 大小的时候，尤其看得见。之前说过，memory access 需要 200+ cycle，只有在有效的 prefetching下，才可能维持住 9 cycle 的 access time。</p>
<p>在第三段中，prefetch起到了非常好的作用。将之前所说的 main memory access time 从 200+ 降到了 9。</p>
<p><em>下面看同样working set测试下，不同的 element 大小对时间影响</em>：</p>
<p><img src="Selection_011.png" alt=""><br>element size 越大（NPAD越大），说明 working set 的 element 个数越少。同时意味着，每个 element 之间的 distance 变大了。在上面的四个测试环境下，distance 是 0, 56, 120, 248 byte。（NPAD×8）</p>
<p>从上面图中，仍然可以看到 cache 对 access time 的影响，仍然可以看到被分成三段。只不过随着 distance 增加，access time 的尺度增长的越来越快。</p>
<p>在第二段中（working set size 小于 L2 size），可以看到 L2 的 access time 已经在 28 左右了。说明 CPU 的 prefetch (from L2 into L1d) 没有起到作用。当 NPAD=7，即一个数据 64B，每读一个数据，就要 load 一个新的 cache line；而 NPAD=0，一个数据4B，一个 cache line 可以存 8 个数据，那么 loop 循环 8 次以后，才需 load 下一个 cache line，所以 prefetch logic 可以 load 一部分 element（NPAD=0时）。因为prefetch 不能 load 整个 next cache line，所以在 element 过大的时候，L2 到 L1d 的 prefetch 不再起作用。 每次都需要从L2中 load 数据。</p>
<p>第三段中，working set 超过了L2 capacity，prefetch 更起不了作用。</p>
<p>另外一个导致 access time slowdown 的原因是TLB cache 的misses。The TLB cache is quite small since it has to be extremely fast. If more pages are accessed repeatedly than the TLB cache has entries for the translation from virtual to physical address has to be constantly repeated. This is a very costly operation. With larger element sizes the cost of a TLB lookup is amortized over fewer elements. That means the total number of TLB entries which have to be computed per list element is higher. 当 element size 过大，那么 每次 TLB 查找获得的 element 个数就少了，意味着间隔更少的元素，就要进行一次 新的page table 查找以更新 TLB cache，从而 <code>耗时/element</code> 大幅度增加。</p>
<ul>
<li>下面观察 TLB 对 access time 的影响：*</li>
</ul>
<ul>
<li><p>use NPAD=7 for elements which occupy one entire cache line</p>
</li>
<li><p>place each list element on a separate page. The rest of each page is left untouched and we do not count it in the total for the working set size</p>
<p>The consequence is that, for the first measurement, each list iteration requires a new cache line and, for every 64 elements, a new page. For the second measurement each iteration requires loading a new cache line which is on a new page.<br>第一个实验，每次 iteration a new element 就要 fetch 一个 new cache line，每 64 个 fetch 一个新 page。<br>第二个实验，每次 iteration a new element 就要 fetch 一个 new cache line，每 1   个 fetch 一个新 page。</p>
</li>
</ul>
<p><img src="Selection_012.png" alt=""><br>We see the distinct steps showing the sizes of the L1d and L2 caches. The second curve looks radically different. The important feature is the huge spike starting when the working set size reaches $2^{13}$ bytes. This is when the TLB cache overflows. With an element size of 64 bytes we can compute that the TLB cache has 64 entries（$2^{13}$开始溢出，说明$2^{12}$正好填满 TLB，那么$2^{12}$ byte 数据，总共有 64byte 的 element 64 个，因为这里的测试条件下，每个 element 都会请求一个新的page，所以 TLB 的entries number 是 64）. There are no page faults affecting the cost since the program locks the memory to prevent it from being swapped out.</p>
<p><em>下面观察 prefetch 的影响</em></p>
<p><img src="Selection_013.png" alt=""></p>
<ul>
<li>The element width is in all cases 16 bytes. The first line is the now familiar list walk which serves as a baseline.</li>
<li>The second line, labeled “Inc”, simply increments the pad[0] member of the current element before going on to the next. （将当前pad[0] 增加1）</li>
<li>The third line, labeled “Addnext0”, takes the pad[0] list element of the next element and adds it to the pad[0] member of the current list element （将下一个 element 的 pad[0] 加到当前的 pad[0] 上）<br>The na¨ıve assumption would be that the “Addnext0” test runs slower because it has more work to do. Before advancing to the next list element a value from that element has to be loaded. This is why it is surprising to see that this test actually runs, for some working set sizes, faster than the “Inc” test. The explanation for this is that the load from the next list element is basically a forced prefetch. Whenever the program advances to the next list element we know for sure that element is already in the L1d cache. As a result we see that the “Addnext0” performs as well as the simple “Follow” test <code>as long as the working set size fits into the L2 cache</code>. （<code>为什么第二段在L2 cache 的时候，Addnext0会比 inc 快呢</code>）</li>
</ul>
<p>The “Addnext0” test runs out of L2 faster than the “Inc” test, though. It needs more data loaded from main memory. This is why the “Addnext0” test reaches the 28 cycles level for a working set size of 2 21 bytes. The 28 cycles level is <code>twice</code> as high as the 14 cycles level the “Follow” test reaches. This is easy to explain, too. Since the other two tests modify memory an L2 cache eviction to make room for new cache lines cannot simply discard the data. Instead <code>it has to be written to memory</code>（所以就会有读写，这两次memory access，所以时间是两倍）. This means the available bandwidth on the FSB is cut in half, hence doubling the time it takes to transfer the data from main memory to L2.</p>
<p>to be continue…</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://lwn.net/Articles/252125/" target="_blank" rel="noopener">CPU caches</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】19 Paging:Faster Translations</title>
    <url>/blog/Paging/</url>
    <content><![CDATA[<p>TLB stands for translation-lookaside buffer.<br>A TLB is part of the chip’s memory-management unit (<code>MMU</code>), and is simply a <code>hardware cache</code> of popular virtual-to-physical address translations.; thus, a better name would be an <code>address-translation cache</code></p>
<p>Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible. 有了 TLB，在访问 memory 的时候，如果 desired translation 在 TLB，则直接 translation，而不用经过 page table 的转义。</p>
<h1 id="TLB-Basic-Algorithm"><a href="#TLB-Basic-Algorithm" class="headerlink" title="TLB Basic Algorithm"></a>TLB Basic Algorithm</h1><p>假设：</p>
<ol>
<li>linear page table</li>
<li>a hardware-managed TLB<a id="more"></a>
<img src="Selection_001.png" alt=""><br>The algorithm the hardware follows works like this: first, extract the virtual page number (<code>VPN</code>) from the virtual address (Line 1 in Figure 19.1), and check if the TLB holds the translation for this VPN (Line 2). If it does, we have a TLB hit, which means the TLB holds the translation. Success! We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory (Lines 5–7), assuming protection checks do not fail (Line 4).</li>
</ol>
<p>If the CPU does not find the translation in the TLB (a <code>TLB miss</code>), we have some more work to do.</p>
<h1 id="Example-Accessing-An-Array"><a href="#Example-Accessing-An-Array" class="headerlink" title="Example: Accessing An Array"></a>Example: Accessing An Array</h1><p><img src="Selection_002.png" alt=""><br>assume we have an array of 10 4-byte integers in memory, starting at virtual address 100. Assume further that we have a small 8-bit virtual address space, with 16-byte pages; thus, a virtual address breaks down into a 4-bit VPN (there are 16 virtual pages) and a 4-bit offset (there are 16 bytes on each of those pages).</p>
<p>As you can see, the array’s first entry (a[0]) begins on (VPN=06, offset=04); only three 4-byte integers fit onto that page. The array continues onto the next page (VPN=07), where the next four entries (a[3] … a[6]) are found. Finally, the last three entries of the 10-entry array (a[7] … a[9]) are located on the next page of the address space (VPN=08).</p>
<p>Now let’s consider a simple loop:</p>
<p><img src="Selection_003.png" alt=""><br> The elements of the array are packed tightly into pages (i.e., they are close to one another in space), and thus only the first access to an element on a page yields a TLB miss.</p>
<p>CISC computers use hardware-managed TLB<br>RISC computers use software-managed TLB</p>
<h1 id="TLB-Contents-What’s-In-There"><a href="#TLB-Contents-What’s-In-There" class="headerlink" title="TLB Contents: What’s In There?"></a>TLB Contents: What’s In There?</h1><p>A TLB entry might look like this:<br>$$VPN | PFN | other bits$$</p>
<p>The TLB is known as a <code>fully-associative cache</code>). The hardware <code>searches</code> the entries in <code>parallel</code> to see if there is a match</p>
<h1 id="TLB-Issue-Context-Switches"><a href="#TLB-Issue-Context-Switches" class="headerlink" title="TLB Issue: Context Switches"></a>TLB Issue: Context Switches</h1><p>With TLBs, some new issues arise when switching between processes (and hence address spaces).</p>
<p>The TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.</p>
<p>Assuming two process run in CPU and the TLB will be like following:</p>
<p><img src="Selection_005.png" alt=""><br>In the TLB above, we clearly have a problem: VPN 10 translates to either PFN 100 (P1) or PFN 170 (P2), but the hardware can’t distinguish which entry is meant for which process.</p>
<ul>
<li>One approach is to simply flush the TLB on context switches, thus emptying it before running the next process.<br>However, there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high</li>
<li>To reduce this overhead, some systems add hardware support to enable sharing of the TLB across context switches. In particular, some hardware systems provide an<code>address space identifier (ASID)</code> field in the TLB.</li>
</ul>
<p><img src="Selection_004.png" alt=""><br>Thus, with address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf" target="_blank" rel="noopener">Translation Lookaside Buffers</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】18 Introduction to Paging</title>
    <url>/blog/intro-to-paging/</url>
    <content><![CDATA[<p>将 memory 分割成 fixed-sized 块，这就叫 <code>paging</code></p>
<p>We divide address space into fixed-sized units, each of which we call a <code>page</code>. Correspondingly, we view physical memory as an array of fixed-sized slots called <code>page frames</code>.</p>
<h1 id="A-Simple-Example-And-Overview"><a href="#A-Simple-Example-And-Overview" class="headerlink" title="A Simple Example And Overview"></a>A Simple Example And Overview</h1><p>OS may find free spaces from a <code>free list</code> to your program, and there will be a mapping relationship between physical address and virtual address.</p>
<p><img src="Selection_001.png" alt=""><br>To record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a <code>per-process</code> data structure known as a <code>page table</code>.</p>
<a id="more"></a>
<p>The major role of the page table is to store <code>address translations</code> for each of the virtual pages of the address space.</p>
<p>It is important to remember that this page table is a per-process data structure (most page table structures we discuss are per-process structures; an exception we’ll touch on is the <code>inverted page table</code>).</p>
<p><code>page table entry (PTE)</code></p>
<p><img src="Selection_002.png" alt=""></p>
<ul>
<li>A <code>valid bit</code> is common to indicate whether the particular translation is valid</li>
<li><code>protection bits</code>, indicating whether the page could be read from, written to, or executed from</li>
<li>A <code>present bit</code> indicates whether this page is in physical memory or on disk (i.e., it has been swapped out). We will understand this machinery further when we study how to swap parts of the address space to disk to support address spaces that are larger than physical memory; swapping allows the OS to free up physical memory by moving rarely-used pages to disk.</li>
<li>A <code>dirty</code> bit is also common, indicating whether the page has been modified since it was brought into memory</li>
<li>A <code>reference</code> bit (a.k.a. <code>accessed bit</code>) is sometimes used to track whether a page has been accessed, and is useful in determining which pages are popular and thus should be kept in memory; such knowledge is critical during page replacement<h1 id="Paging-can-be-slow"><a href="#Paging-can-be-slow" class="headerlink" title="Paging can be slow"></a>Paging can be slow</h1></li>
</ul>
<p><img src="Selection_003.png" alt=""><br>For every memory reference (whether an instruction fetch or an explicit load or store), <code>paging</code> requires us to <code>perform one extra memory reference</code> in order to first fetch the translation from the page table. That is a lot of work! Extra memory references are costly, and in this case will likely slow down the process by a factor of two or more.</p>
<p>看下面一段代码：</p>
<p><img src="Selection_006.png" alt=""><br>汇编后的：</p>
<p><img src="Selection_005.png" alt=""><br>%edi holds the base address of the array, whereas %eax holds the array index (i);</p>
<p><img src="Selection_004.png" alt=""><br>When it runs, each instruction fetch will generate two memory references:</p>
<ul>
<li>one to the page table to find the physical frame that the instruction resides within</li>
<li>one to the instruction itself to fetch it to the CPU for processing</li>
<li>there is one explicit memory reference in the form of the mov instruction; this adds another page table access first (to translate the array virtual address to the correct physical one) and then the array access itself.</li>
</ul>
<p>图解：</p>
<ol>
<li>assume we have a linear (array-based) page table and that it is located at physical address 1KB (1024).</li>
<li>the page size is 1KB, virtual address 1024 resides on the second page of the virtual address space (VPN=1, as VPN=0 is the first page). Let’s assume this virtual page maps to physical frame 4 (VPN 1 → PFN 4).</li>
<li>array itself. Its size is 4000 bytes (1000 integers), and we assume that it resides at virtual addresses 40000 through 44000</li>
<li>Let’s assume these virtual-to-physical mappings for the example: (VPN 39 → PFN 7), (VPN 40 → PFN 8), (VPN 41 → PFN 9), (VPN 42 → PFN 10)</li>
</ol>
<p>there are 10 memory accesses per loop, which includes four instruction fetches, one explicit update of memory, and five page table accesses to translate those four fetches and one explicit update.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/vm-paging.pdf" target="_blank" rel="noopener">Introduction to Paging</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker is not VM</title>
    <url>/blog/dock-not-vm/</url>
    <content><![CDATA[<p>I spend a good portion of my time at Docker talking to community members with varying degrees of familiarity with Docker and I sense a common theme: people’s natural response when first working with Docker is to try and frame it in terms of virtual machines. I can’t count the number of times I have heard Docker containers described as “lightweight VMs”.</p>
<p>I get it because I did the exact same thing when I first started working with Docker. It’s easy to connect those dots as both technologies share some characteristics. Both are designed to provide an isolated environment in which to run an application. Additionally, in both cases that environment is represented as a binary artifact that can be moved between hosts. There may be other similarities, but to me these are the two biggies.</p>
<a id="more"></a>
<p>The key is that the underlying architecture is fundamentally different between the two. The analogy I use (because if you know me, you know I love analogies) is comparing houses (VMs) to apartment buildings (containers).</p>
<p>Houses (the VMs) are fully self-contained and offer protection from unwanted guests. They also each possess their own infrastructure – plumbing, heating, electrical, etc. Furthermore, in the vast majority of cases houses are all going to have at a minimum a bedroom, living area, bathroom, and kitchen. I’ve yet to ever find a “studio house” – even if I buy the smallest house I may end up buying more than I need because that’s just how houses are built.  (for the pedantic out there, yes I’m ignoring the new trend in micro houses because they break my analogy)</p>
<p>Apartments (the containers) also offer protection from unwanted guests, but they are built around shared infrastructure. The apartment building (Docker Host) shares plumbing, heating, electrical, etc. Additionally apartments are offered in all kinds of different sizes – studio to multi-bedroom penthouse. You’re only renting exactly what you need. Finally, just like houses, apartments have front doors that lock to keep out unwanted guests.</p>
<p>With containers, you share the underlying resources of the Docker host and you build an image that is exactly what you need to run your application. You start with the basics and you add what you need. VMs are built in the opposite direction. You are going to start with a full operating system and, depending on your application, might be strip out the things you don’t want.</p>
<p>I’m sure many of you are saying “yeah, we get that. They’re different”. But even as we say this, we still try and adapt our current thoughts and processes around VMs and apply them to containers.</p>
<ul>
<li>“How do I backup a container?”</li>
<li>“What’s my patch management strategy for my running containers?”</li>
<li>“Where does the application server run?”</li>
</ul>
<p>To me the light bulb moment came when I realized that Docker is not a virtualization technology, it’s an application delivery technology. In a VM-centered world, the unit of abstraction is a monolithic VM that stores not only application code, but often its stateful data. A VM takes everything that used to sit on a physical server and just packs it into a single binary so it can be moved around.  But it is still the same thing.  With containers the abstraction is the application; or more accurately a service that helps to make up the application.</p>
<p>With containers, typically many services (each represented as a single container) comprise an application. Applications are now able to be deconstructed into much smaller components which fundamentally changes the way they are managed in production.</p>
<p>So, how do you backup your container, you don’t. Your data doesn’t live in the container, it lives in a named volume that is shared between 1-N containers that you define. You backup the data volume, and forget about the container. Optimally your containers are completely stateless and immutable.</p>
<p>Certainly patches will still be part of your world, but they aren’t applied to running containers. In reality if you patched a running container, and then spun up new ones based on an unpatched image, you’re gonna have a bad time. Ideally you would update your Docker image, stop your running containers, and fire up new ones. Because a container can be spun up in a fraction off a second, it’s just much cheaper to go this route.</p>
<p>Your application server translates into a service run inside of a container. Certainly there may be cases where your microservices-based application will need to connect to a non-containerized service, but for the most part standalone servers where you execute your code give way to one or more containers that provide the same functionality with much less overhead (and offer up much better horizontal scaling).</p>
<p>“But, VMs have traditionally been about lift and shift. What do I do with my existing apps?”</p>
<p>I often have people ask me how to run huge monolithic apps in a container. There are many valid strategies for migrating to a microservices architecture that start with moving an existing monolithic application from a VM into a container but that should be thought of as the first step on a journey, not an end goal.</p>
<p>As you consider how your organization can leverage Docker, try and move away from a VM-focused mindset and realize that Docker is way more than just “a lightweight VM.” It’s an application-centric way to  deliver high-performing, scalable applications on the infrastructure of your choosing.</p>
<p>Check out these resources to start learning more about Docker and containers:</p>
<ul>
<li><a href="https://docker.wistia.com/medias/fqwm0x9tgz" target="_blank" rel="noopener">Watch an Intro to Docker webinar</a></li>
<li><a href="https://hub.docker.com/enterprise/trial/" target="_blank" rel="noopener">Sign up for a free 30 day trial</a></li>
<li><a href="http://www.docker.com/sites/default/files/caaSwhitepaper_V6_0.pdf" target="_blank" rel="noopener">Read the Containers as a Service white paper</a></li>
</ul>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p><a href="https://blog.docker.com/2016/03/containers-are-not-vms/" target="_blank" rel="noopener">CONTAINERS ARE NOT VMS</a></p>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>【WEP】Commodity Hardware Today</title>
    <url>/blog/commodity-hardware-today/</url>
    <content><![CDATA[<h1 id="Commodity-Hardware-today"><a href="#Commodity-Hardware-today" class="headerlink" title="Commodity Hardware today"></a>Commodity Hardware today</h1><p>Over the years personal computers and smaller servers standardized on a chipset with two parts: the <code>Northbridge</code> and <code>Southbridge</code>.</p>
<p><img src="Selection_001.png" alt="Figure 2.1"></p>
<p>A couple of bottlenecks are immediately apparent in this design</p>
<p>RAM bandwidth 争抢 (DMA 允许 device 直接从 RAM 里读写数据，但是都要通过 Northbridge，这样就 CPU 通过 Northbridge 访问 RAM 的 bandwidth冲突)</p>
<blockquote>
<p>In the earliest days of the PC, all communication with devices on either bridge had to pass through the CPU, negatively impacting overall system performance. To work around this problem some devices became capable of direct memory access (DMA). DMA allows devices, with the help of the Northbridge, to store and receive data in RAM directly without the intervention of the CPU (and its inherent performance cost). Today all high-performance devices attached to any of the buses can utilize DMA. While this greatly reduces the workload on the CPU, it also creates contention for the bandwidth of the Northbridge as DMA requests compete with RAM access from the CPUs.</p>
</blockquote>
<a id="more"></a>
<p>因为有限的 Northbandwidth，所以需要 schedule memory access in ways that minimize delays.</p>
<p>On some more expensive systems, the Northbridge does not actually contain the memory controller. Instead the Northbridge can be connected to a number of external memory controllers （外接 memory controller）</p>
<p><img src="Selection_002.png" alt="Figure 2.2"></p>
<p>Pros:</p>
<ol>
<li>more than one memory bus, so the bandwitdth increases</li>
<li>support memory</li>
</ol>
<p>Concurrent memory access patterns reduce delays by simultaneously accessing different memory banks. This is especially true when multiple processors are directly connected to the Northbridge. For such a design, the primary limitation is the internal bandwidth of the Northbridge.</p>
<p>除了上述提升 memory bandwidth 的办法，还有一种，就是将 memory controller 嵌入到 CPU，并将 memory attached 到 CPU 上</p>
<p><img src="Selection_003.png" alt="Figure 2.3"></p>
<p>AMD的 Opteron processor 就是这样，另外 Intel 支持的 Common System Interface (CSI) Nehalem processor 也是差不多的原理。(NUMA - Non-Uniform Memory Architecture)</p>
<p>Cons:</p>
<ol>
<li>RAM 不再 uniform, 从 CPU1访问 CPU2 的RAM，会要求一次 CPU 的 interconnect，如果是 CPU1 访问 CPU4，则会要求 2次 interconnects</li>
</ol>
<h1 id="RAM-Type"><a href="#RAM-Type" class="headerlink" title="RAM Type"></a>RAM Type</h1><h2 id="Static-RAM"><a href="#Static-RAM" class="headerlink" title="Static RAM"></a>Static RAM</h2><p><img src="Selection_004.png" alt="Figure 2.4"></p>
<p>The core of this cell is formed by the four transistors $M1$ to $M4$ which form two cross-coupled inverters. They have two stable states, representing 0 and 1 respectively. The state is stable as long as power on $Vdd$ is available.</p>
<p>the cell state is stable, <code>no refresh cycles are needed</code></p>
<h2 id="Dynamic-RAM"><a href="#Dynamic-RAM" class="headerlink" title="Dynamic RAM"></a>Dynamic RAM</h2><p><img src="Selection_005.png" alt="Figure 2.5"></p>
<p>To read the state of the cell the access line $AL$ is raised; this either causes a current to flow on the data line $DL$ or not, depending on the charge in the capacitor.<br>To write to the cell the data line$DL$ is appropriately set and then $AL$ is raised for a time long enough to charge or drain the capacitor.</p>
<p>Cons:</p>
<ul>
<li><code>Leakage problem</code><br>The use of a capacitor means that reading the cell discharges the capacitor. The procedure cannot be repeated indefinitely, <code>the capacitor must be recharged at some point</code>. Capacity of the capacitor is low, it only takes a short time for the capacity to dissipate.<br>This leakage is why a DRAM cell must be constantly refreshed. For most DRAM chips these days this refresh must happen every 64ms. During the refresh cycle no access to the memory is possible since a refresh is simply a memory read operation where the result is discarded. For some workloads this overhead might stall up to 50% of the memory accesses</li>
<li>information read is not directly usable. A sense amplifier is used to distinguish between 0 and 1</li>
<li>A third problem is that reading a cell causes the charge of the capacitor to be depleted. This means every read operation must be followed by an operation to recharge the capacitor. It does mean, though, the reading memory content <code>requires</code> additional energy and, <code>more importantly, time</code>.</li>
<li>A fourth problem is that charging and draining a capacitor is not instantaneous.</li>
</ul>
<p><img src="equation_001.png" alt=""></p>
<p><img src="Selection_006.png" alt="Figure 2.6"></p>
<p>Unlike the <code>static RAM</code> case where the output is immediately available when the word access line is raised, it will always take a bit of time until the capacitor discharges sufficiently. This delay severely limits how fast DRAM can be.</p>
<p>The speed can vary from only slightly slower than the CPU core to one or two orders of magnitude slower. SRAM 的性能差异很大，可能只比 CPU core 慢一点，或者慢 1-2 个数量级</p>
<h1 id="DRAM-Access-Technical-Details"><a href="#DRAM-Access-Technical-Details" class="headerlink" title="DRAM Access Technical Details"></a>DRAM Access Technical Details</h1><p><code>Synchronous DRAM</code> (SDRAM) and its successors Double Data Rate DRAM (<code>DDR</code>)</p>
<blockquote>
<p>Synchronous DRAM, as the name suggests, works relative to a time source. The memory controller provides a clock, the frequency of which determines the speed of the Front Side Bus (FSB) – the memory controller interface used by the DRAM chips. As of this writing, frequencies of 800MHz, 1,066MHz, or even 1,333MHz are available with higher frequencies (1,600MHz) being announced for the next generation. This does not mean the frequency used on the bus is actually this high. Instead, today’s buses are double- or quad-pumped, meaning that data is transported two or four times per cycle. Higher numbers sell so the manufacturers like to advertise a quad-pumped 200MHz bus as an “effective” 800MHz bus</p>
</blockquote>
<p>For SDRAM today each data transfer consists of 64 bits – 8 bytes. The transfer rate of the FSB is therefore 8 bytes multiplied by the effective bus frequency (6.4GB/s for the quad-pumped 200MHz bus).</p>
<h2 id="Read-Access-Protocol"><a href="#Read-Access-Protocol" class="headerlink" title="Read Access Protocol"></a>Read Access Protocol</h2><p><img src="Selection_008.png" alt="Figure 2.8"><br>bus clock, RAS and CAS signals, and the address and data buses.</p>
<blockquote>
<p>A read cycle <em>begins with</em> the memory controller making the row address available on the address bus and lowering the RAS signal. All signals are read on the rising edge of the clock (CLK) so it does not matter if the signal is not completely square as long as it is stable at the time it is read. <em>Setting the row address</em> causes the RAM chip to start latching the addressed row.<br>The CAS signal can be sent after tRCD (RAS-to-CAS Delay) clock cycles. The column address is then transmitted by making it available on the address bus and lowering the CAS line.</p>
</blockquote>
<p>With all this preparation to get to the data it would be wasteful to only transfer one data word. This is why DRAM modules allow the memory controller to specify how much data is to be transmitted. Often the choice is between 2, 4, or 8 words. This allows <code>filling entire lines in the caches</code>(<code>cache line</code>) without a new RAS/CAS sequence</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://lwn.net/Articles/250967/#TOC" target="_blank" rel="noopener">Commodity Hardware Today</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】15 Address Translation</title>
    <url>/blog/Address-Translation/</url>
    <content><![CDATA[<p>a general mechanism: <code>limited direct execution (LDE)</code>:<br>the idea is simple, for the most part, let program run directly on hardware.</p>
<p>let’s see one example</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="number">128</span>: movl <span class="number">0x0</span>(%ebx), %eax     ;load <span class="number">0</span>+ebx into eax</span><br><span class="line"><span class="number">132</span>: addl $<span class="number">0x03</span>, %eax             ;add <span class="number">3</span> to eax <span class="keyword">register</span></span><br><span class="line"><span class="number">135</span>: movl %eax, <span class="number">0x0</span>(%ebx)     ;store eax back to mem</span><br></pre></td></tr></table></figure>
<p>When these instructions run, from the perspective of the process, the following memory accesses take place.</p>
<ul>
<li>Fetch instruction at address 128</li>
<li>Execute this instruction (load from address 15 KB)</li>
<li>Fetch instruction at address 132</li>
<li>Execute this instruction (no memory reference)</li>
<li>Fetch the instruction at address 135</li>
<li>Execute this instruction (store to address 15 KB)<a id="more"></a>

</li>
</ul>
<p><img src="Selection_001.png" alt=""></p>
<p>how can we relocate this process in memory in a way that is <code>transparent</code> to the process? How can we provide the illusion of a virtual address space starting at 0, when in reality the address space is located at some other physical address?</p>
<p>An example of what physical memory might look like once this process’s address space has been placed in memory is found in Figure</p>
<p><img src="Selection_002.png" alt=""></p>
<p>the OS using the first slot of physical memory for itself, and that it has relocated the process from the example above into the slot starting at physical memory address 32 KB. The other two slots are free (16 KB-32 KB and 48 KB-64 KB).</p>
<h1 id="Dynamic-Hardware-based-Relocation"><a href="#Dynamic-Hardware-based-Relocation" class="headerlink" title="Dynamic (Hardware-based) Relocation"></a>Dynamic (Hardware-based) Relocation</h1><p>we’ll need two hardware registers within each CPU: one is called the <code>base register</code>, and the other the <code>bounds</code> (sometimes called a limitregister). This base-and-bounds pair is going to allow us to place the address space anywhere we’d like in physical memory, and do so while ensuring that the process can only access its own address space.</p>
<p>Each memory reference generated by the process is a<code>virtual address</code>; the hardware in turn adds the contents of the base register to this address and the result is a physical address that can be issued to the memory system</p>
<p>Transforming a virtual address into a physical address is exactly the technique we refer to as <code>address translation</code>; that is, the hardware takes a virtual address the process thinks it is referencing and transforms it into a physical address which is where the data actually resides. Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as<code>dynamic relocation.</code></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/vm-mechanism.pdf" target="_blank" rel="noopener">Address Translation</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】13 The Abstraction Address Spaces</title>
    <url>/blog/Address-space/</url>
    <content><![CDATA[<h1 id="Early-Systems"><a href="#Early-Systems" class="headerlink" title="Early Systems"></a>Early Systems</h1><p>Early machines didn’t provide much of an abstraction to users.</p>
<p><img src="Selection_001.png" alt=""></p>
<p>The OS was a set of routines (a library, really) that sat in memory (starting at physical address 0 in this example), and there would be one running program (a process) that currently sat in physical memory (starting at physical address 64k in this example) and used the rest of memory.</p>
<h1 id="Multiprogramming-and-Time-Sharing"><a href="#Multiprogramming-and-Time-Sharing" class="headerlink" title="Multiprogramming and Time Sharing"></a>Multiprogramming and Time Sharing</h1><p>After a time, because machines were expensive, people began to share machines more effectively. Thus the era of <code>multiprogramming</code> was born [DV66], in which multiple processes were ready to run at a given time, and the OS would switch between them, for example when one decided to perform an I/O. Doing so increased the effective utilization of the CPU. Such increases in efficiency were particularly important in those days where each machine cost hundreds of thousands or even millions of dollars.</p>
<p>Soon enough, however, people began demanding more of machines, and the era of <code>time sharing</code> was born.</p>
<a id="more"></a>
<p><img src="Selection_002.png" alt=""></p>
<p>In the diagram, there are three processes (A, B, and C) and each of them have a small part of the 512KB physical memory carved out for them. Assuming a single CPU, the OS chooses to run one of the processes (say A), while the others (B and C) sit in the ready queue waiting to run.</p>
<h1 id="The-Address-Space"><a href="#The-Address-Space" class="headerlink" title="The Address Space"></a>The Address Space</h1><p>In particular, allowing multiple programs to reside concurrently in memory makes<code>protection</code>an important issue.</p>
<p>OS create a easy to use abstraction of physical memory: <code>address space</code></p>
<p><code>code</code> live in memory<br><code>stack</code> is used to keep track and pass parameters, return value to and from rountines<br><code>heap</code> is used for dynamically-allocated</p>
<p>Process A in tries to perform a load at address 0 (which we will call a <code>virtual address</code>), somehow the OS, in tandem with some hardware support, will have to make sure the load doesn’t actually go to physical address 0 but rather to physical address 320KB。</p>
<h1 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h1><p><code>Transparency:</code> make program not aware of the fact that the memory is virtualized<br>‘Efficiency’: time and space, rely on hardware support, including features such as <code>TLBs</code><br><code>Protection</code>: protect process from one another. So we could deliver property of isolation among processes.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/vm-intro.pdf" target="_blank" rel="noopener">Address Spaces</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】43 Log-structured File System</title>
    <url>/blog/LFS/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background:"></a>Background:</h1><ul>
<li>System merories are growing</li>
<li>There is a large gap between random I/O performance and sequential I/O performance</li>
<li>Existing file systems perform poorly on many common workloads</li>
<li>File systems are not RAID-aware</li>
</ul>
<p>LFS</p>
<ol>
<li>first buffer all updates (inlcuding metadata)  in an in memory <code>segment</code>,</li>
<li>when segment is full, it is written to disk in one long, sequential transfer to an unused part of disk</li>
</ol>
<p>LFS never overwrite existing data, but rather always writes segments to free locations. Because segments are large, the disk is used efficiently, and performance of the file system approaches its zenith.</p>
<a id="more"></a>
<h1 id="Writing-To-Disk-Sequentially"><a href="#Writing-To-Disk-Sequentially" class="headerlink" title="Writing To Disk Sequentially"></a>Writing To Disk Sequentially</h1><p>Imagine we are writing a data block D to a file. Writing the data block to disk might result in the following on-disk layout, with D written at disk address A0:</p>
<p><img src="Selection_001.png" alt=""><br>it is not only data that gets written to disk; there is also other metadata that needs to be updated. In this case, let’s also write the inode (I) of the file to disk, and have it point to the data block D. When written to disk, the data block and inode would look something like this (note that the inode looks as big as the data block, which generally isn’t the case; in most systems, data blocks are 4 KB in size, whereas an inode is much smaller, around 128 bytes):</p>
<p>This basic idea, of simply <code>writing all updates</code> (such as data blocks, inodes, etc.) to the disk sequentially, sits at the heart of LFS.</p>
<p>保证顺序写不足够保证 throughput，因为对于 HDD，如果每次写的数据块之间间隔过长，磁盘已经转过头了，就必须等待下一次经过时候。所以为了保证HDD的 throughput，必须 , you must issue <code>a large number of contiguous writes</code> (or <code>one large write</code>) to the drive in order to achieve good write performance.</p>
<p><code>write buffering</code> 应运而生。</p>
<p>LFS buffer all updates and when reach to limit, write the whole <code>segment</code> to disk at once.</p>
<p>Here is an example, in which LFS buffers two sets of updates into a small segment; actual segments are larger (a few MB). The first update is of four block writes to file <code>j</code>; the second is one block being added to file <code>k</code>. LFS then commits the entire segment of seven blocks to disk at once. The resulting on-disk layout of these blocks is as follows:</p>
<p><img src="Selection_002.png" alt=""></p>
<h1 id="How-Much-To-Buffer"><a href="#How-Much-To-Buffer" class="headerlink" title="How Much To Buffer?"></a>How Much To Buffer?</h1><ul>
<li>assume that positioning before each write takes roughly <code>Tposition</code> seconds</li>
<li>disk transfer rate is <code>Rpeak</code> MB/s</li>
</ul>
<p>let’s assume we are writing out <code>D</code> MB. The time to write out this chunk of data (<code>Twrite</code>) is the positioning time <code>Tposition</code> plus the time to transfer D.</p>
<p><img src="Selection_003.png" alt=""><br>effective rate of writing :</p>
<p><img src="Selection_004.png" alt=""><br>we want the effective rate to be some fraction F of the peak rate, where 0 &lt; F &lt; 1.<br>In mathematical form, this means we want Reffective = F × Rpeak.</p>
<p><img src="Selection_005.png" alt=""><br>with a disk with a positioning time of 10 milliseconds and peak transfer rate of 100 MB/s; assume we want an effective bandwidth of 90% of peak (F = 0.9). In this case, D = 0.9 0.1 × 100 MB/s × 0.01 seconds = 9 MB</p>
<h1 id="Problem-Finding-Inodes"><a href="#Problem-Finding-Inodes" class="headerlink" title="Problem: Finding Inodes"></a>Problem: Finding Inodes</h1><p>We’ve managed to scatter the inodes all throughout the disk! Worse, we never overwrite in place, and thus the latest version of an inode (i.e., the one we want) keeps moving.</p>
<h1 id="Solution-Through-Indirection-The-Inode-Map"><a href="#Solution-Through-Indirection-The-Inode-Map" class="headerlink" title="Solution Through Indirection: The Inode Map"></a>Solution Through Indirection: The Inode Map</h1><p> LFS introduced a level of indirection between inode numbers and the inodes through a data structure called the <code>inode map</code> (imap).<br>The imap is a structure that takes an inode number as input and produces the disk address of the most recent version of the inode. Thus, you can imagine it would often be implemented as a simple array, with 4 bytes (a disk pointer) per entry. Any time an inode is written to disk, the imap is updated with its new location.<br>总结就是有个 inode map 结构存储了最新的 inode 信息，并随之更新。</p>
<p>问题是，inode map 存在哪儿，如果存在一个 fixed 的位置，那么随之而来的问题就是，每次更新文件，就需要重新 seek 到 inode map 位置并更新，这样就和 FFS 没有区别了（更新文件以后更新inode 和 metadata）。</p>
<p>LFS places chunks of the inode map right next to where it is writing all of the other new information. Thus, when appending a data block to a file k, LFS actually writes the new data block, its inode, and a piece of the inode map all together onto the disk, as follows:</p>
<p><img src="Selection_006.png" alt=""><br>In this picture, the piece of the imap array stored in the block marked imap tells LFS that the inode k is at disk address A1; this inode, in turn, tells LFS that its data block D is at address A0.</p>
<h1 id="Completing-The-Solution-The-Checkpoint-Region"><a href="#Completing-The-Solution-The-Checkpoint-Region" class="headerlink" title="Completing The Solution: The Checkpoint Region"></a>Completing The Solution: The Checkpoint Region</h1><p> LFS has a fixed place on disk, known as <code>checkpoint region</code> <code>(CR)</code>, checkpioint region contains pointers to the latest pieces of the inode map, so  the inode map pieces can be found by reading the CR first. And the CR is only updated periodically, thus performance is not ill-affected.</p>
<h1 id="Reading-A-File-From-Disk-A-Recap"><a href="#Reading-A-File-From-Disk-A-Recap" class="headerlink" title="Reading A File From Disk: A Recap"></a>Reading A File From Disk: A Recap</h1><p>Assuming there is nothing in memory.</p>
<ol>
<li>read checkpoint region to get the entire inode map and cache it in memory</li>
<li>when given an inode number of a file, LFS looks up the inode-number to inode-disk-address mapping in the imap, read the latest version of the inode.</li>
<li>after get the inode, read a block like the typical UNIX like system<br>通常情况，inode map 都是被cache 在内存中的，所以 LFS 读文件的唯一额外操作，就是从 inode map 中读取到 最新 inode 的 address。</li>
</ol>
<h1 id="What-About-Directories"><a href="#What-About-Directories" class="headerlink" title="What About Directories?"></a>What About Directories?</h1><p>creating a file foo in a directory would lead to the following new structures on disk:</p>
<p><img src="Selection_007.png" alt=""></p>
<p>There is one other serious problem in LFS that the inode map solves, known as the <code>recursive update problem</code>.(就是一次文件更新，引起其目录 inode 更新，引发更上层的目录更新，直到 / 目录)<br>Specifically, whenever an inode is updated, its location on disk changes. If we hadn’t been careful, this would have also entailed an update to the directory that points to this file, which then would have mandated a change to the parent of that directory, and so on, all the way up the file system tree.</p>
<p>LFS cleverly avoids this problem with the inode map. Even though the location of an inode may change, the change is never reflected in the directory itself; rather, the imap structure is updated while the directory holds the same name-to-inumber mapping. Thus, through indirection, LFS avoids the recursive update problem.</p>
<h1 id="A-New-Problem-Garbage-Collection"><a href="#A-New-Problem-Garbage-Collection" class="headerlink" title="A New Problem: Garbage Collection"></a>A New Problem: Garbage Collection</h1><p>LFS leaves old versions of file structures scattered throughout the disk. We (rather unceremoniously) call these old versions<code>garbage.</code><br>, LFS instead keeps only the latest live version of a file; thus (in the background), LFS must periodically find these old dead versions of file data, inodes, and other structures, and <code>clean</code> them; cleaning should thus make blocks on disk free again for use in a subsequent writes. Note that the process of cleaning is a form of <code>garbage collection</code>, a technique that arises in programming languages that automatically free unused memory for programs.</p>
<p>如果 LFS 只是简单的将 old version block 删除，那么就会形成很多零星的 free space，导致以后可能没有整块的 block 可以写入。</p>
<p>Instead, the LFS cleaner works on a segment-by-segment basis, thus clearing up large chunks of space for subsequent writing. The basic cleaning process works as follows:</p>
<blockquote>
<p>Periodically, the LFS cleaner reads in a number of old (partially-used) segments, determines which blocks are live within these segments, and then write out a new set of segments with just the live blocks within them, freeing up the old ones for writing</p>
</blockquote>
<p>Here arouse two problems:</p>
<ol>
<li>how can LFS tell which blocks within a segment are live, and which are dead?</li>
<li>how often should the cleaner run, and which segments should it pick to clean?</li>
</ol>
<h1 id="Determining-Block-Liveness"><a href="#Determining-Block-Liveness" class="headerlink" title="Determining Block Liveness"></a>Determining Block Liveness</h1><p>There is a <code>segment summary block</code>  at the head of the segment,  includes, for each data block D, its inode number (which file it belongs to) and its offset (which block of the file this is).</p>
<ol>
<li>For a block D located on disk at address A, look in the segment summary block and find its <code>inode number N</code> and <code>offset T</code>.</li>
<li>look in the imap to find where N lives and read N from disk (perhaps it is already in memory, which is even better)</li>
<li>using the <code>offset T</code>, look in the inode (or some indirect block) to see where the inode thinks the Tth block of this file is on disk.</li>
</ol>
<p>If it points exactly to disk address A, LFS can conclude that the block D is live. If it points anywhere else, LFS can conclude that D is not in use (i.e., it is dead) and thus know that this version is no longer needed.</p>
<p><img src="Selection_008.png" alt=""></p>
<p>也可以在imap里面存储最新数据的 version number，然后和 segment summary block 里面的 version number 对比，如果一致则 live，不一致则是 garbage，减少一次读取。</p>
<h1 id="Which-Blocks-To-Clean-And-When"><a href="#Which-Blocks-To-Clean-And-When" class="headerlink" title="Which Blocks To Clean, And When?"></a>Which Blocks To Clean, And When?</h1><p>an approach which tries to segregate hot and cold segments.：</p>
<p>A hot segment is one in which the contents are being frequently over-written; thus, for such a segment, the best policy is to wait a long time before cleaning it, as more and more blocks are getting over-written (in new segments) and thus being freed for use. A cold segment, in contrast, may have a few dead blocks but the rest of its contents are relatively stable. Thus, the authors conclude that one should clean cold segments sooner and hot segments later</p>
<h1 id="Crash-Recovery-And-The-Log"><a href="#Crash-Recovery-And-The-Log" class="headerlink" title="Crash Recovery And The Log"></a>Crash Recovery And The Log</h1><p>crash in writing to a segment, writing to the CR:</p>
<ol>
<li>因为会周期性的更新 CR，如果某次一旦crash，下次恢复的就是恢复老的 version，但是最新的数据会丢失。为了尽可能多恢复数据，使用 <code>roll forward</code> 技术，首先根据 CR 找到上个有效的 version 处，然后往后扫描一个 segment，看是否有 valid 数据在里面，然后恢复。</li>
<li>有两个 CR，在更新CR 的 时候，先在CR 的header数据中写入一个 timestamp，然后接着写 body，然后写 last block with timestamp，如果两个 timestamp 不匹配，则丢弃；每次都选取有效的，并且时间戳最近的那个 CR 恢复</li>
</ol>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>WAFL, ZFS, btrfs,都是 LFS</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-lfs.pdf" target="_blank" rel="noopener">Log-structured File System (LFS)</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】42 FSCK-and-journaling</title>
    <url>/blog/FSCK-and-journaling/</url>
    <content><![CDATA[<p>old approach: <code>fsck</code>(file system checker)<br>new approach: <code>journaling</code>(also known as <code>write-ahead-logging</code>)</p>
<h1 id="A-Detailed-Example"><a href="#A-Detailed-Example" class="headerlink" title="A Detailed Example"></a>A Detailed Example</h1><p><img src="Selection_001.png" alt=""><br>If you look at the structures in the picture, you can see that a single inode is allocated (<code>inode number 2</code>), which is marked in the <code>inode bitmap</code>, and a single allocated data block (<code>data block 4</code>), also marked in the <code>data bitmap</code>. The inode is denoted <code>I[v1]</code>,</p>
<p>let’s see the simple inode inside:</p>
<a id="more"></a>
<p><img src="Selection_002.png" alt=""><br>In this simplified inode,</p>
<ul>
<li>the size of the file is 1 (it has one block allocated),</li>
<li>the first direct pointer points to block 4 (the first data block of the file, Da)</li>
<li>all three other direct pointers are set to null (indicating that they are not used)</li>
</ul>
<p>When we append to the file, we are adding a new data block to it, and update on-disk structures.</p>
<p><img src="Selection_003.png" alt=""></p>
<p><img src="Selection_004.png" alt=""><br>we must update three blocks to do this update.</p>
<ul>
<li>Data bitmap</li>
<li>Inodes</li>
<li>Data Blocks</li>
</ul>
<p>when user issues a write() system call, usually it will cache them in the main memory first for some time, then to the disk. If a crash happens and will interfere with these updates to the disk.</p>
<h1 id="Crash-Scenarios"><a href="#Crash-Scenarios" class="headerlink" title="Crash Scenarios"></a>Crash Scenarios</h1><p>Three outcomes when crash:</p>
<ul>
<li>Just the data block (Db) is written to disk.(no a bit deal)</li>
<li>Just the updated inode (I[v2]) is written to disk.(it could cause new problem: file-system inconsistency. The on-disk bitmap is telling us that data block 5 has not been allocated, but the inode is saying that it has. This disagreement in the file system data structures is an inconsistency in the data structures of the file system;)</li>
<li>Just the updated Data bitmap (B[v2]) is written to disk.(this cause a space leak, as block 5 would never be used by the file system)</li>
<li>…</li>
</ul>
<h1 id="Solution-1-The-File-System-Checker"><a href="#Solution-1-The-File-System-Checker" class="headerlink" title="Solution #1: The File System Checker"></a>Solution #1: The File System Checker</h1><p>fsck is too slow:<br>scanning the entire disk to find all the allocated blocks and read the entire directory tree may take many minutes or hours. Performance of fsck, as disks grew in capacity and RAIDs grew in popularity, became prohibitive.</p>
<p>This situation is akin to dropping your keys on the floor in your bedroom, and then commencing a search-the-entire-house-for-keys recovery algorithm, starting in the basement and working your way through every room. It works but is wasteful.</p>
<h1 id="Solution-2-Journaling-or-Write-Ahead-Logging"><a href="#Solution-2-Journaling-or-Write-Ahead-Logging" class="headerlink" title="Solution #2: Journaling (or Write-Ahead Logging)"></a>Solution #2: Journaling (or Write-Ahead Logging)</h1><p>Let’s see EXT3.</p>
<p>Most of the on-disk structures are identical to Linux ext2(disk is divided into block groups). The new key structure is the <code>journal</code> itself.<br>ext2 file system looks like:</p>
<p><img src="Selection_005.png" alt=""><br>Assuming the journal is palced within the same file system image (though sometimes it is placed on a separate device, or as a file within the file system), an ext3 file system with a journal looks like this:</p>
<p><img src="Selection_006.png" alt=""></p>
<h1 id="Data-Journaling"><a href="#Data-Journaling" class="headerlink" title="Data Journaling"></a>Data Journaling</h1><p>Let’s look at a simple example to understand how data journaling works.<br>Say we have our canonical update again, where we wish to write the inode (I[v2]), bitmap (B[v2]), and data block (Db) to disk again. Before writing them to their final disk locations, we are now first going to write them to the log (a.k.a. journal). This is what this will look like in the log:</p>
<p><img src="Selection_007.png" alt=""><br>You can see we have written five blocks here. The transaction begin (<code>TxB</code>) tells us about this update, including information about the pending update to the file system), as well as some kind of <code>transaction identifier (TID)</code>. The middle three blocks just contain the exact contents of the blocks themselves; this is known as <code>physical logging</code> as we are putting the exact physical contents of the update in the journal (an alternate idea, <code>logical logging</code>, puts a more compact logical representation of the update in the journal, e.g., “this update wishes to append data block Db to file X”, which is a little more complex but can save space in the log and perhaps improve performance). The final block (<code>TxE</code>) is a marker of the end of this transaction, and will also contain the TID.</p>
<p>Once this transaction is safely on disk, we are ready to overwrite the old structures in the file system; this process is called <code>checkpointing</code>. Thus, to <code>checkpoint</code> the file system (i.e., bring it up to date with the pending update in the journal), we issue the writes I[v2], B[v2], and Db to their disk locations as seen above; if these writes complete successfully, we have successfully checkpointed the file system and are basically done.</p>
<p>So the steps of operation:</p>
<ul>
<li>Journal write:</li>
<li>Checkpoint: 就是将 journal 里面的disk更新信息，写到原本的位置中</li>
</ul>
<p>Best way is to issue all the five block at once. However, when the changing data is big, OS may reorder the write sequence, it may write TxB, I[v2], B[v2], and TxE and only later write Db. Unfortunately, if the disk loses power between (1) and (2), this is what ends up on disk:</p>
<p><img src="Selection_008.png" alt=""></p>
<p>FS can not know if the fourth block is wrong. so in recovery, it can cause problem, even worse if the data in there belongs to superblock, which could render the FS unmountable.</p>
<blockquote>
<p>ASIDE: FORCING WRITES TO DISK To enforce ordering between two disk writes, modern file systems have to take a few extra precautions. In olden times, forcing ordering between two writes, A and B, was easy: just issue the write of A to the disk, wait for the disk to interrupt the OS when the write is complete, and then issue the write of B. Things got slightly more complex due to the increased use of write caches within disks. With write buffering enabled (sometimes called <code>immediate reporting</code>), a disk will inform the OS the write is complete when it simply has been placed in the disk’s memory cache, and has not yet reached disk. If the OS then issues a subsequent write, it is not guaranteed to reach the disk after previous writes; thus ordering between writes is not preserved. One solution is to disable <code>write buffering</code>. However, more modern systems take extra precautions and issue explicit write barriers; such a barrier, when it completes, guarantees that all writes issued before the barrier will reach disk before any writes issued after the barrier. All of this machinery requires a great deal of trust in the correct operation of the disk. Unfortunately, recent research shows that <strong>some disk manufacturers, in an effort to deliver “higher performing” disks, explicitly ignore write-barrier requests</strong>, thus making the disks seemingly run faster but at the risk of incorrect operation [C+13, R+11]. As Kahan said, the fast almost always beats out the slow, even if the fast is wrong.</p>
</blockquote>
<p>To avoid this problem, the file system issues the transactional write in two steps.</p>
<ul>
<li>it writes all blocks except the TxE block to the journal, issuing these writes all at once</li>
</ul>
<p><img src="Selection_009.png" alt=""></p>
<ul>
<li>When those writes complete(There will be a flush() CMD before TxE), the file system issues the write of the TxE block, thus leaving the journal in this final, safe state:</li>
</ul>
<p><img src="Selection_010.png" alt=""></p>
<p>So the final process is:</p>
<ul>
<li><code>Journal write</code>: write the first 4 chunks</li>
<li><code>Journal commit</code>: wait 1st complete and write TxE to the log <code>( This could cause inefficience when transfer data)</code></li>
<li><code>Checkpoint</code>: Write contents of the update to their final on-disk locations</li>
</ul>
<blockquote>
<p>to solve the efficiency problem, we could inlcude a <code>checksum</code> in TxB and TxE, then we could write the entire transaction at once, without any wait. During recovery, if the file system sees a mismatch in the computed checksum versus the stored checksum, then it can discard the update in journal.</p>
</blockquote>
<h1 id="Making-The-Log-Finite"><a href="#Making-The-Log-Finite" class="headerlink" title="Making The Log Finite"></a>Making The Log Finite</h1><ul>
<li>FS buffers updates in memory for some time(few seconds);</li>
<li>when write to the disk, first write to journal</li>
<li>after transaction complete, FS checkpoint those blocks to final location on disk</li>
</ul>
<p>Log has finite size and in order to continue log new write, FS treat the log as a circular data structure, namely <code>circular log.</code></p>
<p>Once a checkpoint success, FS should free the space. FS could simply mark the oldest and newest no-checkpointed transaction in a <code>journal superblock</code>.</p>
<p><img src="Selection_011.png" alt=""><br>In the journal superblock (not to be confused with the main file system superblock), the journaling system records enough information to know which transactions have not yet been checkpointed, and thus reduces recovery time as well as enables re-use of the log in a circular fashion. And thus we add another step to our basic protocol:</p>
<ol>
<li>Journal write:</li>
<li>Jornal commit:</li>
<li>Checkpoint:</li>
<li><code>Free</code>: mark the transaction free in the journal by updating the journal superblock.(意思就是标记这个更新已经结束，可以被复写）</li>
</ol>
<p>However, all the process discussed above indicated that we will <code>write data twice</code>（because we store all the new data in journal log)</p>
<h1 id="Metadata-Journaling"><a href="#Metadata-Journaling" class="headerlink" title="Metadata Journaling"></a>Metadata Journaling</h1><p>all the discussion above is called <code>data journaling</code>(as in linux ext3). A simple form of journaling is called <code>orderd journaling</code>( <code>metadata journaling</code> ).</p>
<p><img src="Selection_012.png" alt=""><br>The data block Db, previously written to the log, would instead be written to the file system proper, avoiding the extra write; given that most I/O traffic to the disk is data, not writing data twice substantially reduces the I/O load of journaling.</p>
<p>Then when should we write data blocks to disk?</p>
<p>ext3 do like this:</p>
<ol>
<li>Data write: write data to final location; wait for completion(optional)</li>
<li>Journal metadata write</li>
<li>Journal commit:Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction (including data) is now committed. 在这一步完成同时，必须保证data也被传输结束。（那如果使用checksum的话，怎么保证journalok时候，data也肯定ok呢？如果metadata journal 没有问题，data还在transaction，这时候crash了，那recovery岂不是错的？）</li>
<li>Checkpoint commit:</li>
<li>Free<br>（this is ordered journaling, ext3 also provide <code>unordered modes</code>, which data can be written at any time)</li>
</ol>
<h1 id="Solution-3-Other-Approaches"><a href="#Solution-3-Other-Approaches" class="headerlink" title="Solution #3: Other Approaches"></a>Solution #3: Other Approaches</h1><ul>
<li><p>Soft Updates</p>
<blockquote>
<p>This approach carefully orders all writes to the file system to ensure that the on-disk structures are never left in an inconsistent state. For example, by writing a pointed-to data block to disk before the inode that points to it, we can ensure that the inode never points to garbage; similar rules can be derived for all the structures of the file system</p>
</blockquote>
</li>
<li><p>copy-on-write(COW) like FS ZFS</p>
<blockquote>
<p>This technique never overwrites files or directories in place; rather, it places new updates to previously unused locations on disk. After a number of updates are completed, COW file systems flip the root structure of the file system to include pointers to the newly updated structures. Doing so makes keeping the file system consistent straightforward</p>
</blockquote>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-journaling.pdf" target="_blank" rel="noopener">FSCK and Journaling</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>【TP】41 Fast File System (FFS)</title>
    <url>/blog/fast-file-system/</url>
    <content><![CDATA[<p>Old Unix file system: Simple and block size is small( 512B )</p>
<p><img src="Selection_001.png" alt="1"><br>Problems: Performance terrible. Because old system treats disk like a random-access memory. delivering 2% overall disk bandwith:<br>    * data spread all over the place, expensive positioning cose (data block could be very far away from its inode)<br>    * file system getting fragmented<br><img src="Selection_002.png" alt="2"></p>
<p>E gets spread across the disk, and as a result, when accessing E, you don’t get peak (sequential) performance from the disk. Rather, you first read E1 and E2, then seek, then read E3 and E4. This fragmentation problem happened all the time in the old UNIX file system, and it hurt performance.</p>
<p>small block size cause transferring data from disk inefficient.</p>
<a id="more"></a>
<h1 id="Fast-File-system-FFS-from-Berkeley-group"><a href="#Fast-File-system-FFS-from-Berkeley-group" class="headerlink" title="Fast File system(FFS) from Berkeley group"></a>Fast File system(FFS) from Berkeley group</h1><p>let file system be “disk aware”. Keep the same api (open(), read(), write(), close()) but changing internal implementation.</p>
<p>Organize block into block group. cylinder groups 就是连续的柱面组合。<br><img src="Selection_003.png" alt="3"><br><img src="Selection_004.png" alt="4"></p>
<p>In order to use groups to store files. File system includes all the structures you might expect a file system within each group.<br><img src="Selection_005.png" alt="5"><br>S: superblock.<br>ib: inode bitmap<br>db: data bitmap</p>
<h1 id="Policies-How-To-Allocate-Files-and-Directories"><a href="#Policies-How-To-Allocate-Files-and-Directories" class="headerlink" title="Policies: How To Allocate Files and Directories"></a>Policies: How To Allocate Files and Directories</h1><p>basic mantra: keep related stuff together, keep unrelated stuff far apart.</p>
<p>related data in same block group.</p>
<ol>
<li>data blocks of a file in the same group to avoid long seeks</li>
<li>put files belong to same directory in the same cylinder group</li>
</ol>
<p>see an example:</p>
<ul>
<li>assuem there are only 10 inodes and 10 data blocks in each group</li>
<li>the three directories (the root directory /, /a, and /b) and</li>
<li>four files (/a/c, /a/d, /a/e, /b/f) are placed within them per the FFS policies</li>
<li>Assume the regular files are each two blocks in size, and that the directories have just a single block of data</li>
<li>we use the obvious symbols for each file or directory (i.e., / for the root directory, a for /a, f for /b/f, and so forth)<br><img src="Selection_006.png" alt="6"><br>FFS policy’s two positive things:</li>
<li>data blocks of each file are near file’s inode</li>
<li>files in the same directory are near one another</li>
</ul>
<h1 id="The-Large-File-Exception"><a href="#The-Large-File-Exception" class="headerlink" title="The Large-File Exception"></a>The Large-File Exception</h1><p>for large file, FFS does not save them all in one group, because it prevents subsequent “related” file from being placed within this block group, and may hurt file-access locality.</p>
<p>So instead of saving file like this:<br><img src="Selection_007.png" alt="7"><br>actually saving like this:<br><img src="Selection_008.png" alt="8"></p>
<p>increase chunk size can improve disk performance. because it can reduce the seek time(change to different group).</p>
<p>Assuming</p>
<ul>
<li>average positioning time for disk is 10ms</li>
<li>disk transfer data at 40MB/s<br>if we want to spend half our time seeking between chunks and half our time transferring data(achieve 50% of peak disk performance), then we need to spend 10ms transferring data for every 10ms positioning.<br>So the question becomes: how big does a chunk have to be in order to spend 10 ms in transfer?<br>  the answer is 40M/s*10ms = 400KB （因为每次peak都要耗费10ms，所以50%效率就是还有10ms用来传输，那么一个chunk至少满足这个要求，才会不用再次进行seek）</li>
</ul>
<p>if we want to achieve 90% of peak bandwith:<br>    the answer is 40M/s*90ms = 3.6MB （因为每次peak都要耗费10ms，所以90%效率就是还有90ms用来传输，那么一个chunk至少满足这个要求，才会不用再次进行seek）</p>
<p>FFS did not use this type of calculation in order to spread large files across groups. based on the structure of the inode itself:</p>
<ul>
<li>The first twelve direct blocks were placed in the same group as the inode;</li>
<li>each subsequent indirect block, and all the blocks it pointed to, was placed in a different group</li>
</ul>
<p>With a block size of 4KB, and 32-bit disk addresses, this strategy implies that every 1024 blocks of the file (4MB) were placed in separate groups</p>
<h1 id="A-Few-Other-Things-About-FFS"><a href="#A-Few-Other-Things-About-FFS" class="headerlink" title="A Few Other Things About FFS"></a>A Few Other Things About FFS</h1><p>If use 4KB block, when many files are 2KB, then it will waste half of disk size, causing internal fragmentation.</p>
<p>The solution is FFS introduce <code>sub-blocks</code>, which is 512-byte little blocks that file system could allocate to files. So for the 2KB small files, then it wil occupy 4 sub-blocks and not waste the entire 4KB block. As file grew, the FS will continue allocating 512-byte sub-blocks to it untill it accuire a full 4KB of data. At this point, FFS will find a 4KB block, copy the sub-blocks into it, and free the sub-blocks for future use.</p>
<p><strong>But again, this will waste time for seeking 512byte subblocks</strong>. The FFS modifying the libc library and buffer write and then issue 4KB chunks to the file system.</p>
<p>Before the SCSI and other modern device interface, at the time, disks were much less sophisticated and required the host CPU to control their operation in a more hands-on way. A problem arose in FFS when a file was placed on consecutive sectors of the disk.<br><img src="Selection_009.png" alt="9"><br> imagine this is the overlook of a disk, number indicates different blocks.</p>
<p>In particular, the problem arose during sequential reads. FFS would first issue a read to block 0; by the time the read was complete, and FFS issued a read to block 1, it was too late: block 1 had rotated under the head and now the read to block 1 would incur a full rotation. (简而言之，就是发出读block0命令，刚发读取block1命令，block1已经被跳过了，所以需要等disk再转一圈）</p>
<p><img src="Selection_010.png" alt="10"><br> FFS could use this layout to solve rotation problem.</p>
<p>FFS was smart enough to figure out for a particular disk, how many blocks it should skip in doing layout in order to avoid the extra rotations. This technique was called <code>parameterization</code>. (这时候你会有疑问，这样的话就只能达到50%的throughput)</p>
<p>Modern disk, in order to solve this problem, internally read the entire track in and buffer it in an internal disk cache( <code>track buffer</code>). Then<br> on subsequent reads to the track, the disk will just return the desired data from its cache.(利用disk cache 解决 throughput 问题)</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-ffs.pdf" target="_blank" rel="noopener">Fast File System (FFS)</a></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>LevelDb 详解</title>
    <url>/blog/leveldb-intro/</url>
    <content><![CDATA[<p>说起LevelDb也许您不清楚，但是如果作为IT工程师，不知道下面两位大神级别的工程师，那您的领导估计会Hold不住了：Jeff Dean和Sanjay Ghemawat。这两位是Google公司重量级的工程师，为数甚少的Google Fellow之二。</p>
<p>Jeff Dean其人：<a href="http://research.google.com/people/jeff/index.html，Google大规模分布式平台Bigtable和MapReduce主要设计和实现者。" target="_blank" rel="noopener">http://research.google.com/people/jeff/index.html，Google大规模分布式平台Bigtable和MapReduce主要设计和实现者。</a></p>
<p>Sanjay Ghemawat其人：<a href="http://research.google.com/people/sanjay/index.html，Google大规模分布式平台GFS，Bigtable和MapReduce主要设计和实现工程师。" target="_blank" rel="noopener">http://research.google.com/people/sanjay/index.html，Google大规模分布式平台GFS，Bigtable和MapReduce主要设计和实现工程师。</a></p>
<p>LevelDb就是这两位大神级别的工程师发起的开源项目，简而言之，LevelDb是能够处理十亿级别规模Key-Value型数据持久性存储的C++ 程序库。正像上面介绍的，这二位是Bigtable的设计和实现者，如果了解Bigtable的话，应该知道在这个影响深远的分布式存储系统中有两个核心的部分：<code>Master Server</code>和<code>Tablet Server</code>。其中Master Server做一些管理数据的存储以及分布式调度工作，实际的分布式数据存储以及读写操作是由Tablet Server完成的，而LevelDb则可以理解为一个简化版的Tablet Server。</p>
<a id="more"></a>
<p>LevelDb有如下一些特点：</p>
<ul>
<li>首先，LevelDb是一个持久化存储的KV系统，和Redis这种内存型的KV系统不同，LevelDb不会像Redis一样狂吃内存，而是将大部分数据存储到磁盘上。</li>
<li>其次，LevleDb在存储数据时，是根据记录的key值有序存储的，就是说相邻的key值在存储文件中是依次顺序存储的，而应用可以自定义key大小比较函数，LevleDb会按照用户定义的比较函数依序存储这些记录。</li>
<li>再次，像大多数KV系统一样，LevelDb的操作接口很简单，基本操作包括写记录，读记录以及删除记录。也支持针对多条操作的原子批量操作。</li>
<li>另外，LevelDb支持数据快照（snapshot）功能，使得读取操作不受写操作影响，可以在读操作过程中始终看到一致的数据。</li>
</ul>
<p>除此外，LevelDb还支持数据压缩等操作，这对于减小存储空间以及增快IO效率都有直接的帮助。</p>
<p>LevelDb性能非常突出，官方网站报道其随机写性能达到40万条记录每秒，而随机读性能达到6万条记录每秒。总体来说，LevelDb的写操作要大大快于读操作，而顺序读写操作则大大快于随机读写操作。至于为何是这样，看了我们后续推出的LevelDb日知录，估计您会了解其内在原因。</p>
<h1 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h1><p>LevelDb本质上是一套存储系统以及在这套存储系统上提供的一些操作接口。为了便于理解整个系统及其处理流程，我们可以从两个不同的角度来看待LevleDb：<code>静态角度</code>和<code>动态角度</code>。</p>
<p>从静态角度，可以假想整个系统正在运行过程中（不断插入删除读取数据），此时我们给LevelDb照相，从照片可以看到之前系统的数据在内存和磁盘中是如何分布的，处于什么状态等；<br>从动态的角度，主要是了解系统是如何写入一条记录，读出一条记录，删除一条记录的，同时也包括除了这些接口操作外的内部操作比如compaction，系统运行时崩溃后如何恢复系统等等方面。</p>
<p> 本节所讲的整体架构主要从静态角度来描述，之后接下来的几节内容会详述静态结构涉及到的文件或者内存数据结构，LevelDb日知录后半部分主要介绍动态视角下的LevelDb，就是说整个系统是怎么运转起来的。</p>
<p>LevelDb作为存储系统，数据记录的存储介质包括内存以及磁盘文件，如果像上面说的，当LevelDb运行了一段时间，此时我们给LevelDb进行透视拍照，那么您会看到如下一番景象：</p>
<p><img src="LevelDb1.png" alt="LevelDb1"></p>
<p>从图中可以看出，构成LevelDb静态结构的包括六个主要部分：内存中的<code>MemTable</code>和<code>Immutable MemTable</code>以及磁盘上的几种主要文件：<code>Current文件</code>，<code>Manifest文件</code>，<code>log文件</code>以及<code>SSTable文件</code>。当然，LevelDb除了这六个主要部分还有一些辅助的文件，但是以上六个文件和数据结构是LevelDb的主体构成元素。</p>
<p>LevelDb的Log文件和Memtable与Bigtable论文中介绍的是一致的，当应用写入一条Key:Value记录的时候，LevelDb会先往log文件里写入，成功后将记录插进Memtable中，这样基本就算完成了写入操作，因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，所以这是为何说LevelDb写入速度极快的主要原因。</p>
<p>Log文件在系统中的作用主要是用于系统崩溃恢复而不丢失数据，假如没有Log文件，因为写入的记录刚开始是保存在内存中的，此时如果系统崩溃，内存中的数据还没有来得及Dump到磁盘，所以会丢失数据（Redis就存在这个问题）。为了避免这种情况，LevelDb在写入内存前先将操作记录到Log文件中，然后再记入内存中，这样即使系统崩溃，也可以从Log文件中恢复内存中的Memtable，不会造成数据的丢失。</p>
<p>当Memtable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevleDb会生成新的Log文件和Memtable，原先的Memtable就成为Immutable Memtable，顾名思义，就是说这个Memtable的内容是不可更改的，只能读不能写入或者删除。新到来的数据被记入新的Log文件和Memtable，LevelDb后台调度会将Immutable Memtable的数据导出到磁盘，形成一个新的SSTable文件。SSTable就是由内存中的数据不断导出并进行Compaction操作后形成的，而且SSTable的所有文件是一种层级结构，第一层为Level 0，第二层为Level 1，依次类推，层级逐渐增高，这也是为何称之为LevelDb的原因。</p>
<p>SSTable中的文件是Key有序的，就是说在文件中小key记录排在大Key记录之前，各个Level的SSTable都是如此，但是这里需要注意的一点是：<code>Level 0</code>的SSTable文件（后缀为.sst）和其它Level的文件相比有<code>特殊性</code>：这个层级内的.sst文件，两个文件<code>可能存在key重叠</code>，比如有两个level 0的sst文件，文件A和文件B，文件A的key范围是：{bar， car}，文件B的Key范围是{blue，samecity}，那么很可能两个文件都存在key=”blood”的记录。对于其它Level的SSTable文件来说，则不会出现同一层级内.sst文件的key重叠现象，就是说Level L中任意两个.sst文件，那么可以保证它们的key值是不会重叠的。这点需要特别注意，后面您会看到很多操作的差异都是由于这个原因造成的。</p>
<p>SSTable中的某个文件属于特定层级，而且其存储的记录是key有序的，那么必然有文件中的最小key和最大key，这是非常重要的信息，LevelDb应该记下这些信息。Manifest就是干这个的，它记载了SSTable各个文件的管理信息，比如属于哪个Level，文件名称叫啥，最小key和最大key各自是多少。下图是Manifest所存储内容的示意：</p>
<p><img src="LevelDb2.png" alt="LevelDb2"></p>
<p>图中只显示了两个文件（manifest会记载所有SSTable文件的这些信息），即Level 0的test.sst1和test.sst2文件，同时记载了这些文件各自对应的key范围，比如test.sstt1的key范围是“an”到 “banana”，而文件test.sst2的key范围是“baby”到“samecity”，可以看出两者的key范围是有重叠的。</p>
<p>Current文件是干什么的呢？这个文件的内容只有一个信息，就是记载当前的manifest文件名。因为在LevleDb的运行过程中，随着Compaction的进行，SSTable文件会发生变化，会有新的文件产生，老的文件被废弃，Manifest也会跟着反映这种变化，此时往往会新生成Manifest文件来记载这种变化，而Current则用来指出哪个Manifest文件才是我们关心的那个Manifest文件。</p>
<p>以上介绍的内容就构成了LevelDb的整体静态结构，在LevelDb日知录接下来的内容中，我们会首先介绍重要文件或者内存数据的具体数据布局与结构。</p>
<h1 id="Log文件"><a href="#Log文件" class="headerlink" title="Log文件"></a>Log文件</h1><p>上节内容讲到log文件在LevelDb中的主要作用是系统故障恢复时，能够保证不会丢失数据。因为在将记录写入内存的Memtable之前，会先写入Log文件，这样即使系统发生故障，Memtable中的数据没有来得及Dump到磁盘的SSTable文件，LevelDB也可以根据log文件恢复内存的Memtable数据结构内容，不会造成系统丢失数据，在这点上LevelDb和Bigtable是一致的。</p>
<p>下面我们带大家看看log文件的具体物理和逻辑布局是怎样的，LevelDb对于一个log文件，会把它切割成以32K为单位的物理Block，每次读取的单位以一个Block作为基本读取单位，下图展示的log文件由3个Block构成，所以从物理布局来讲，一个log文件就是由连续的32K大小Block构成的。</p>
<p><img src="LevelDb3.png" alt="LevelDb3"></p>
<p>在应用的视野里是看不到这些Block的，应用看到的是一系列的Key:Value对，在LevelDb内部，会将一个Key:Value对看做一条记录的数据，另外在这个数据前增加一个记录头，用来记载一些管理信息，以方便内部处理，下图显示了一个记录在LevelDb内部是如何表示的。<br><img src="LevelDb4.png" alt="LevelDb4"></p>
<p>记录头包含三个字段，<code>ChechSum</code>是对“类型”和“数据”字段的校验码，为了避免处理不完整或者是被破坏的数据，当LevelDb读取记录数据时候会对数据进行校验，如果发现和存储的CheckSum相同，说明数据完整无破坏，可以继续后续流程。“记录长度”记载了数据的大小，“数据”则是上面讲的Key:Value数值对，“类型”字段则指出了每条记录的逻辑结构和log文件物理分块结构之间的关系，具体而言，主要有以下四种类型：FULL/FIRST/MIDDLE/LAST。</p>
<p>如果记录类型是FULL，代表了当前记录内容完整地存储在一个物理Block里，没有被不同的物理Block切割开；如果记录被相邻的物理Block切割开，则类型会是其他三种类型中的一种。我们以之前图中所示的例子来具体说明。</p>
<p>假设目前存在三条记录，Record A，Record B和Record C，其中Record A大小为10K，Record B 大小为80K，Record C大小为12K，那么其在log文件中的逻辑布局会如图3.1所示。Record A是图中蓝色区域所示，因为大小为10K&lt;32K，能够放在一个物理Block中，所以其类型为FULL；Record B 大小为80K，而Block 1因为放入了Record A，所以还剩下22K，不足以放下Record B，所以在Block 1的剩余部分放入Record B的开头一部分，类型标识为FIRST，代表了是一个记录的起始部分；Record B还有58K没有存储，这些只能依次放在后续的物理Block里面，因为Block 2大小只有32K，仍然放不下Record B的剩余部分，所以Block 2全部用来放Record B，且标识类型为MIDDLE，意思是这是Record B中间一段数据；Record B剩下的部分可以完全放在Block 3中，类型标识为LAST，代表了这是Record B的末尾数据；图中黄色的Record C因为大小为12K，Block 3剩下的空间足以全部放下它，所以其类型标识为FULL。</p>
<p>从这个小例子可以看出逻辑记录和物理Block之间的关系，LevelDb一次物理读取为一个Block，然后根据类型情况拼接出逻辑记录，供后续流程处理。</p>
<h1 id="SSTable文件"><a href="#SSTable文件" class="headerlink" title="SSTable文件"></a>SSTable文件</h1><p>SSTable是Bigtable中至关重要的一块，对于LevelDb来说也是如此，对LevelDb的SSTable实现细节的了解也有助于了解Bigtable中一些实现细节。</p>
<p>本节内容主要讲述SSTable的静态布局结构，我们曾在“LevelDb日知录之二：整体架构”中说过，SSTable文件形成了不同Level的层级结构，至于这个层级结构是如何形成的我们放在后面Compaction一节细说。本节主要介绍SSTable某个文件的物理布局和逻辑布局结构，这对了解LevelDb的运行过程很有帮助。</p>
<p>LevelDb不同层级有很多SSTable文件（以后缀.sst为特征），所有.sst文件内部布局都是一样的。上节介绍Log文件是物理分块的，SSTable也一样会将文件划分为固定大小的物理存储块，但是两者逻辑布局大不相同，根本原因是：Log文件中的记录是Key无序的，即先后记录的key大小没有明确大小关系，而.sst文件内部则是根据记录的Key由小到大排列的，从下面介绍的SSTable布局可以体会到Key有序是为何如此设计.sst文件结构的关键。</p>
<p><img src="LevelDb5.png" alt="LevelDb5"></p>
<p>上图展示了一个.sst文件的物理划分结构，同Log文件一样，也是划分为固定大小的存储块，每个Block分为三个部分，黄色部分是<code>数据存储区</code>， 蓝色的Type区用于标识数据存储区是否采用了数据压缩算法（Snappy压缩或者无压缩两种），CRC部分则是数据校验码，用于判别数据是否在生成和传输中出错。</p>
<p>以上是.sst的物理布局，下面介绍.sst文件的逻辑布局，所谓逻辑布局，就是说尽管大家都是物理块，但是每一块存储什么内容，内部又有什么结构等。下面展示了.sst文件的内部逻辑解释。</p>
<p><img src="LevelDb6.png" alt="LevelDb6"><br>图4.2 逻辑布局</p>
<p>从上图可以看出，从大的方面，可以将.sst文件划分为<code>数据存储区</code>和<code>数据管理区</code>，数据存储区存放实际的Key:Value数据，数据管理区则提供一些索引指针等管理数据，目的是更快速便捷的查找相应的记录。两个区域都是在上述的分块基础上的，就是说文件的前面若干块实际存储KV数据，后面数据管理区存储管理数据。管理数据又分为四种不同类型：</p>
<ul>
<li>Meta Block</li>
<li>MetaBlock 索引</li>
<li>Index Block数据索引块</li>
<li>Footer文件尾部块</li>
</ul>
<p>LevelDb 1.2版对于Meta Block尚无实际使用，只是保留了一个接口，估计会在后续版本中加入内容，下面我们看看数据索引区和文件尾部Footer的内部结构。</p>
<p><img src="LevelDb7.png" alt="LevelDb7"><br>图4.3 数据索引</p>
<p>上图是数据索引的内部结构示意图。再次强调一下，Data Block内的KV记录是按照Key由小到大排列的，数据索引区的每条记录是对某个Data Block建立的索引信息，每条索引信息包含三个内容，以数据块i的索引<code>Index i</code>来说：</p>
<ul>
<li>红色部分的第一个字段记载大于等于数据块i中最大的Key值的那个Key;</li>
<li>第二个字段指出数据块i在.sst文件中的起始位置;</li>
<li>第三个字段指出Data Block i的大小（有时候是有数据压缩的）。</li>
</ul>
<p>后面两个字段好理解，是用于定位数据块在文件中的位置的，第一个字段需要详细解释一下，在索引里保存的这个Key值未必一定是某条记录的Key，以上图的例子来说，假设数据块i 的最小Key=“samecity”，最大Key=“the best”;数据块i+1的最小Key=“the fox”，最大Key=“zoo”，那么对于数据块i的索引Index i来说，其第一个字段记载大于等于数据块i的最大Key(“the best”)同时要小于数据块i+1的最小Key(“the fox”)，所以例子中Index i的第一个字段是：“the c”，这个是满足要求的；而Index i+1的第一个字段则是“zoo”，即数据块i+1的最大Key。</p>
<p>文件末尾Footer块的内部结构见下图，metaindex_handle指出了metaindex block的起始位置和大小；inex_handle指出了index Block的起始地址和大小；这两个字段可以理解为索引的索引，是为了正确读出索引值而设立的，后面跟着一个填充区和魔数。</p>
<p><img src="LevelDb8.png" alt="LevelDb8"><br>图4.4 Footer</p>
<p>上面主要介绍的是数据管理区的内部结构，下面我们看看数据区的一个Block的数据部分内部是如何布局的（图4.1中的h黄色部分），图4.5是其内部布局示意图。</p>
<p><img src="LevelDb9.png" alt="LevelDb9"><br>图4.5 数据Block内部结构</p>
<p>从图中可以看出，其内部也分为两个部分，前面是一个个KV记录，其顺序是根据Key值由小到大排列的，在Block尾部则是一些“重启点”（Restart Point）,其实是一些指针，指出Block内容中的一些记录位置。</p>
<p>“重启点”是干什么的呢？我们一再强调，Block内容里的KV记录是按照Key大小有序的，这样的话，相邻的两条记录很可能Key部分存在重叠，比如key i=“the Car”，Key i+1=“the color”,那么两者存在重叠部分“the c”，为了减少Key的存储量，Key i+1可以只存储和上一条Key不同的部分“olor”，两者的共同部分从Key i中可以获得。记录的Key在Block内容部分就是这么存储的，主要目的是减少存储开销。“重启点”的意思是：在这条记录开始，不再采取只记载不同的Key部分，而是重新记录所有的Key值，假设Key i+1是一个重启点，那么Key里面会完整存储“the color”，而不是采用简略的“olor”方式。Block尾部就是指出哪些记录是这些重启点的。</p>
<p><img src="LevelDb10.png" alt="LevelDb10"><br> 图4.6 记录格式</p>
<p>在Block内容区，每个KV记录的内部结构是怎样的？图4.6给出了其详细结构，每个记录包含5个字段：key共享长度，比如上面的“olor”记录， 其key和上一条记录共享的Key部分长度是“the c”的长度，即5；key非共享长度，对于“olor”来说，是4；value长度指出Key:Value中Value的长度，在后面的Value内容字段中存储实际的Value值；而key非共享内容则实际存储“olor”这个Key字符串。</p>
<p>上面讲的这些就是.sst文件的全部内部奥秘。</p>
<h1 id="MemTable详解"><a href="#MemTable详解" class="headerlink" title="MemTable详解"></a>MemTable详解</h1><p>LevelDb日知录前述小节大致讲述了磁盘文件相关的重要静态结构，本小节讲述内存中的数据结构Memtable，Memtable在整个体系中的重要地位也不言而喻。总体而言，所有KV数据都是存储在Memtable，Immutable Memtable和SSTable中的，Immutable Memtable从结构上讲和Memtable是完全一样的，区别仅仅在于其是只读的，不允许写入操作，而Memtable则是允许写入和读取的。当Memtable写入的数据占用内存到达指定数量，则自动转换为Immutable Memtable，等待Dump到磁盘中，系统会自动生成新的Memtable供写操作写入新数据，理解了Memtable，那么Immutable Memtable自然不在话下。</p>
<p>LevelDb的MemTable提供了将KV数据写入，删除以及读取KV记录的操作接口，但是事实上Memtable并不存在真正的删除操作,删除某个Key的Value在Memtable内是作为插入一条记录实施的，但是会打上一个Key的删除标记，真正的删除操作是Lazy的，会在以后的Compaction过程中去掉这个KV。</p>
<p>需要注意的是，LevelDb的Memtable中KV对是根据Key大小有序存储的，在系统插入新的KV时，LevelDb要把这个KV插到合适的位置上以保持这种Key有序性。其实，LevelDb的Memtable类只是一个接口类，真正的操作是通过背后的SkipList来做的，包括插入操作和读取操作等，所以Memtable的核心数据结构是一个SkipList。</p>
<p>SkipList是由William Pugh发明。他在Communications of the ACM June 1990, 33(6) 668-676 发表了Skip lists: a probabilistic alternative to balanced trees，在该论文中详细解释了SkipList的数据结构和插入删除操作。</p>
<p>SkipList是平衡树的一种替代数据结构，但是和红黑树不相同的是，SkipList对于树的平衡的实现是基于一种随机化的算法的，这样也就是说SkipList的插入和删除的工作是比较简单的。</p>
<p>关于SkipList的详细介绍可以参考这篇文章：<a href="http://www.cnblogs.com/xuqiang/archive/2011/05/22/2053516.html，讲述的很清楚，LevelDb的SkipList基本上是一个具体实现，并无特殊之处。" target="_blank" rel="noopener">http://www.cnblogs.com/xuqiang/archive/2011/05/22/2053516.html，讲述的很清楚，LevelDb的SkipList基本上是一个具体实现，并无特殊之处。</a></p>
<p>SkipList不仅是维护有序数据的一个简单实现，而且相比较平衡树来说，在插入数据的时候可以避免频繁的树节点调整操作，所以写入效率是很高的，LevelDb整体而言是个高写入系统，SkipList在其中应该也起到了很重要的作用。Redis为了加快插入操作，也使用了SkipList来作为内部实现数据结构。</p>
<h1 id="写入与删除记录"><a href="#写入与删除记录" class="headerlink" title="写入与删除记录"></a>写入与删除记录</h1><p>在之前的五节LevelDb日知录中，我们介绍了LevelDb的一些静态文件及其详细布局，从本节开始，我们看看LevelDb的一些动态操作，比如读写记录，Compaction，错误恢复等操作。</p>
<p>本节介绍levelDb的记录更新操作，即插入一条KV记录或者删除一条KV记录。levelDb的更新操作速度是非常快的，源于其内部机制决定了这种更新操作的简单性。</p>
<p><img src="LevelDb11.png" alt="LevelDb11"><br>图6.1 LevelDb写入记录</p>
<p>图6.1是levelDb如何更新KV数据的示意图，从图中可以看出，对于一个插入操作Put(Key,Value)来说，完成插入操作包含两个具体步骤：首先是将这条KV记录以顺序写的方式追加到之前介绍过的log文件末尾，因为尽管这是一个磁盘读写操作，但是文件的顺序追加写入效率是很高的，所以并不会导致写入速度的降低；第二个步骤是:如果写入log文件成功，那么将这条KV记录插入内存中的Memtable中，前面介绍过，Memtable只是一层封装，其内部其实是一个Key有序的SkipList列表，插入一条新记录的过程也很简单，即先查找合适的插入位置，然后修改相应的链接指针将新记录插入即可。完成这一步，写入记录就算完成了，所以一个插入记录操作涉及一次磁盘文件追加写和内存SkipList插入操作，这是为何levelDb写入速度如此高效的根本原因。</p>
<p>从上面的介绍过程中也可以看出：log文件内是key无序的，而Memtable中是key有序的。那么如果是删除一条KV记录呢？对于levelDb来说，并不存在立即删除的操作，而是与插入操作相同的，区别是，插入操作插入的是Key:Value 值，而删除操作插入的是“Key:删除标记”，并不真正去删除记录，而是后台Compaction的时候才去做真正的删除操作。</p>
<p>levelDb的写入操作就是如此简单。真正的麻烦在后面将要介绍的读取操作中。  </p>
<h1 id="读取记录"><a href="#读取记录" class="headerlink" title="读取记录"></a>读取记录</h1><p>LevelDb是针对大规模Key/Value数据的单机存储库，从应用的角度来看，LevelDb就是一个存储工具。而作为称职的存储工具，常见的调用接口无非是新增KV，删除KV，读取KV，更新Key对应的Value值这么几种操作。LevelDb的接口没有直接支持更新操作的接口，如果需要更新某个Key的Value,你可以选择直接生猛地插入新的KV，保持Key相同，这样系统内的key对应的value就会被更新；或者你可以先删除旧的KV， 之后再插入新的KV，这样比较委婉地完成KV的更新操作。</p>
<p>假设应用提交一个Key值，下面我们看看LevelDb是如何从存储的数据中读出其对应的Value值的。图7-1是LevelDb读取过程的整体示意图。</p>
<p><img src="LevelDb12.png" alt="LevelDb12"><br>图7-1 LevelDb读取记录流程</p>
<p>LevelDb首先会去查看内存中的Memtable，如果Memtable中包含key及其对应的value，则返回value值即可；如果在Memtable没有读到key，则接下来到同样处于内存中的Immutable Memtable中去读取，类似地，如果读到就返回，若是没有读到,那么只能万般无奈下从磁盘中的大量SSTable文件中查找。因为SSTable数量较多，而且分成多个Level，所以在SSTable中读数据是相当蜿蜒曲折的一段旅程。总的读取原则是这样的：首先从属于level 0的文件中查找，如果找到则返回对应的value值，如果没有找到那么到level 1中的文件中去找，如此循环往复，直到在某层SSTable文件中找到这个key对应的value为止（或者查到最高level，查找失败，说明整个系统中不存在这个Key)。</p>
<p>那么为什么是从Memtable到Immutable Memtable，再从Immutable Memtable到文件，而文件中为何是从低level到高level这么一个查询路径呢？道理何在？之所以选择这么个查询路径，是因为从信息的更新时间来说，很明显Memtable存储的是最新鲜的KV对；Immutable Memtable中存储的KV数据对的新鲜程度次之；而所有SSTable文件中的KV数据新鲜程度一定不如内存中的Memtable和Immutable Memtable的。对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。也就是说，上面列出的查找路径就是按照数据新鲜程度排列出来的，越新鲜的越先查找。</p>
<p>为啥要优先查找新鲜的数据呢？这个道理不言而喻，举个例子。比如我们先往levelDb里面插入一条数据 {key=”<a href="http://www.samecity.com&quot;" target="_blank" rel="noopener">www.samecity.com&quot;</a>  value=”我们”},过了几天，samecity网站改名为：69同城，此时我们插入数据{key=”<a href="http://www.samecity.com&quot;" target="_blank" rel="noopener">www.samecity.com&quot;</a>  value=”69同城”}，同样的key,不同的value；逻辑上理解好像levelDb中只有一个存储记录，即第二个记录，但是在levelDb中很可能存在两条记录，即上面的两个记录都在levelDb中存储了，此时如果用户查询key=”<a href="http://www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。" target="_blank" rel="noopener">www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。</a></p>
<p>前文有讲：对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。这是一个结论，理论上需要一个证明过程，否则会招致如下的问题：为神马呢？从道理上讲呢，很明白：因为Level L+1的数据不是从石头缝里蹦出来的，也不是做梦梦到的，那它是从哪里来的？Level L+1的数据是从Level L 经过Compaction后得到的（如果您不知道什么是Compaction，那么……..也许以后会知道的），也就是说，您看到的现在的Level L+1层的SSTable数据是从原来的Level L中来的，现在的Level L比原来的Level L数据要新鲜，所以可证，现在的Level L比现在的Level L+1的数据要新鲜。</p>
<p>SSTable文件很多，如何快速地找到key对应的value值？在LevelDb中，level 0一直都爱搞特殊化，在level 0和其它level中查找某个key的过程是不一样的。因为level 0下的不同文件可能key的范围有重叠，某个要查询的key有可能多个文件都包含，这样的话LevelDb的策略是先找出level 0中哪些文件包含这个key（manifest文件中记载了level和对应的文件及文件里key的范围信息，LevelDb在内存中保留这种映射表）， 之后按照文件的新鲜程度排序，新的文件排在前面，之后依次查找，读出key对应的value。而如果是非level 0的话，因为这个level的文件之间key是不重叠的，所以只从一个文件就可以找到key对应的value。</p>
<p>最后一个问题,如果给定一个要查询的key和某个key range包含这个key的SSTable文件，那么levelDb是如何进行具体查找过程的呢？levelDb一般会先在内存中的Cache中查找是否包含这个文件的缓存记录，如果包含，则从缓存中读取；如果不包含，则打开SSTable文件，同时将这个文件的索引部分加载到内存中并放入Cache中。 这样Cache里面就有了这个SSTable的缓存项，但是只有索引部分在内存中，之后levelDb根据索引可以定位到哪个内容Block会包含这条key，从文件中读出这个Block的内容，在根据记录一一比较，如果找到则返回结果，如果没有找到，那么说明这个level的SSTable文件并不包含这个key，所以到下一级别的SSTable中去查找。</p>
<p>从之前介绍的LevelDb的写操作和这里介绍的读操作可以看出，相对写操作，读操作处理起来要复杂很多，所以写的速度必然要远远高于读数据的速度，也就是说，LevelDb比较适合写操作多于读操作的应用场合。而如果应用是很多读操作类型的，那么顺序读取效率会比较高，因为这样大部分内容都会在缓存中找到，尽可能避免大量的随机读取操作。</p>
<h1 id="Compaction操作"><a href="#Compaction操作" class="headerlink" title="Compaction操作"></a>Compaction操作</h1><p>前文有述，对于LevelDb来说，写入记录操作很简单，删除记录仅仅写入一个删除标记就算完事，但是读取记录比较复杂，需要在内存以及各个层级文件中依照新鲜程度依次查找，代价很高。为了加快读取速度，levelDb采取了compaction的方式来对已有的记录进行整理压缩，通过这种方式，来删除掉一些不再有效的KV数据，减小数据规模，减少文件数量等。</p>
<p>levelDb的compaction机制和过程与Bigtable所讲述的是基本一致的，Bigtable中讲到三种类型的compaction: minor ，major和full。所谓minor Compaction，就是把memtable中的数据导出到SSTable文件中；major compaction就是合并不同层级的SSTable文件，而full compaction就是将所有SSTable进行合并。</p>
<p>LevelDb包含其中两种，<code>minor</code>和<code>major</code>。</p>
<p>我们将为大家详细叙述其机理。</p>
<p>先来看看<code>minor Compaction</code>的过程。Minor compaction 的目的是当内存中的memtable大小到了一定值时，将内容保存到磁盘文件中，图8.1是其机理示意图。</p>
<p><img src="LevelDb13.png" alt="LevelDb13"><br>图8.1 minor compaction</p>
<p>从8.1可以看出，当memtable数量到了一定程度会转换为immutable memtable，此时不能往其中写入记录，只能从中读取KV内容。之前介绍过，immutable memtable其实是一个多层级队列SkipList，其中的记录是根据key有序排列的。所以这个minor compaction实现起来也很简单，就是按照immutable memtable中记录由小到大遍历，并依次写入一个level 0 的新建SSTable文件中，写完后建立文件的index 数据，这样就完成了一次minor compaction。从图中也可以看出，对于被删除的记录，在minor compaction过程中并不真正删除这个记录，原因也很简单，这里只知道要删掉key记录，但是这个KV数据在哪里?那需要复杂的查找，所以在minor compaction的时候并不做删除，只是将这个key作为一个记录写入文件中，至于真正的删除操作，在以后更高层级的compaction中会去做。</p>
<p>当某个level下的SSTable文件数目超过一定设置值后，levelDb会从这个level的SSTable中选择一个文件（level&gt;0），将其和高一层级的level+1的SSTable文件合并，这就是major compaction。</p>
<p>我们知道在大于0的层级中，每个SSTable文件内的Key都是由小到大有序存储的，而且不同文件之间的key范围（文件内最小key和最大key之间）不会有任何重叠。Level 0的SSTable文件有些特殊，尽管每个文件也是根据Key由小到大排列，但是因为level 0的文件是通过minor compaction直接生成的，所以任意两个level 0下的两个sstable文件可能再key范围上有重叠。所以在做major compaction的时候，对于大于level 0的层级，选择其中一个文件就行，但是对于level 0来说，指定某个文件后，本level中很可能有其他SSTable文件的key范围和这个文件有重叠，这种情况下，要找出所有有重叠的文件和level 1的文件进行合并，即level 0在进行文件选择的时候，可能会有多个文件参与major compaction。</p>
<p>levelDb在选定某个level进行compaction后，还要选择是具体哪个文件要进行compaction，levelDb在这里有个小技巧， 就是说轮流来，比如这次是文件A进行compaction，那么下次就是在key range上紧挨着文件A的文件B进行compaction，这样每个文件都会有机会轮流和高层的level 文件进行合并。</p>
<p>如果选好了level L的文件A和level L+1层的文件进行合并，那么问题又来了，应该选择level L+1哪些文件进行合并？levelDb选择L+1层中和文件A在key range上有重叠的所有文件来和文件A进行合并。</p>
<p>也就是说，选定了level L的文件A,之后在level L+1中找到了所有需要合并的文件B,C,D…..等等。剩下的问题就是具体是如何进行major 合并的？就是说给定了一系列文件，每个文件内部是key有序的，如何对这些文件进行合并，使得新生成的文件仍然Key有序，同时抛掉哪些不再有价值的KV 数据。</p>
<p>图8.2说明了这一过程。</p>
<p><img src="LevelDb14.png" alt="LevelDb14"><br>图8.2 SSTable Compaction</p>
<p>Major compaction的过程如下：对多个文件采用多路归并排序的方式，依次找出其中最小的Key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个Key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入level L+1层中新生成的一个SSTable文件中。就这样对KV数据一一处理，形成了一系列新的L+1层数据文件，之前的L层文件和L+1层参与compaction 的文件数据此时已经没有意义了，所以全部删除。这样就完成了L层和L+1层文件记录的合并过程。</p>
<p>那么在major compaction过程中，判断一个KV记录是否抛弃的标准是什么呢？其中一个标准是:对于某个key来说，如果在小于L层中存在这个Key，那么这个KV在major compaction过程中可以抛掉。因为我们前面分析过，对于层级低于L的文件中如果存在同一Key的记录，那么说明对于Key来说，有更新鲜的Value存在，那么过去的Value就等于没有意义了，所以可以删除。</p>
<h1 id="levelDb中的Cache"><a href="#levelDb中的Cache" class="headerlink" title="levelDb中的Cache"></a>levelDb中的Cache</h1><p>书接前文，前面讲过对于levelDb来说，读取操作如果没有在内存的memtable中找到记录，要多次进行磁盘访问操作。假设最优情况，即第一次就在level 0中最新的文件中找到了这个key，那么也需要读取2次磁盘，一次是将SSTable的文件中的index部分读入内存，这样根据这个index可以确定key是在哪个block中存储；第二次是读入这个block的内容，然后在内存中查找key对应的value。</p>
<p>levelDb中引入了两个不同的Cache:Table Cache和Block Cache。其中Block Cache是配置可选的，即在配置文件中指定是否打开这个功能。</p>
<p><img src="LevelDb15.png" alt="LevelDb15"><br>图9.1 table cache</p>
<p>图9.1是table cache的结构。在Cache中，key值是SSTable的文件名称，Value部分包含两部分，一个是指向磁盘打开的SSTable文件的文件指针，这是为了方便读取内容；另外一个是指向内存中这个SSTable文件对应的Table结构指针，table结构在内存中，保存了SSTable的index内容以及用来指示block cache用的cache_id ,当然除此外还有其它一些内容。</p>
<p>比如在get(key)读取操作中，如果levelDb确定了key在某个level下某个文件A的key range范围内，那么需要判断是不是文件A真的包含这个KV。此时，levelDb会首先查找Table Cache，看这个文件是否在缓存里，如果找到了，那么根据index部分就可以查找是哪个block包含这个key。如果没有在缓存中找到文件，那么打开SSTable文件，将其index部分读入内存，然后插入Cache里面，去index里面定位哪个block包含这个Key 。如果确定了文件哪个block包含这个key，那么需要读入block内容，这是第二次读取。</p>
<p><img src="LevelDb16.png" alt="LevelDb16"><br>图9.2 block cache</p>
<p>Block Cache是为了加快这个过程的，图9.2是其结构示意图。其中的key是文件的cache_id加上这个block在文件中的起始位置block_offset。而value则是这个Block的内容。</p>
<p>如果levelDb发现这个block在block cache中，那么可以避免读取数据，直接在cache里的block内容里面查找key的value就行，如果没找到呢？那么读入block内容并把它插入block cache中。levelDb就是这样通过两个cache来加快读取速度的。从这里可以看出，如果读取的数据局部性比较好，也就是说要读的数据大部分在cache里面都能读到，那么读取效率应该还是很高的，而如果是对key进行顺序读取效率也应该不错，因为一次读入后可以多次被复用。但是如果是随机读取，您可以推断下其效率如何。</p>
<h1 id="Version、VersionEdit、VersionSet"><a href="#Version、VersionEdit、VersionSet" class="headerlink" title="Version、VersionEdit、VersionSet"></a>Version、VersionEdit、VersionSet</h1><p>Version 保存了当前磁盘以及内存中所有的文件信息，一般只有一个Version叫做”current” version（当前版本）。Leveldb还保存了一系列的历史版本，这些历史版本有什么作用呢？</p>
<p>当一个Iterator创建后，Iterator就引用到了current version(当前版本)，只要这个Iterator不被delete那么被Iterator引用的版本就会一直存活。这就意味着当你用完一个Iterator后，需要及时删除它。</p>
<p>当一次Compaction结束后（会生成新的文件，合并前的文件需要删除），Leveldb会创建一个新的版本作为当前版本，原先的当前版本就会变为历史版本。</p>
<p>VersionSet 是所有Version的集合，管理着所有存活的Version。</p>
<p>VersionEdit 表示Version之间的变化，相当于delta 增量，表示有增加了多少文件，删除了文件。下图表示他们之间的关系。</p>
<p>Version0 +VersionEdit–&gt;Version1</p>
<p>VersionEdit会保存到MANIFEST文件中，当做数据恢复时就会从MANIFEST文件中读出来重建数据。</p>
<p>leveldb的这种版本的控制，让我想到了双buffer切换，双buffer切换来自于图形学中，用于解决屏幕绘制时的闪屏问题，在服务器编程中也有用处。</p>
<p>比如我们的服务器上有一个字典库，每天我们需要更新这个字典库，我们可以新开一个buffer，将新的字典库加载到这个新buffer中，等到加载完毕，将字典的指针指向新的字典库。</p>
<p>leveldb的version管理和双buffer切换类似，但是如果原version被某个iterator引用，那么这个version会一直保持，直到没有被任何一个iterator引用，此时就可以删除这个version。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.cnblogs.com/haippy/archive/2011/12/04/2276064.html" target="_blank" rel="noopener">Leveldb 实现原理</a></p>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>How the append-only btree work</title>
    <url>/blog/how-the-append-only-btree-work/</url>
    <content><![CDATA[<p>原文：<a href="http://www.bzero.se/ldapd/btree.html" target="_blank" rel="noopener">http://www.bzero.se/ldapd/btree.html</a></p>
<p>该 tree 也被称为 Copy-On-Write Tree<br>考虑下图的这个3层 b tree.<br><img src="http://www.bzero.se/ldapd/how-the-btree-works.png" alt="3levelbtree"><br>该树由两层的 branch page（root 也是一个 branch page）和 5 个 leaf page 组成。key 和 data 都存储在 leaf page 里面。</p>
<a id="more"></a>
<p>这里，leaf chaining（叶节点之间的指针连接）并没有被支持，也就是叶子节点之间的顺序 access 特性不被支持（即没有指针从一个 leaf 指向下一个 leaf），这是因为该特性的实现，会要求每次 update 都去 rewrite 整个 tree。</p>
<p>该 tree 的 page 在 database 文件中，被顺序存储着。添加 page numbers 也只是意味着增加 file 的 offset（类似于给vector数组后面添加一个位置一样）。</p>
<p>meta page ？包括：</p>
<ul>
<li>一个指向 root page 的指针</li>
<li>一个 SHA1 hash</li>
<li>一个静态计数器（全局计数器）？</li>
</ul>
<p>当一个 file 被打开，它将会被从尾部的 page 开始扫描，直到找到一个有效的 meta page，从而根据上述的指针，找到 root page。</p>
<p><img src="http://www.bzero.se/ldapd/sequential-page-view.png" alt="sequential-page-view"></p>
<p>比如现在要更新 leaf page 8 上的值，不同于直接在该 page 上进行更改覆写，这里会直接产生一个具有 new value 的 page，并 append 到 file 尾部。如下图中的 page 12。</p>
<p><img src="http://www.bzero.se/ldapd/updated-btree.png" alt="updated-btree"></p>
<p>因为原本作为 leaf page 8 的位置，修改到了 page 12，它的每个 parent page 都需要更新对应的指针。</p>
<p>leaf 7 没有被影响。而 branch 6 作为被修改 leaf 的 parent，其指针值被影响了，所以一个新的 page 被创建出来 – branch page 11，同时一个新的 root 也被创建出来 – root 13，更新后的 tree 如上图所示。</p>
<p>这样，任何拥有 root page 9 的用户，仍然能够跟踪到没有被修改之前的值。这就是 database 自己的一个 snapshot。</p>
<p>在该 database file 中，新 page 只是不断的被 append 到 file 尾部，已经写入值的 page 并不会被影响。</p>
<p>修改一次数据后，当每个相关的 page 都被更新完毕，就会产生一个新的 meta page，指向新的 root page，如下图：</p>
<p><img src="http://www.bzero.se/ldapd/flattened-btree-page-structure.png" alt="flattened-btree-page-structure"></p>
<p>从结果上看，对一个page的修改（修改 leaf page 8），会导致 4 个新 page 被 append 到 file 尾部。这在一定程度上浪费了磁盘空间，但是这样顺序写操作，能够非常大的提升随即写性能。并且这里并不需要再记录 transaction log，用于数据恢复，该 database file 本身，就是一个 transaction log。</p>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>inode相关命令</title>
    <url>/blog/inode-command/</url>
    <content><![CDATA[<p>​<img src="inode1.png" alt="inode1"><br><code>touch</code>：新建文件<br><code>ls -i</code>：显示文件的inode<br><code>stat filename</code>：显示文件的所有状态信息，包括大小，inode id，link 数目，创建时间，修改时间等<br><code>ln file1 filelink1</code>：给file1创建一个名字叫做filelink1的链接，具有相同的 inode id<br><img src="1503226895173.png" alt="inodeid"><br>如果有一个文件名很奇怪，无法使用正常的 rm 命令删除，比如：“ab*<br>那么可以使用 <code>find . -inum xxxx -delete</code> 命令删除<br><img src="findcmd.png" alt="findcmd"><br><code>df -i</code>：查看inode资源的使用情况<br><img src="dfi.png" alt="dfi"><br>可以清楚看到inode的最大使用数目。</p>
<a id="more"></a>
<p>也可以使用 find 命令找出文件所在目录<br><img src="dir.png" alt="dir"></p>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>linux-cmd</tag>
      </tags>
  </entry>
  <entry>
    <title>第二章-IO大法</title>
    <url>/blog/talking-storage-chapter2/</url>
    <content><![CDATA[<h1 id="IO大法"><a href="#IO大法" class="headerlink" title="IO大法"></a>IO大法</h1><h2 id="PCI"><a href="#PCI" class="headerlink" title="PCI"></a>PCI</h2><p>PCI是Peripheral Component Interconnect(外设部件互连标准)。其连接在<code>南桥</code>上。<br><code>北桥</code>连接系统内存，CPU以及高速总线（ex. PCIE）。<br>PCI的<code>地址总线</code>和<code>数据总线</code>是<code>时分复用</code>（Time Division Multiplexing，TDM），即采用同一物理连接的不同时段来传输不同的信号。</p>
<p>数据传输时，分为传输的发起者（Master）和数据的接受者（Slave），同一时刻只有一对设备可以传输数据。</p>
<h3 id="中断共享"><a href="#中断共享" class="headerlink" title="中断共享"></a>中断共享</h3><p>硬件上，采用<code>电平触发</code>（PCI板卡设备用三极管拉低信号）<br>软件上，采用<code>中断链</code>（如果多个板卡共享一个中断，那么一个中断处理函数结束会指向下一个处理函数，发生中断时候，逐个检查，是则处理，不是则跳过）</p>
<a id="more"></a>
<h2 id="数据通信"><a href="#数据通信" class="headerlink" title="数据通信"></a>数据通信</h2><p>CPU向存储所在的地址（比如0x0A）发送命令。</p>
<ul>
<li>发送读（/写）命令</li>
<li>指明LBA（硬盘逻辑区块）</li>
<li>指明读取的内容到哪一段内存。</li>
</ul>
<ol>
<li>第一条指令指定了读时配置：完成是否触发中断，是否启用磁盘缓存。。。</li>
<li>第二条指令进行磁盘的<code>逻辑区块</code>到<code>实际区块</code>查找，转到该扇区，读取数据</li>
<li>第三条指令，在数据读出以后，会进入 DMA 操作，不需要 CPU 接入，读取结束，CPU从内存读取数据，并进行计算。</li>
</ol>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>大话存储</tag>
      </tags>
  </entry>
  <entry>
    <title>计算图形学中的微积分-反向传播算法</title>
    <url>/blog/Calculus-on-Computational-Graphs/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Backpropagation is the key algorithm that makes training deep models computationally tractable. For modern neural networks, it can make training with gradient descent as much as ten million times faster, relative to a naive implementation. That’s the difference between a model taking a week to train and taking 200,000 years.</p>
<p>Beyond its use in deep learning, backpropagation is a powerful computational tool in many other areas, ranging from weather forecasting to analyzing numerical stability – it just goes by different names. In fact, the algorithm has been reinvented at least dozens of times in different fields (see <a href="http://www.math.uiuc.edu/documenta/vol-ismp/52_griewank-andreas-b.pdf" target="_blank" rel="noopener">Griewank (2010)</a>). The general, application independent, name is “reverse-mode differentiation.”</p>
<a id="more"></a>
<p>Fundamentally, it’s a technique for calculating derivatives quickly. And it’s an essential trick to have in your bag, not only in deep learning, but in a wide variety of numerical computing situations.</p>
<h1 id="Computational-Graphs"><a href="#Computational-Graphs" class="headerlink" title="Computational Graphs"></a>Computational Graphs</h1><p>Computational graphs are a nice way to think about mathematical expressions. For example, consider the expression <code>e=(a+b)*(b+1)</code>. There are three operations: two additions and one multiplication. To help us talk about this, let’s introduce two intermediary variables, <code>c</code> and <code>d</code> so that every function’s output has a variable. We now have:</p>
<p>$$c=a+b$$</p>
<p>$$d=b+1$$</p>
<p>$$e=c*d$$</p>
<p>To create a computational graph, we make each of these operations, along with the input variables, into nodes. When one node’s value is the input to another node, an arrow goes from one to another.<br><img src="http://colah.github.io/posts/2015-08-Backprop/img/tree-def.png" alt="pic1"></p>
<p>These sorts of graphs come up all the time in computer science, especially in talking about functional programs. They are very closely related to the notions of dependency graphs and call graphs. They’re also the core abstraction behind the popular deep learning framework Theano.</p>
<p>We can evaluate the expression by setting the input variables to certain values and computing nodes up through the graph. For example, let’s set <code>a=2</code> and <code>b=1</code>:<br><img src="http://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png" alt="pic2"></p>
<p>The expression evaluates to <code>6</code>.</p>
<h1 id="Derivatives-on-Computational-Graphs"><a href="#Derivatives-on-Computational-Graphs" class="headerlink" title="Derivatives on Computational Graphs"></a>Derivatives on Computational Graphs</h1><p>If one wants to understand derivatives in a computational graph, the key is to understand derivatives on the edges. If <code>a</code> directly affects <code>c</code>, then we want to know how it affects <code>c</code>. If <code>a</code> changes a little bit, how does <code>c</code> change? We call this the partial derivative of <code>c</code> with respect to <code>a</code>.</p>
<p>To evaluate the partial derivatives in this graph, we need the sum rule and the product rule:</p>
<p>$$\frac{\partial}{\partial a}(a+b) = \frac{\partial a}{\partial a} + \frac{\partial b}{\partial a} = 1$$</p>
<p>$$\frac{\partial}{\partial u}uv = u\frac{\partial v}{\partial u} + v\frac{\partial u}{\partial u} = v$$</p>
<p>Below, the graph has the derivative on each edge labeled.<br><img src="http://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" alt="pic3"></p>
<p>What if we want to understand how nodes that aren’t directly connected affect each other? Let’s consider how <code>e</code> is affected by <code>a</code>. If we change <code>a</code> at a speed of 1, <code>c</code> also changes at a speed of <code>1</code>. In turn, <code>c</code> changing at a speed of <code>1</code> causes <code>e</code> to change at a speed of <code>2</code>. So <code>e</code> changes at a rate of <code>1∗2</code> with respect to <code>a</code>.</p>
<p>The general rule is to sum over all possible paths from one node to the other, multiplying the derivatives on each edge of the path together. For example, to get the derivative of ee with respect to <code>b</code> we get:</p>
<p>$$\frac{\partial e}{\partial b}= 1<em>2 + 1</em>3$$</p>
<p>This accounts for how b affects e through c and also how it affects it through d.</p>
<p>This general “sum over paths” rule is just a different way of thinking about the multivariate chain rule.</p>
<h1 id="Factoring-Paths"><a href="#Factoring-Paths" class="headerlink" title="Factoring Paths"></a>Factoring Paths</h1><p>The problem with just “summing over the paths” is that it’s very easy to get a combinatorial explosion in the number of possible paths.</p>
<p><img src="http://colah.github.io/posts/2015-08-Backprop/img/chain-def-greek.png" alt="pic4"></p>
<p>In the above diagram, there are three paths from <code>X</code> to <code>Y</code>, and a further three paths from <code>Y</code> to <code>Z</code>. If we want to get the derivative $\frac{\partial Z}{\partial X}$ by summing over all paths, we need to sum over <code>3∗3=9</code> paths:</p>
<p>$$\frac{\partial Z}{\partial X} = \alpha\delta + \alpha\epsilon + \alpha\zeta + \beta\delta + \beta\epsilon + \beta\zeta + \gamma\delta + \gamma\epsilon + \gamma\zeta$$</p>
<p>The above only has nine paths, but it would be easy to have the number of paths to grow exponentially as the graph becomes more complicated.</p>
<p>Instead of just naively summing over the paths, it would be much better to factor them:</p>
<p>$$\frac{\partial Z}{\partial X} = (\alpha + \beta + \gamma)(\delta + \epsilon + \zeta)$$</p>
<p>This is where “forward-mode differentiation” and “reverse-mode differentiation” come in. They’re algorithms for efficiently computing the sum by factoring the paths. Instead of summing over all of the paths explicitly, they compute the same sum more efficiently by merging paths back together at every node. In fact, both algorithms touch each edge exactly once!</p>
<p>Forward-mode differentiation starts at an input to the graph and moves towards the end. At every node, it sums all the paths feeding in. Each of those paths represents one way in which the input affects that node. By adding them up, we get the total way in which the node is affected by the input, it’s derivative.</p>
<p><img src="http://colah.github.io/posts/2015-08-Backprop/img/chain-forward-greek.png" alt="pic5"></p>
<p>Though you probably didn’t think of it in terms of graphs, forward-mode differentiation is very similar to what you implicitly learned to do if you took an introduction to calculus class.</p>
<p>Reverse-mode differentiation, on the other hand, starts at an output of the graph and moves towards the beginning. At each node, it merges all paths which originated at that node.</p>
<p><img src="http://colah.github.io/posts/2015-08-Backprop/img/chain-backward-greek.png" alt="pic6"></p>
<p>Forward-mode differentiation tracks how one input affects every node. Reverse-mode differentiation tracks how every node affects one output. That is, forward-mode differentiation applies the operator $\frac{\partial}{\partial X}$ to every node, while reverse mode differentiation applies the operator $\frac{\partial Z}{\partial}$ to every node</p>
<h1 id="Computational-Victories"><a href="#Computational-Victories" class="headerlink" title="Computational Victories"></a>Computational Victories</h1><p>At this point, you might wonder why anyone would care about reverse-mode differentiation. It looks like a strange way of doing the same thing as the forward-mode. Is there some advantage?</p>
<p>Let’s consider our original example again:</p>
<p><img src="http://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" alt="pic7"></p>
<p>We can use forward-mode differentiation from <code>b</code> up. This gives us the derivative of every node with respect to <code>b</code>.</p>
<p><img src="http://colah.github.io/posts/2015-08-Backprop/img/tree-forwradmode.png" alt="pic8"></p>
<p>We’ve computed $\frac{\partial e}{\partial b}$, the derivative of our output with respect to one of our inputs.</p>
<p>What if we do reverse-mode differentiation from $e$ down? This gives us the derivative of $e$ with respect to every node:</p>
<p><img src="http://colah.github.io/posts/2015-08-Backprop/img/tree-backprop.png" alt="pic9"></p>
<p>When I say that reverse-mode differentiation gives us the derivative of e with respect to every node, I really do mean every node. We get both $\frac{\partial e}{\partial a}$ and $\frac{\partial e}{\partial b}$, the derivatives of $e$ with respect to both inputs. Forward-mode differentiation gave us the derivative of our output with respect to a single input, but reverse-mode differentiation gives us all of them.</p>
<p>For this graph, that’s only a factor of two speed up, but imagine a function with a million inputs and one output. Forward-mode differentiation would require us to go through the graph a million times to get the derivatives. Reverse-mode differentiation can get them all in one fell swoop! A speed up of a factor of a million is pretty nice!</p>
<p>When training neural networks, we think of the cost (a value describing how bad a neural network performs) as a function of the parameters (numbers describing how the network behaves). We want to calculate the derivatives of the cost with respect to all the parameters, for use in gradient descent. Now, there’s often millions, or even tens of millions of parameters in a neural network. So, reverse-mode differentiation, called backpropagation in the context of neural networks, gives us a massive speed up!</p>
<p>(Are there any cases where forward-mode differentiation makes more sense? Yes, there are! Where the reverse-mode gives the derivatives of one output with respect to all inputs, the forward-mode gives us the derivatives of all outputs with respect to one input. If one has a function with lots of outputs, forward-mode differentiation can be much, much, much faster.)</p>
<h1 id="Isn’t-This-Trivial"><a href="#Isn’t-This-Trivial" class="headerlink" title="Isn’t This Trivial?"></a>Isn’t This Trivial?</h1><p>When I first understood what backpropagation was, my reaction was: “Oh, that’s just the chain rule! How did it take us so long to figure out?” I’m not the only one who’s had that reaction. It’s true that if you ask “is there a smart way to calculate derivatives in feedforward neural networks?” the answer isn’t that difficult.</p>
<p>But I think it was much more difficult than it might seem. You see, at the time backpropagation was invented, people weren’t very focused on the feedforward neural networks that we study. It also wasn’t obvious that derivatives were the right way to train them. Those are only obvious once you realize you can quickly calculate derivatives. There was a circular dependency.</p>
<p>Worse, it would be very easy to write off any piece of the circular dependency as impossible on casual thought. Training neural networks with derivatives? Surely you’d just get stuck in local minima. And obviously it would be expensive to compute all those derivatives. It’s only because we know this approach works that we don’t immediately start listing reasons it’s likely not to.</p>
<p>That’s the benefit of hindsight. Once you’ve framed the question, the hardest work is already done.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Derivatives are cheaper than you think. That’s the main lesson to take away from this post. In fact, they’re unintuitively cheap, and us silly humans have had to repeatedly rediscover this fact. That’s an important thing to understand in deep learning. It’s also a really useful thing to know in other fields, and only more so if it isn’t common knowledge.</p>
<p>Are there other lessons? I think there are.</p>
<p>Backpropagation is also a useful lens for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. The classic example of this is the problem of vanishing gradients in recurrent neural networks.</p>
<p>Finally, I claim there is a broad algorithmic lesson to take away from these techniques. Backpropagation and forward-mode differentiation use a powerful pair of tricks (linearization and dynamic programming) to compute derivatives more efficiently than one might think possible. If you really understand these techniques, you can use them to efficiently calculate several other interesting expressions involving derivatives. We’ll explore this in a later blog post.</p>
<p>This post gives a very abstract treatment of backpropagation. I strongly recommend reading Michael Nielsen’s chapter on it for an excellent discussion, more concretely focused on neural networks.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">Calculus on Computational Graphs: Backpropagation</a></p>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>构建tensorflow开发环境</title>
    <url>/blog/Install-tensorflow-devEnv/</url>
    <content><![CDATA[<p>笔者使用的是windows 10 系统。下面会使用docker来安装 tensorflow。</p>
<h1 id="安装-docker"><a href="#安装-docker" class="headerlink" title="安装 docker"></a>安装 docker</h1><p>对于window下docker 的安装，直接进入官网 <a href="https://www.docker.com/docker-windows" target="_blank" rel="noopener">https://www.docker.com/docker-windows</a> 下载安装包。需要注意的是，docker for windows 支持的是 <code>64bit</code> 操作系统。所以 <code>32bit</code> 的系统的环境，暂时这种方式还不支持。</p>
<h1 id="安装-tensorflow"><a href="#安装-tensorflow" class="headerlink" title="安装 tensorflow"></a>安装 tensorflow</h1><p>使用命令</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">run</span> -it -p <span class="number">8888</span>:<span class="number">8888</span> gcr.io/tensorflow/tensorflow</span><br></pre></td></tr></table></figure>
<p>就会自动从 <a href="http://gcr.io" target="_blank" rel="noopener">http://gcr.io</a> 上，下载 <code>tensorflow</code> 的镜像并运行，等待下载安装结束，直接在浏览器中输入 <code>localhost:8888</code> 进行访问。<br><img src="/img/post/201704/ff1427c2-3dbf-4f54-84c4-ac5a3a5f823d.png" alt="tensorflow"></p>
<a id="more"></a>
<p>就会进入 <a href="https://jupyter.org/" target="_blank" rel="noopener"><code>Jupyter Notebook</code></a> 应用。Jupter Notebook 是一个开源的 web 应用，是一款交互式的笔记。它可以在线运行输入的代码，并实时显示结果。所以我们可以在 Jupter Notebook 中，编写我们的 tensorflow 代码，同时可以直接看到代码运行结果。</p>
<h1 id="代码编写"><a href="#代码编写" class="headerlink" title="代码编写"></a>代码编写</h1><p>这里我们尝试修改例程 <code>1_hello_tensorflow_ipynb</code> 的代码，并用快捷键 <code>ctrl+Enter</code> 查看代码运行结果。<br><img src="/img/post/201704/71d000d8-57c3-4678-866e-ff0d56fda55b.png" alt="原代码"></p>
<p>我们这里尝试修改 <code>input2</code> 的向量为 <code>[2, 2, 4]</code> ，并使用快捷键 <code>ctrl+Enter</code> 查看二者相加的结果：<br><img src="/img/post/201704/bc16f459-722e-426e-8aa3-6429e0a369da.png" alt="修改后"></p>
<p> 可以看到 <code>input1</code> 和 <code>input2</code> 相加的结果已经变成了 <code>[3, 3, 3, 5]</code> 。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://jupyter.org/" target="_blank" rel="noopener">Jupter Notebook</a></li>
<li><a href="http://www.tuicool.com/articles/a6JRr2Y" target="_blank" rel="noopener">Jupyter Notebook 快速入门</a></li>
</ul>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Jekyll使用总结</title>
    <url>/blog/jekyll-using/</url>
    <content><![CDATA[<h1 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h1><h2 id="categories"><a href="#categories" class="headerlink" title="categories"></a>categories</h2><p>当文章设置了 categories 属性以后，访问该文章时候就会归入对应的 url 路径。<br>比如设置了：<code>categories: [&#39;Life&#39;]</code>，那么访问该文章的时候，URL路径就是 <a href="http://webname/`Life`/" target="_blank" rel="noopener">http://webname/`Life`/</a>…<br>比如设置了：<code>categories: [&#39;Life&#39;, &#39;eassy&#39;]</code>，那么访问该文章的时候，URL路径就是 <a href="http://webname/`Life`/`essay`/" target="_blank" rel="noopener">http://webname/`Life`/`essay`/</a>…<br>因为 <code>｛%</code> 会被 jekyll 解析成内部语法，所以用中文字符 <code>｛</code> 替换了 英文字符 <code>{</code> 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">｛% for category in site.categories %｝</span><br><span class="line"></span><br><span class="line">｛｛ category [0] ｝｝ 是 category name</span><br><span class="line">｛｛ category [1] ｝｝ 包含 category  下的 posts</span><br><span class="line"></span><br><span class="line">｛\% endfor %｝</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="tags"><a href="#tags" class="headerlink" title="tags"></a>tags</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">｛\% for tag in site.tags %｝</span><br><span class="line"></span><br><span class="line">｛｛ tag[0] ｝｝是 tag name</span><br><span class="line">｛｛ tag[1] ｝｝包含 tag 下的 posts</span><br><span class="line"></span><br><span class="line">｛% endfor %｝</span><br></pre></td></tr></table></figure>
<h1 id="过滤器使用"><a href="#过滤器使用" class="headerlink" title="过滤器使用"></a>过滤器使用</h1><h2 id="生成二级目录"><a href="#生成二级目录" class="headerlink" title="生成二级目录"></a>生成二级目录</h2><p>categories作为一级目录<br>tags作为二级目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">｛% for category in site.categories %｝    </span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">    ｛｛ category[0] ｝｝ (｛｛ category[1].size｝｝)</span><br><span class="line">        &lt;li&gt;        </span><br><span class="line">        ｛% for tag in site.tags %｝</span><br><span class="line">            ｛% assign blogPosts = site.posts | where: &apos;categories&apos;,  category[0] | where: &apos;tags&apos;, tag[0]%｝ // 这里使用 where filter 来找到属于当前category下的 属于tag[0] 的所有文章。</span><br><span class="line">            ｛% if blogPosts.size != 0 %｝</span><br><span class="line">                ｛｛ tag[0] ｝｝ ｛｛ blogPosts.size｝｝</span><br><span class="line">            ｛% endif %｝</span><br><span class="line">        ｛% endfor %｝       </span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">｛% endfor %｝</span><br></pre></td></tr></table></figure>
<p>最后生成的效果如图：</p>
<p><img src="/img/post/20170408/db593a7f-0d24-4e0f-a1ec-58ec7a399ee4.png" alt="效果图"></p>
<h1 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h1><h2 id="jekyll-生成文章过慢"><a href="#jekyll-生成文章过慢" class="headerlink" title="jekyll 生成文章过慢"></a>jekyll 生成文章过慢</h2><p>在某天的某刻，突然发现进行一次微小的post文章修改，jekyll serve –watch 命令下，regeneration 需要耗费 20s 多，以往都是 1-2s 就结束了。反复定位，发现是文章的 title 中写了一个 “C++”，而将其改为 “Cpp”以后就好了。虽然不知道为什么，但是问题还是解决了。猜测也许是因为 <code>+</code> 被jekyll 解析时候出了问题。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://stackoverflow.com/questions/27583597/categories-in-jekyll" target="_blank" rel="noopener">Categories in Jekyll</a></p>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>前端开发</tag>
        <tag>查阅</tag>
      </tags>
  </entry>
  <entry>
    <title>Google Cpp Style Guide - Naming</title>
    <url>/blog/Google-Cpp-Style-Guide-Naming/</url>
    <content><![CDATA[<h1 id="通用命名规则"><a href="#通用命名规则" class="headerlink" title="通用命名规则"></a>通用命名规则</h1><p>函数命名，变量命名，文件命名要有描述性；少用缩写。</p>
<p>尽可能给有描述性的命名，别心疼空间，毕竟让代码易于新读者理解很重要。不要用只有项目开发者能理解的缩写，也不要通过砍掉几个字母来缩写单词</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> price_count_reader;  <span class="comment">// 无缩写</span></span><br><span class="line"><span class="keyword">int</span> num_errors;  <span class="comment">// “num” 本来就很常见</span></span><br><span class="line"><span class="keyword">int</span> num_dns_connections; <span class="comment">// 人人都知道 “DNS” 是啥</span></span><br></pre></td></tr></table></figure>
<p><code>不要像下面这样写：</code></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n;                     <span class="comment">// 莫名其妙。</span></span><br><span class="line"><span class="keyword">int</span> nerr;                  <span class="comment">// 怪缩写。</span></span><br><span class="line"><span class="keyword">int</span> n_comp_conns;          <span class="comment">// 怪缩写。</span></span><br><span class="line"><span class="keyword">int</span> wgc_connections;       <span class="comment">// 只有贵团队知道是啥意思。</span></span><br><span class="line"><span class="keyword">int</span> pc_reader;             <span class="comment">// "pc" 有太多可能的解释了。</span></span><br><span class="line"><span class="keyword">int</span> cstmr_id;              <span class="comment">// 有删减若干字母。</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h1 id="文件命名"><a href="#文件命名" class="headerlink" title="文件命名"></a>文件命名</h1><p>文件名要全部小写, 可以包含下划线 (<code>_</code>) 或连字符 (<code>-</code>). 按项目约定来. 如果并没有项目约定，”_” 更好。</p>
<p>可接受的文件命名:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* my_useful_class.cc</span><br><span class="line">* my-useful-class.cc</span><br><span class="line">* myusefulclass.cc</span><br><span class="line">* muusefulclass_test.cc // ``_unittest`` 和 ``_regtest`` 已弃用。</span><br></pre></td></tr></table></figure>

<p>C++ 文件要以 <code>.cc</code> 结尾, 头文件以 <code>.h</code> 结尾. 专门插入文本的文件则以 <code>.inc</code> 结尾，参见 <code>1.1. Self-contained</code> 头文件。</p>
<p>不要使用已经存在于 <code>/usr/include</code> 下的文件名 (Yang.Y 注: 即编译器搜索系统头文件的路径), 如 <code>db.h</code>.</p>
<p>通常应尽量让文件名更加明确. <code>http_server_logs.h</code> 就比 <code>logs.h</code> 要好. 定义类时文件名一般成对出现, 如 <code>foo_bar.h</code> 和 <code>foo_bar.cc</code>, 对应于类 <code>FooBar</code>.</p>
<p>内联函数必须放在 <code>.h</code> 文件中. 如果内联函数比较短, 就直接放在 <code>.h</code> 中.</p>
<h1 id="类型命名"><a href="#类型命名" class="headerlink" title="类型命名"></a>类型命名</h1><p>类型名称的每个单词首字母均大写, 不包含下划线: <code>MyExcitingClass</code>, <code>MyExcitingEnum</code>.</p>
<p>所有类型命名 —— 类, 结构体, 类型定义 (<code>typedef</code>), 枚举 —— 均使用相同约定. 例如:</p>
<h1 id="变量命名"><a href="#变量命名" class="headerlink" title="变量命名"></a>变量命名</h1><p>变量名一律小写, 单词之间用下划线连接. 类的成员变量以下划线结尾, 但结构体的就不用，如:: <code>a_local_variable</code>, <code>a_struct_data_member</code>, <code>a_class_data_member_</code>.</p>
<p><strong>普通变量命名:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> table_name;  <span class="comment">// 可 - 用下划线。</span></span><br><span class="line"><span class="built_in">string</span> tablename;   <span class="comment">// 可 - 全小写。</span></span><br></pre></td></tr></table></figure>
<p><code>但是不要这样写：</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> tableName;   <span class="comment">// 差 - 混合大小写。</span></span><br></pre></td></tr></table></figure>

<p><strong>类数据成员：</strong></p>
<p>不管是静态的还是非静态的，类数据成员都可以和普通变量一样, 但要接下划线。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TableInfo</span> &#123;</span></span><br><span class="line">  ...</span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="built_in">string</span> table_name_;  <span class="comment">// 可 - 尾后加下划线。</span></span><br><span class="line">  <span class="built_in">string</span> tablename_;   <span class="comment">// 可。</span></span><br><span class="line">  <span class="keyword">static</span> Pool&lt;TableInfo&gt;* pool_;  <span class="comment">// 可。</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p><strong>结构体变量:</strong><br>不管是静态的还是非静态的，结构体数据成员都可以和普通变量一样, 不用像类那样接下划线:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">UrlTableProperties</span> &#123;</span></span><br><span class="line">    <span class="built_in">string</span> name;</span><br><span class="line">    <span class="keyword">int</span> num_entries;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结构体与类的讨论参考 <code>结构体 vs. 类</code> 一节.</p>
<p><strong>全局变量:</strong></p>
<p>对全局变量没有特别要求, 少用就好, 但如果你要用, 可以用 <code>g_</code> 或其它标志作为前缀, 以便更好的区分局部变量.</p>
<h1 id="常量命名"><a href="#常量命名" class="headerlink" title="常量命名"></a>常量命名</h1><p>在全局或类里的常量名称前加 <code>k</code>: kDaysInAWeek. 且除去开头的 <code>k</code> 之外每个单词开头字母均大写。</p>
<p>所有编译时常量, 无论是局部的, 全局的还是类中的, 和其他变量稍微区别一下. <code>k</code> 后接大写字母开头的单词:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> kDaysInAWeek = <span class="number">7</span>;</span><br></pre></td></tr></table></figure>

<p>这规则适用于编译时的局部作用域常量，不过要按变量规则来命名也可以。</p>
<h1 id="函数命名"><a href="#函数命名" class="headerlink" title="函数命名"></a>函数命名</h1><p>常规函数使用大小写混合, 取值和设值函数则要求与变量名匹配: <code>MyExcitingFunction()</code>, <code>MyExcitingMethod()</code>, <code>my_exciting_member_variable()</code>, <code>set_my_exciting_member_variable()</code>.</p>
<p><strong>常规函数:</strong></p>
<p>函数名的每个单词首字母大写, 没有下划线。</p>
<p>如果您的某函数出错时就要直接 crash, 那么就在函数名加上 OrDie. 但这函数本身必须集成在产品代码里，且平时也可能会出错。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">AddTableEntry()</span><br><span class="line">DeleteUrl()</span><br><span class="line">OpenFileOrDie()</span><br></pre></td></tr></table></figure>

<p><strong>取值和设值函数:</strong></p>
<p>取值（Accessors）和设值（Mutators）函数要与存取的变量名匹配. 这儿摘录一个类, <code>num_entries_</code> 是该类的实例变量:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        ...</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">num_entries</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> num_entries_; &#125;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">set_num_entries</span><span class="params">(<span class="keyword">int</span> num_entries)</span> </span>&#123; num_entries_ = num_entries; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="keyword">int</span> num_entries_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>其它非常短小的内联函数名也可以用小写字母, 例如. 如果你在循环中调用这样的函数甚至都不用缓存其返回值, 小写命名就可以接受.</p>
<h1 id="名字空间命名"><a href="#名字空间命名" class="headerlink" title="名字空间命名"></a>名字空间命名</h1><p>名字空间用小写字母命名, 并基于项目名称和目录结构: <code>google_awesome_project</code>.</p>
<p>关于名字空间的讨论和如何命名, 参考 <code>名字空间</code> 一节.</p>
<h1 id="枚举命名"><a href="#枚举命名" class="headerlink" title="枚举命名"></a>枚举命名</h1><p>枚举的命名应当和 常量 或 宏 一致: <code>kEnumName</code> 或是 <code>ENUM_NAME</code>.</p>
<p>单独的枚举值应该优先采用 常量 的命名方式. 但 宏 方式的命名也可以接受. 枚举名 <code>UrlTableErrors</code> (以及 <code>AlternateUrlTableErrors</code>) 是类型, 所以要用大小写混合的方式.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> UrlTableErrors &#123;</span><br><span class="line">    kOK = <span class="number">0</span>,</span><br><span class="line">    kErrorOutOfMemory,</span><br><span class="line">    kErrorMalformedInput,</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">enum</span> AlternateUrlTableErrors &#123;</span><br><span class="line">    OK = <span class="number">0</span>,</span><br><span class="line">    OUT_OF_MEMORY = <span class="number">1</span>,</span><br><span class="line">    MALFORMED_INPUT = <span class="number">2</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>2009 年 1 月之前, 我们一直建议采用 宏 的方式命名枚举值. 由于枚举值和宏之间的命名冲突, 直接导致了很多问题. 由此, 这里改为优先选择常量风格的命名方式. 新代码应该尽可能优先使用常量风格. 但是老代码没必要切换到常量风格, 除非宏风格确实会产生编译期问题.</p>
<h1 id="宏命名"><a href="#宏命名" class="headerlink" title="宏命名"></a>宏命名</h1><p>你并不打算 使用宏, 对吧? 如果你一定要用, 像这样命名: <code>MY_MACRO_THAT_SCARES_SMALL_CHILDREN</code>.</p>
<p>参考 预处理宏; 通常 不应该 使用宏. 如果不得不用, 其命名像枚举命名一样全部大写, 使用下划线:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#define ROUND(x) ...</span><br><span class="line">#define PI_ROUNDED 3.0</span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>Banner图转自<a href="http://blog.csdn.net/voidccc/article/details/37599203" target="_blank" rel="noopener">一张图总结Google C++编程规范</a></li>
<li>转载自<a href="http://zh-google-styleguide.readthedocs.io/en/latest/google-cpp-styleguide/naming/" target="_blank" rel="noopener">Google开源风格项目指南</a></li>
<li>英文原文:<a href="https://google.github.io/styleguide/cppguide.html#Naming" target="_blank" rel="noopener">Google C++ Style Guide</a></li>
</ul>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>查阅</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown基本语法</title>
    <url>/blog/markdown-syntax/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>转载于: <a href="https://github.com/riku/Markdown-Syntax-CN/blob/master/syntax.md#precode" target="_blank" rel="noopener">Markdown 语法说明 (简体中文版)</a></p>
<h2 id="宗旨"><a href="#宗旨" class="headerlink" title="宗旨"></a>宗旨</h2><p>Markdown 的目标是实现「易读易写」。</p>
<p>可读性，无论如何，都是最重要的。一份使用 Markdown 格式撰写的文件应该可以直接以纯文本发布，并且看起来不会像是由许多标签或是格式指令所构成。Markdown 语法受到一些既有 text-to-HTML 格式的影响，包括 [Setext] <a href="http://docutils.sourceforge.net/mirror/setext.html" target="_blank" rel="noopener">1</a>、[atx] <a href="http://www.aaronsw.com/2002/atx/" target="_blank" rel="noopener">2</a>、[Textile] <a href="http://textism.com/tools/textile/" target="_blank" rel="noopener">3</a>、[reStructuredText] <a href="http://docutils.sourceforge.net/rst.html" target="_blank" rel="noopener">4</a>、[Grutatext] <a href="http://www.triptico.com/software/grutatxt.html" target="_blank" rel="noopener">5</a> 和 [EtText] <a href="http://ettext.taint.org/doc/" target="_blank" rel="noopener">6</a>，而最大灵感来源其实是纯文本电子邮件的格式。</p>
<a id="more"></a>
<p>总之， Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像*强调*。Markdown 的列表看起来，嗯，就是列表。Markdown 的区块引用看起来就真的像是引用一段文字，就像你曾在电子邮件中见过的那样。</p>
<h2 id="兼容-HTML"><a href="#兼容-HTML" class="headerlink" title="兼容 HTML"></a>兼容 HTML</h2><p>Markdown 语法的目标是：成为一种适用于网络的<em>书写</em>语言。</p>
<p>Markdown 不是想要取代 HTML，甚至也没有要和它相近，它的语法种类很少，只对应 HTML 标记的一小部分。Markdown 的构想<em>不是</em>要使得 HTML 文档更容易书写。在我看来， HTML 已经很容易写了。Markdown 的理念是，能让文档更容易读、写和随意改。HTML 是一种<em>发布</em>的格式，Markdown 是一种<em>书写</em>的格式。就这样，Markdown 的格式语法只涵盖纯文本可以涵盖的范围。</p>
<p>不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML 撰写。不需要额外标注这是 HTML 或是 Markdown；只要直接加标签就可以了。</p>
<p>要制约的只有一些 HTML 区块元素――比如 <code>&lt;div&gt;</code>、<code>&lt;table&gt;</code>、<code>&lt;pre&gt;</code>、<code>&lt;p&gt;</code> 等标签，必须在前后加上空行与其它内容区隔开，还要求它们的开始标签与结尾标签不能用制表符或空格来缩进。Markdown 的生成器有足够智能，不会在 HTML 区块标签外加上不必要的 <code>&lt;p&gt;</code> 标签。</p>
<p>例子如下，在 Markdown 文件里加上一段 HTML 表格：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">这是一个普通段落。</span><br><span class="line"></span><br><span class="line">&lt;table&gt;</span><br><span class="line">    &lt;tr&gt;</span><br><span class="line">        &lt;td&gt;Foo&lt;<span class="regexp">/td&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>tr&gt;</span><br><span class="line">&lt;<span class="regexp">/table&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">这是另一个普通段落。</span></span><br></pre></td></tr></table></figure>
<p>请注意，在 HTML 区块标签间的 Markdown 格式语法将不会被处理。比如，你在 HTML 区块内使用 Markdown 样式的<code>*强调*</code>会没有效果。</p>
<p>HTML 的区段（行内）标签如 <code>&lt;span&gt;</code>、<code>&lt;cite&gt;</code>、<code>&lt;del&gt;</code> 可以在 Markdown 的段落、列表或是标题里随意使用。依照个人习惯，甚至可以不用 Markdown 格式，而直接采用 HTML 标签来格式化。举例说明：如果比较喜欢 HTML 的 <code>&lt;a&gt;</code> 或 <code>&lt;img&gt;</code> 标签，可以直接使用这些标签，而不用 Markdown 提供的链接或是图像标签语法。</p>
<p>和处在 HTML 区块标签间不同，Markdown 语法在 HTML 区段标签间是有效的。</p>
<h2 id="特殊字符自动转换"><a href="#特殊字符自动转换" class="headerlink" title="特殊字符自动转换"></a>特殊字符自动转换</h2><p>在 HTML 文件中，有两个字符需要特殊处理： <code>&lt;</code> 和 <code>&amp;</code> 。 <code>&lt;</code> 符号用于起始标签，<code>&amp;</code> 符号则用于标记 HTML 实体，如果你只是想要显示这些字符的原型，你必须要使用实体的形式，像是 <code>&amp;lt;</code> 和 <code>&amp;amp;</code>。</p>
<p><code>&amp;</code> 字符尤其让网络文档编写者受折磨，如果你要打「<code>AT&amp;T</code>」 ，你必须要写成「<code>AT&amp;amp;T</code>」。而网址中的 <code>&amp;</code> 字符也要转换。比如你要链接到：</p>
<p>  <a href="http://images.google.com/images?num=30" target="_blank" rel="noopener">http://images.google.com/images?num=30</a> <code>&amp;</code>q=larry+bird</p>
<p>你必须要把网址转换写为：</p>
<p>  <a href="http://images.google.com/images?num=30" target="_blank" rel="noopener">http://images.google.com/images?num=30</a> <code>&amp;amp;</code>q=larry+bird</p>
<p>才能放到链接标签的 <code>href</code> 属性里。不用说也知道这很容易忽略，这也可能是 HTML 标准检验所检查到的错误中，数量最多的。</p>
<p>Markdown 让你可以自然地书写字符，需要转换的由它来处理好了。如果你使用的 <code>&amp;</code> 字符是 HTML 字符实体的一部分，它会保留原状，否则它会被转换成 <code>&amp;amp</code>;。</p>
<p>所以你如果要在文档中插入一个版权符号 <code>©</code>，你可以这样写：</p>
<pre><code>&amp;copy;</code></pre><p>Markdown 会保留它不动。而若你写：</p>
<pre><code>AT&amp;T</code></pre><p>Markdown 就会将它转为：</p>
<pre><code>AT&amp;amp;T</code></pre><p>类似的状况也会发生在 <code>&lt;</code> 符号上，因为 Markdown 允许 <a href="#html">兼容 HTML</a> ，如果你是把 <code>&lt;</code> 符号作为 HTML 标签的定界符使用，那 Markdown 也不会对它做任何转换，但是如果你写：</p>
<pre><code>4 &lt; 5</code></pre><p>Markdown 将会把它转换为：</p>
<pre><code>4 &amp;lt; 5</code></pre><p>不过需要注意的是，code 范围内，不论是行内还是区块， <code>&lt;</code> 和 <code>&amp;</code> 两个符号都<em>一定</em>会被转换成 HTML 实体，这项特性让你可以很容易地用 Markdown 写 HTML code （和 HTML 相对而言， HTML 语法中，你要把所有的 <code>&lt;</code> 和 <code>&amp;</code> 都转换为 HTML 实体，才能在 HTML 文件里面写出 HTML code。）</p>
<hr>
<h1 id="区块元素"><a href="#区块元素" class="headerlink" title="区块元素"></a>区块元素</h1><h2 id="段落和换行"><a href="#段落和换行" class="headerlink" title="段落和换行"></a>段落和换行</h2><p>一个 Markdown 段落是由一个或多个连续的文本行组成，它的前后要有一个以上的空行（空行的定义是显示上看起来像是空的，便会被视为空行。比方说，若某一行只包含空格和制表符，则该行也会被视为空行）。普通段落不该用空格或制表符来缩进。</p>
<p>「由一个或多个连续的文本行组成」这句话其实暗示了 Markdown 允许段落内的强迫换行（插入换行符），这个特性和其他大部分的 text-to-HTML 格式不一样（包括 Movable Type 的「Convert Line Breaks」选项），其它的格式会把每个换行符都转成 <code>&lt;br /&gt;</code> 标签。</p>
<p>如果你<em>确实</em>想要依赖 Markdown 来插入 <code>&lt;br /&gt;</code> 标签的话，在插入处先按入两个以上的空格然后回车。</p>
<p>的确，需要多费点事（多加空格）来产生 <code>&lt;br /&gt;</code> ，但是简单地「每个换行都转换为 <code>&lt;br /&gt;</code>」的方法在 Markdown 中并不适合， Markdown 中 email 式的 <a href="#blockquote">区块引用</a> 和多段落的 <a href="#list">列表</a> 在使用换行来排版的时候，不但更好用，还更方便阅读。</p>
<h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p>Markdown 支持两种标题的语法，类 [Setext] <a href="http://docutils.sourceforge.net/mirror/setext.html" target="_blank" rel="noopener">1</a> 和类 [atx] <a href="http://www.aaronsw.com/2002/atx/" target="_blank" rel="noopener">2</a> 形式。</p>
<p>类 Setext 形式是用底线的形式，利用 <code>=</code> （最高阶标题）和 <code>-</code> （第二阶标题），例如：</p>
<pre><code>This is an H1
=============

This is an H2
-------------</code></pre><p>任何数量的 <code>=</code> 和 <code>-</code> 都可以有效果。</p>
<p>类 Atx 形式则是在行首插入 1 到 6 个 <code>#</code> ，对应到标题 1 到 6 阶，例如：</p>
<pre><code># 这是 H1

## 这是 H2

###### 这是 H6</code></pre><p>你可以选择性地「闭合」类 atx 样式的标题，这纯粹只是美观用的，若是觉得这样看起来比较舒适，你就可以在行尾加上 <code>#</code>，而行尾的 <code>#</code> 数量也不用和开头一样（行首的井字符数量决定标题的阶数）：</p>
<pre><code># 这是 H1 #

## 这是 H2 ##

### 这是 H3 ######</code></pre><h2 id="区块引用"><a href="#区块引用" class="headerlink" title="区块引用"></a>区块引用</h2><p>Markdown 标记区块引用是使用类似 email 中用 <code>&gt;</code> 的引用方式。如果你还熟悉在 email 信件中的引言部分，你就知道怎么在 Markdown 文件中建立一个区块引用，那会看起来像是你自己先断好行，然后在每行的最前面加上 <code>&gt;</code> ：</p>
<pre><code>&gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet,
&gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus.
&gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.
&gt;
&gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse
&gt; id sem consectetuer libero luctus adipiscing.</code></pre><p>Markdown 也允许你偷懒只在整个段落的第一行最前面加上 <code>&gt;</code> ：</p>
<pre><code>&gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet,
consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus.
Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.

&gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse
id sem consectetuer libero luctus adipiscing.</code></pre><p>区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 <code>&gt;</code> ：</p>
<pre><code>&gt; This is the first level of quoting.
&gt;
&gt; &gt; This is nested blockquote.
&gt;
&gt; Back to the first level.</code></pre><p>引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等：</p>
<pre><code>&gt; ## 这是一个标题。
&gt;
&gt; 1.   这是第一行列表项。
&gt; 2.   这是第二行列表项。
&gt;
&gt; 给出一些例子代码：
&gt;
&gt;     return shell_exec(&quot;echo $input | $markdown_script&quot;);</code></pre><p>任何像样的文本编辑器都能轻松地建立 email 型的引用。例如在 BBEdit 中，你可以选取文字后然后从选单中选择<em>增加引用阶层</em>。</p>
<h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><p>Markdown 支持有序列表和无序列表。</p>
<p>无序列表使用星号、加号或是减号作为列表标记：</p>
<pre><code>*   Red
*   Green
*   Blue</code></pre><p>等同于：</p>
<pre><code>+   Red
+   Green
+   Blue</code></pre><p>也等同于：</p>
<pre><code>-   Red
-   Green
-   Blue</code></pre><p>有序列表则使用数字接着一个英文句点：</p>
<pre><code>1.  Bird
2.  McHale
3.  Parish</code></pre><p>很重要的一点是，你在列表标记上使用的数字并不会影响输出的 HTML 结果，上面的列表所产生的 HTML 标记为：</p>
<pre><code>&lt;ol&gt;
&lt;li&gt;Bird&lt;/li&gt;
&lt;li&gt;McHale&lt;/li&gt;
&lt;li&gt;Parish&lt;/li&gt;
&lt;/ol&gt;</code></pre><p>如果你的列表标记写成：</p>
<pre><code>1.  Bird
1.  McHale
1.  Parish</code></pre><p>或甚至是：</p>
<pre><code>3. Bird
1. McHale
8. Parish</code></pre><p>你都会得到完全相同的 HTML 输出。重点在于，你可以让 Markdown 文件的列表数字和输出的结果相同，或是你懒一点，你可以完全不用在意数字的正确性。</p>
<p>如果你使用懒惰的写法，建议第一个项目最好还是从 1. 开始，因为 Markdown 未来可能会支持有序列表的 start 属性。</p>
<p>列表项目标记通常是放在最左边，但是其实也可以缩进，最多 3 个空格，项目标记后面则一定要接着至少一个空格或制表符。</p>
<p>要让列表看起来更漂亮，你可以把内容用固定的缩进整理好：</p>
<pre><code>*   Lorem ipsum dolor sit amet, consectetuer adipiscing elit.
    Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,
    viverra nec, fringilla in, laoreet vitae, risus.
*   Donec sit amet nisl. Aliquam semper ipsum sit amet velit.
    Suspendisse id sem consectetuer libero luctus adipiscing.</code></pre><p>但是如果你懒，那也行：</p>
<pre><code>*   Lorem ipsum dolor sit amet, consectetuer adipiscing elit.
Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,
viverra nec, fringilla in, laoreet vitae, risus.
*   Donec sit amet nisl. Aliquam semper ipsum sit amet velit.
Suspendisse id sem consectetuer libero luctus adipiscing.</code></pre><p>如果列表项目间用空行分开，在输出 HTML 时 Markdown 就会将项目内容用 <code>&lt;p&gt;</code><br>标签包起来，举例来说：</p>
<pre><code>*   Bird
*   Magic</code></pre><p>会被转换为：</p>
<pre><code>&lt;ul&gt;
&lt;li&gt;Bird&lt;/li&gt;
&lt;li&gt;Magic&lt;/li&gt;
&lt;/ul&gt;</code></pre><p>但是这个：</p>
<pre><code>*   Bird

*   Magic</code></pre><p>会被转换为：</p>
<pre><code>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bird&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Magic&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</code></pre><p>列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符：</p>
<pre><code>1.  This is a list item with two paragraphs. Lorem ipsum dolor
    sit amet, consectetuer adipiscing elit. Aliquam hendrerit
    mi posuere lectus.

    Vestibulum enim wisi, viverra nec, fringilla in, laoreet
    vitae, risus. Donec sit amet nisl. Aliquam semper ipsum
    sit amet velit.

2.  Suspendisse id sem consectetuer libero luctus adipiscing.</code></pre><p>如果你每行都有缩进，看起来会看好很多，当然，再次地，如果你很懒惰，Markdown 也允许：</p>
<pre><code>*   This is a list item with two paragraphs.

    This is the second paragraph in the list item. You&apos;re
only required to indent the first line. Lorem ipsum dolor
sit amet, consectetuer adipiscing elit.

*   Another item in the same list.</code></pre><p>如果要在列表项目内放进引用，那 <code>&gt;</code> 就需要缩进：</p>
<pre><code>*   A list item with a blockquote:

    &gt; This is a blockquote
    &gt; inside a list item.</code></pre><p>如果要放代码区块的话，该区块就需要缩进<em>两次</em>，也就是 8 个空格或是 2 个制表符：</p>
<pre><code>*   一列表项包含一个列表区块：

        &lt;代码写在这&gt;</code></pre><p>当然，项目列表很可能会不小心产生，像是下面这样的写法：</p>
<pre><code>1986. What a great season.</code></pre><p>换句话说，也就是在行首出现<em>数字-句点-空白</em>，要避免这样的状况，你可以在句点前面加上反斜杠。</p>
<pre><code>1986\. What a great season.</code></pre><h2 id="代码区块"><a href="#代码区块" class="headerlink" title="代码区块"></a>代码区块</h2><p>和程序相关的写作或是标签语言原始码通常会有已经排版好的代码区块，通常这些区块我们并不希望它以一般段落文件的方式去排版，而是照原来的样子显示，Markdown 会用 <code>&lt;pre&gt;</code> 和 <code>&lt;code&gt;</code> 标签来把代码区块包起来。</p>
<p>要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：</p>
<pre><code>这是一个普通段落：

    这是一个代码区块。</code></pre><p>Markdown 会转换成：</p>
<pre><code>&lt;p&gt;这是一个普通段落：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;这是一个代码区块。
&lt;/code&gt;&lt;/pre&gt;</code></pre><p>这个每行一阶的缩进（4 个空格或是 1 个制表符），都会被移除，例如：</p>
<pre><code>Here is an example of AppleScript:

    tell application &quot;Foo&quot;
        beep
    end tell</code></pre><p>会被转换为：</p>
<pre><code>&lt;p&gt;Here is an example of AppleScript:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tell application &quot;Foo&quot;
    beep
end tell
&lt;/code&gt;&lt;/pre&gt;</code></pre><p>一个代码区块会一直持续到没有缩进的那一行（或是文件结尾）。</p>
<p>在代码区块里面， <code>&amp;</code> 、 <code>&lt;</code> 和 <code>&gt;</code> 会自动转成 HTML 实体，这样的方式让你非常容易使用 Markdown 插入范例用的 HTML 原始码，只需要复制贴上，再加上缩进就可以了，剩下的 Markdown 都会帮你处理，例如：</p>
<pre><code>&lt;div class=&quot;footer&quot;&gt;
    &amp;copy; 2004 Foo Corporation
&lt;/div&gt;</code></pre><p>会被转换为：</p>
<pre><code>&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&quot;footer&quot;&amp;gt;
    &amp;amp;copy; 2004 Foo Corporation
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;</code></pre><p>代码区块中，一般的 Markdown 语法不会被转换，像是星号便只是星号，这表示你可以很容易地以 Markdown 语法撰写 Markdown 语法相关的文件。</p>
<h2 id="分隔线"><a href="#分隔线" class="headerlink" title="分隔线"></a>分隔线</h2><p>你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：</p>
<pre><code>* * *

***

*****

- - -

---------------------------------------</code></pre><hr>
<h1 id="区段元素"><a href="#区段元素" class="headerlink" title="区段元素"></a>区段元素</h1><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p>Markdown 支持两种形式的链接语法： <em>行内式</em>和<em>参考式</em>两种形式。</p>
<p>不管是哪一种，链接文字都是用 [方括号] 来标记。</p>
<p>要建立一个<em>行内式</em>的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字，只要在网址后面，用双引号把 title 文字包起来即可，例如：</p>
<pre><code>This is [an example](http://example.com/ &quot;Title&quot;) inline link.

[This link](http://example.net/) has no title attribute.</code></pre><p>会产生：</p>
<pre><code>&lt;p&gt;This is &lt;a href=&quot;http://example.com/&quot; title=&quot;Title&quot;&gt;
an example&lt;/a&gt; inline link.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://example.net/&quot;&gt;This link&lt;/a&gt; has no
title attribute.&lt;/p&gt;</code></pre><p>如果你是要链接到同样主机的资源，你可以使用相对路径：</p>
<pre><code>See my [About](/about/) page for details.   </code></pre><p><em>参考式</em>的链接是在链接文字的括号后面再接上另一个方括号，而在第二个方括号里面要填入用以辨识链接的标记：</p>
<pre><code>This is [an example][id] reference-style link.</code></pre><p>你也可以选择性地在两个方括号中间加上一个空格：</p>
<pre><code>This is [an example] [id] reference-style link.</code></pre><p>接着，在文件的任意处，你可以把这个标记的链接内容定义出来：</p>
<pre><code>[id]: http://example.com/  &quot;Optional Title Here&quot;</code></pre><p>链接内容定义的形式为：</p>
<ul>
<li>方括号（前面可以选择性地加上至多三个空格来缩进），里面输入链接文字</li>
<li>接着一个冒号</li>
<li>接着一个以上的空格或制表符</li>
<li>接着链接的网址</li>
<li>选择性地接着 title 内容，可以用单引号、双引号或是括弧包着</li>
</ul>
<p>下面这三种链接的定义都是相同：</p>
<pre><code>[foo]: http://example.com/  &quot;Optional Title Here&quot;
[foo]: http://example.com/  &apos;Optional Title Here&apos;
[foo]: http://example.com/  (Optional Title Here)</code></pre><p><strong>请注意：</strong>有一个已知的问题是 Markdown.pl 1.0.1 会忽略单引号包起来的链接 title。</p>
<p>链接网址也可以用尖括号包起来：</p>
<pre><code>[id]: &lt;http://example.com/&gt;  &quot;Optional Title Here&quot;</code></pre><p>你也可以把 title 属性放到下一行，也可以加一些缩进，若网址太长的话，这样会比较好看：</p>
<pre><code>[id]: http://example.com/longish/path/to/resource/here
    &quot;Optional Title Here&quot;</code></pre><p>网址定义只有在产生链接的时候用到，并不会直接出现在文件之中。</p>
<p>链接辨别标签可以有字母、数字、空白和标点符号，但是并<em>不</em>区分大小写，因此下面两个链接是一样的：</p>
<pre><code>[link text][a]
[link text][A]</code></pre><p><em>隐式链接标记</em>功能让你可以省略指定链接标记，这种情形下，链接标记会视为等同于链接文字，要用隐式链接标记只要在链接文字后面加上一个空的方括号，如果你要让 “Google” 链接到 google.com，你可以简化成：</p>
<pre><code>[Google][]</code></pre><p>然后定义链接内容：</p>
<pre><code>[Google]: http://google.com/</code></pre><p>由于链接文字可能包含空白，所以这种简化型的标记内也许包含多个单词：</p>
<pre><code>Visit [Daring Fireball][] for more information.</code></pre><p>然后接着定义链接：</p>
<pre><code>[Daring Fireball]: http://daringfireball.net/</code></pre><p>链接的定义可以放在文件中的任何一个地方，我比较偏好直接放在链接出现段落的后面，你也可以把它放在文件最后面，就像是注解一样。</p>
<p>下面是一个参考式链接的范例：</p>
<pre><code>I get 10 times more traffic from [Google] [1] than from
[Yahoo] [2] or [MSN] [3].

  [1]: http://google.com/        &quot;Google&quot;
  [2]: http://search.yahoo.com/  &quot;Yahoo Search&quot;
  [3]: http://search.msn.com/    &quot;MSN Search&quot;</code></pre><p>如果改成用链接名称的方式写：</p>
<pre><code>I get 10 times more traffic from [Google][] than from
[Yahoo][] or [MSN][].

  [google]: http://google.com/        &quot;Google&quot;
  [yahoo]:  http://search.yahoo.com/  &quot;Yahoo Search&quot;
  [msn]:    http://search.msn.com/    &quot;MSN Search&quot;</code></pre><p>上面两种写法都会产生下面的 HTML。</p>
<pre><code>&lt;p&gt;I get 10 times more traffic from &lt;a href=&quot;http://google.com/&quot;
title=&quot;Google&quot;&gt;Google&lt;/a&gt; than from
&lt;a href=&quot;http://search.yahoo.com/&quot; title=&quot;Yahoo Search&quot;&gt;Yahoo&lt;/a&gt;
or &lt;a href=&quot;http://search.msn.com/&quot; title=&quot;MSN Search&quot;&gt;MSN&lt;/a&gt;.&lt;/p&gt;</code></pre><p>下面是用行内式写的同样一段内容的 Markdown 文件，提供作为比较之用：</p>
<pre><code>I get 10 times more traffic from [Google](http://google.com/ &quot;Google&quot;)
than from [Yahoo](http://search.yahoo.com/ &quot;Yahoo Search&quot;) or
[MSN](http://search.msn.com/ &quot;MSN Search&quot;).</code></pre><p>参考式的链接其实重点不在于它比较好写，而是它比较好读，比较一下上面的范例，使用参考式的文章本身只有 81 个字符，但是用行内形式的却会增加到 176 个字元，如果是用纯 HTML 格式来写，会有 234 个字元，在 HTML 格式中，标签比文本还要多。</p>
<p>使用 Markdown 的参考式链接，可以让文件更像是浏览器最后产生的结果，让你可以把一些标记相关的元数据移到段落文字之外，你就可以增加链接而不让文章的阅读感觉被打断。</p>
<h2 id="强调"><a href="#强调" class="headerlink" title="强调"></a>强调</h2><p>Markdown 使用星号（<code>*</code>）和底线（<code>_</code>）作为标记强调字词的符号，被 <code>*</code> 或 <code>_</code> 包围的字词会被转成用 <code>&lt;em&gt;</code> 标签包围，用两个 <code>*</code> 或 <code>_</code> 包起来的话，则会被转成 <code>&lt;strong&gt;</code>，例如：</p>
<pre><code>*single asterisks*

_single underscores_

**double asterisks**

__double underscores__</code></pre><p>会转成：</p>
<pre><code>&lt;em&gt;single asterisks&lt;/em&gt;

&lt;em&gt;single underscores&lt;/em&gt;

&lt;strong&gt;double asterisks&lt;/strong&gt;

&lt;strong&gt;double underscores&lt;/strong&gt;</code></pre><p>你可以随便用你喜欢的样式，唯一的限制是，你用什么符号开启标签，就要用什么符号结束。</p>
<p>强调也可以直接插在文字中间：</p>
<pre><code>un*frigging*believable</code></pre><p>但是<strong>如果你的 <code>*</code> 和 <code>_</code> 两边都有空白的话，它们就只会被当成普通的符号</strong>。</p>
<p>如果要在文字前后直接插入普通的星号或底线，你可以用反斜线：</p>
<pre><code>\*this text is surrounded by literal asterisks\*</code></pre><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>如果要标记一小段行内代码，你可以用反引号把它包起来（<code>`</code>），例如：</p>
<pre><code>Use the `printf()` function.</code></pre><p>会产生：</p>
<pre><code>&lt;p&gt;Use the &lt;code&gt;printf()&lt;/code&gt; function.&lt;/p&gt;</code></pre><p>如果要在代码区段内插入反引号，你可以用多个反引号来开启和结束代码区段：</p>
<pre><code>``There is a literal backtick (`) here.``</code></pre><p>这段语法会产生：</p>
<pre><code>&lt;p&gt;&lt;code&gt;There is a literal backtick (`) here.&lt;/code&gt;&lt;/p&gt;</code></pre><p>代码区段的起始和结束端都可以放入一个空白，起始端后面一个，结束端前面一个，这样你就可以在区段的一开始就插入反引号：</p>
<pre><code>A single backtick in a code span: `` ` ``

A backtick-delimited string in a code span: `` `foo` ``</code></pre><p>会产生：</p>
<pre><code>&lt;p&gt;A single backtick in a code span: &lt;code&gt;`&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A backtick-delimited string in a code span: &lt;code&gt;`foo`&lt;/code&gt;&lt;/p&gt;</code></pre><p>在代码区段内，<code>&amp;</code> 和尖括号<strong>都</strong>会被自动地转成 HTML 实体，这使得插入 HTML 原始码变得很容易，Markdown 会把下面这段：</p>
<pre><code>Please don&apos;t use any `&lt;blink&gt;` tags.</code></pre><p>转为：</p>
<pre><code>&lt;p&gt;Please don&apos;t use any &lt;code&gt;&amp;lt;blink&amp;gt;&lt;/code&gt; tags.&lt;/p&gt;</code></pre><p>你也可以这样写：</p>
<pre><code>`&amp;#8212;` is the decimal-encoded equivalent of `&amp;mdash;`.</code></pre><p>以产生：</p>
<pre><code>&lt;p&gt;&lt;code&gt;&amp;amp;#8212;&lt;/code&gt; is the decimal-encoded
equivalent of &lt;code&gt;&amp;amp;mdash;&lt;/code&gt;.&lt;/p&gt;</code></pre><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p>很明显地，要在纯文字应用中设计一个「自然」的语法来插入图片是有一定难度的。</p>
<p>Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： <em>行内式</em>和<em>参考式</em>。</p>
<p>行内式的图片语法看起来像是：</p>
<pre><code>![Alt text](/path/to/img.jpg)

![Alt text](/path/to/img.jpg &quot;Optional title&quot;)</code></pre><p>详细叙述如下：</p>
<ul>
<li>一个惊叹号 <code>!</code></li>
<li>接着一个方括号，里面放上图片的替代文字</li>
<li>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上<br>选择性的 ‘title’ 文字。</li>
</ul>
<p>参考式的图片语法则长得像这样：</p>
<pre><code>![Alt text][id]</code></pre><p>「id」是图片参考的名称，图片参考的定义方式则和连结参考一样：</p>
<pre><code>[id]: url/to/image  &quot;Optional title attribute&quot;</code></pre><p>到目前为止， Markdown 还没有办法指定图片的宽高，如果你需要的话，你可以使用普通的 <code>&lt;img&gt;</code> 标签。</p>
<hr>
<h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><h2 id="自动链接"><a href="#自动链接" class="headerlink" title="自动链接"></a>自动链接</h2><p>Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用尖括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如：</p>
<pre><code>&lt;http://example.com/&gt;</code></pre><p>Markdown 会转为：</p>
<pre><code>&lt;a href=&quot;http://example.com/&quot;&gt;http://example.com/&lt;/a&gt;</code></pre><p>邮址的自动链接也很类似，只是 Markdown 会先做一个编码转换的过程，把文字字符转成 16 进位码的 HTML 实体，这样的格式可以糊弄一些不好的邮址收集机器人，例如：</p>
<pre><code>&lt;address@example.com&gt;</code></pre><p>Markdown 会转成：</p>
<pre><code>&lt;a href=&quot;&amp;#x6D;&amp;#x61;i&amp;#x6C;&amp;#x74;&amp;#x6F;:&amp;#x61;&amp;#x64;&amp;#x64;&amp;#x72;&amp;#x65;
&amp;#115;&amp;#115;&amp;#64;&amp;#101;&amp;#120;&amp;#x61;&amp;#109;&amp;#x70;&amp;#x6C;e&amp;#x2E;&amp;#99;&amp;#111;
&amp;#109;&quot;&gt;&amp;#x61;&amp;#x64;&amp;#x64;&amp;#x72;&amp;#x65;&amp;#115;&amp;#115;&amp;#64;&amp;#101;&amp;#120;&amp;#x61;
&amp;#109;&amp;#x70;&amp;#x6C;e&amp;#x2E;&amp;#99;&amp;#111;&amp;#109;&lt;/a&gt;</code></pre><p>在浏览器里面，这段字串（其实是 <code>&lt;a href=&quot;mailto:address@example.com&quot;&gt;address@example.com&lt;/a&gt;</code>）会变成一个可以点击的「<a href="mailto:address@example.com" target="_blank" rel="noopener">address@example.com</a>」链接。</p>
<p>（这种作法虽然可以糊弄不少的机器人，但并不能全部挡下来，不过总比什么都不做好些。不管怎样，公开你的信箱终究会引来广告信件的。）</p>
<h2 id="反斜杠"><a href="#反斜杠" class="headerlink" title="反斜杠"></a>反斜杠</h2><p>Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 <code>&lt;em&gt;</code> 标签），你可以在星号的前面加上反斜杠：</p>
<pre><code>\*literal asterisks\*</code></pre><p>Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：</p>
<pre><code>\   反斜线
`   反引号
*   星号
_   底线
{}  花括号
[]  方括号
()  括弧
#   井字号
+   加号
-   减号
.   英文句点
!   惊叹号</code></pre>]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>查阅</tag>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title>10分钟入门requireJs</title>
    <url>/blog/how-to-use-requirejs/</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><hr>
<p>下载 <code>requireJs</code>，然后在 <code>head</code> 中</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script src=<span class="string">"js/require.js"</span> data-main=<span class="string">"js/main"</span> defer <span class="keyword">async</span>=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure>

<p><code>async</code> 属性表明这个文件需要异步加载，避免网页失去响应。IE不支持这个属性，只支持 <code>defer</code>，所以把 <code>defer</code> 也写上。</p>
<p><code>data-main</code> 属性的作用是，指定网页程序的主模块。在上例中，就是js目录下面的 <code>main.js</code>，这个文件会第一个被 <code>require.js</code> 加载。由于<code>require.js</code> 默认的文件后缀名是js，所以可以把 <code>main.js</code> 简写成 <code>main</code>。</p>
<h1 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h1><hr>
<p><code>require</code> 会定义三个变量：<strong>define</strong>, <strong>require</strong>, <strong>requirejs</strong>，其中 <code>require</code> === <code>requirejs</code>，一般使用 <code>require</code> 更简短</p>
<a id="more"></a>
<ul>
<li>define 从名字就可以看出这个api是用来定义一个<strong>模块</strong></li>
<li>require 加载依赖模块，并执行加载完后的回调函数</li>
</ul>
<p>比如我们想写一个 a.js 的模块，实现一个功能：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">define(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">fun1</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        alert(<span class="string">"it works"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fun1();</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>这里通过 <code>define</code> 函数定义了一个模块，这是 <code>requirejs</code> 的标准写法。如果想在页面中使用该 js ，可以直接在 <code>html</code> 文件中调用：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">require</span>([<span class="string">"js/a"</span>]);</span><br></pre></td></tr></table></figure>

<p>如果我们的网页目录如下：</p>
<p><img src="/img/post/20170325/f9a21c59-60fe-4757-ba4d-5ce2db23756f.png" alt="Alt text"></p>
<p>并且 <code>index.html</code> 内容如下，通过主动 <code>require</code> 的方式调用 a.js：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">        &lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"js/lib/require.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">        &lt;!-- 这里只是主动加载了a.js，并没有定义加载结束后的回调函数，其功能即为只执行a.js中代码 --&gt;</span><br><span class="line">        &lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">            <span class="built_in">require</span>([<span class="string">"js/a"</span>]);</span><br><span class="line">        &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">      &lt;span&gt;body&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>body&gt;</span><br><span class="line">&lt;<span class="regexp">/html&gt;</span></span><br></pre></td></tr></table></figure>

<p>这时候网页就会弹出一个alert 对话框：<br><img src="/img/post/20170325/4cb5a7d9-a6f5-46fe-91be-88b60ac7a2eb.png" alt="Alt text"></p>
<p>如果将 <code>index.html</code> 写成如下：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">        &lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"js/lib/require.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">        &lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">            <span class="built_in">require</span>([<span class="string">"js/a"</span>], <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">                alert(<span class="string">"call back!"</span>);</span><br><span class="line">            &#125;);</span><br><span class="line">        &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">      &lt;span&gt;body&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>body&gt;</span><br><span class="line">&lt;<span class="regexp">/html&gt;</span></span><br></pre></td></tr></table></figure>

<p>那么会在 ‘It works’ 后，再弹出一个对话框：<br><img src="/img/post/20170325/ab9e13af-eb74-4469-a451-533c1ab52760.png" alt="Alt text"></p>
<h1 id="加载文件和全局配置"><a href="#加载文件和全局配置" class="headerlink" title="加载文件和全局配置"></a>加载文件和全局配置</h1><p>也许你会说，这样调用的方式，如果我有 a,b,c,d… 等 js 代码，不还是得一个个写到 <code>head</code> 里的 <code>&lt;script&gt;</code> 标签内调用么。不急，下面才是重点。</p>
<p>首先我们在 js 目录下新建一个 <code>main.js</code>：</p>
<p><img src="/img/post/20170325/dc5e5da3-fbce-4713-a010-dacae5bbd7f6.png" alt="Alt text"></p>
<p>然后在 <code>main.js</code> 中写如下代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">require</span>.config(&#123;</span><br><span class="line">    paths : &#123;</span><br><span class="line">        <span class="string">"a"</span> : <span class="string">"js/a"</span>   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">require</span>([<span class="string">'a'</span>],<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">alert(<span class="string">'finish load'</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>然后在 <code>index.html</code> 中这样写：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">        &lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"js/lib/require.js"</span> &gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">       &lt;script&gt;</span><br><span class="line">           <span class="built_in">require</span>([<span class="string">'js/main'</span>]);</span><br><span class="line">       &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">      &lt;span&gt;body&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>body&gt;</span><br><span class="line">&lt;<span class="regexp">/html&gt;</span></span><br></pre></td></tr></table></figure>

<p>这里用到了 <code>require.config</code>。<code>require.config</code> 是用来配置模块加载位置的。即给我们的 js 模块，取一个 别名，之后进行 <code>require</code> 的时候，就不用以输入路径的方式来调用，直接写这个 别名 就行。</p>
<p>可以看到上面代码写 <code>require</code> 的时候，直接是使用 <code>require([&#39;a&#39;]</code> 而不是之前的 <code>require([&quot;js/a&quot;]</code>。</p>
<p>这样如果我们有很多的代码，就可以在 <code>main.js</code> 中，先配置各个模块的路径，并起别名，然后挨个 <code>require</code> 调用就是了。就不用对 <code>head</code> 标签进行修改。</p>
<p>同时如果我们在 <code>index.html</code> 中这样写：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"js/lib/require.js"</span> data-main=<span class="string">"js/main"</span> defer <span class="keyword">async</span>=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure>

<p>这里 <code>data-main</code> 属性的作用是，指定网页程序的主模块。在上例中，就是js目录下面的 <code>main.js</code>，这个文件会自动被 <code>require.js</code> 加载。这样我们就只用写一个 <code>&lt;script&gt;</code> 标签，就实现了对所有的 js 模块的调用。</p>
<p><code>data-main</code> 还有一个重要的功能：<br>当 <code>script</code> 标签指定 <code>data-main</code> 属性时，<code>require</code> 会默认的将 <code>data-main</code> 指定的 js 为根路径。即之后如果写 <code>require</code> 来调用 js 模块，不需要再添加 ‘js’ 目录前缀。</p>
<p><code>async</code> 属性表明这个文件需要异步加载，避免网页失去响应。IE不支持这个属性，只支持 <code>defer</code>，所以把 <code>defer</code> 也写上。</p>
<p>下面是使用 <code>data-main</code> 属性时候的 html 和 js 文件写法：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">   &lt;head&gt;</span><br><span class="line">       &lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"js/lib/require.js"</span> data-main=<span class="string">"js/main"</span> defer <span class="keyword">async</span>=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">    &lt;<span class="regexp">/head&gt;</span></span><br><span class="line"><span class="regexp">    &lt;body&gt;</span></span><br><span class="line"><span class="regexp">       &lt;span&gt;body&lt;/</span>span&gt;</span><br><span class="line">    &lt;<span class="regexp">/body&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>html&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">require</span>.config(&#123;</span><br><span class="line">    <span class="comment">//这里看到指定 a.js 模块路径的时候，并没有写上其路径前缀 'js/'，这就是 data-main 属性的作用</span></span><br><span class="line">    paths : &#123;</span><br><span class="line">        <span class="string">"a_alias"</span> : <span class="string">"a"</span>   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">require</span>([<span class="string">'a_alias'</span>],<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">alert(<span class="string">'finish load'</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h1 id="加载不符合AMD规范模块"><a href="#加载不符合AMD规范模块" class="headerlink" title="加载不符合AMD规范模块"></a>加载不符合AMD规范模块</h1><hr>
<p>标准的写法是需要使用一个：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">define(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="comment">// your code here</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>将你的代码写在这个 <code>define</code> 内部。但是我们如果想要使用一个不是这种标准的 js 模块怎么破？</p>
<p>这里就要使用到 <code>shim</code>。我们看下面的完整配置 <code>main.js</code> ：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">requirejs.config(&#123;</span><br><span class="line">        paths: &#123;</span><br><span class="line">            jquery: [               </span><br><span class="line">                <span class="string">'lib/jquery.min'</span></span><br><span class="line">            ],</span><br><span class="line">            bootstrap: [</span><br><span class="line">                <span class="string">'//cdn.bootcss.com/bootstrap/3.3.7/js/bootstrap.min'</span>, <span class="comment">// 这里支持输入多个备选路径，前一个失败就是选择下一个</span></span><br><span class="line">                <span class="string">'lib/bootstrap.min'</span></span><br><span class="line">            ],</span><br><span class="line">            my:<span class="string">'my'</span></span><br><span class="line">        &#125;,</span><br><span class="line">        shim:&#123;</span><br><span class="line">            <span class="string">'bootstrap'</span>:&#123;</span><br><span class="line">                deps:[<span class="string">'jquery'</span>] <span class="comment">// 这里指定依赖关系，bootstrap 要在 jquery 加载完成之后再加载</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">'my'</span>:&#123;</span><br><span class="line">                deps:[<span class="string">'jquery'</span>,<span class="string">'bootstrap'</span>],</span><br><span class="line">                exports:<span class="string">'my_alis'</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">require</span>([</span><br><span class="line">        <span class="string">'jquery'</span>,<span class="string">'bootstrap'</span>,<span class="string">'my'</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="function"><span class="keyword">function</span>(<span class="params">$, my_alis</span>)</span>&#123;</span><br><span class="line">            <span class="comment">// my 中的变量和函数，只在这里有效。</span></span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>

<p>这里假设我使用了 <code>jquery</code> 和 <code>bootstrap</code> 模块，还有一个自己写的业务脚本 <code>my.js</code>。 <code>my.js</code> 因为要依赖于 <code>jquery</code> 和 <code>bootstrap</code> 所以在依赖关系中写了上面的配置。</p>
<p>关键在于 <code>export</code> 关键字的声明。这样我们就可以加载一个没有使用 <code>AMD</code> 规范编程的 js 模块了。并且模块的所有变量，和函数，都不是在全局域的，只在 回调函数中有效。从而避免了全局变量的污染问题。</p>
<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><hr>
<ul>
<li><a href="http://www.runoob.com/w3cnote/requirejs-tutorial-2.html" target="_blank" rel="noopener">JS模块化工具requirejs教程(二)：基本知识</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2012/11/require_js.html" target="_blank" rel="noopener">Javascript模块化编程（三）：require.js的用法</a></li>
</ul>
]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>前端开发</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title>A monitoring software for evaporator</title>
    <url>/blog/A-monitoring-software-for-evaporator/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Accompanied by the development of automation technology, configuration technology, DCS PLC and other industrial control technology advance with each passing day, monitoring configuration software is becoming more and more widespread in the field of industrial control. To improve the productive efficiency of evaporators, and to ensure the safety of the production, this project has designed a control system.</p>
<p>This project uses the Siemens WinCC configuration software to develop a monitoring system, configuring the communication between the upper and the lower computer. Siemens WinCC configuration software is a user-friendly interface software, by which users can edit interface of the operation screen, monitor screen, the alarm screen, real-time trend curves, history trend curve, and print as required.</p>
<p>The system has real-time dynamic display and operation functions. It can monitor the production process of the falling film evaporator system, can achieve remote operation of the on-site equipment, and can show the operation status of the device. It can also draw the pressure-time curve, record the time when operators login, and export alarm records, data records and other records from the database.</p>
<p>This system lowers the operator’s labor strength and improves productive efficiency via setting user permissions and alarm through a user-friendly interface.</p>
<h1 id="Software"><a href="#Software" class="headerlink" title="Software"></a>Software</h1><p>This project was finished in Perth, Australia, and was used to produce condensed carrot juice.</p>
<p><img src="index.png" alt=""></p>
<p>This project uses PID-controller to control the pressure, flow rate and temperature.</p>
<p><img src="facility.png" alt=""></p>
<p><img src="alarm.png" alt=""></p>
<p><img src="evaporaters.png" alt=""></p>
<p><img src="evaporater.jpg" alt=""></p>
]]></content>
      <categories>
        <category>project</category>
      </categories>
  </entry>
  <entry>
    <title>LCR meter</title>
    <url>/blog/LCR-meter/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This project is about the ARM, CPLD, and DDS technology. Based on the LCR meter theory, the whole system consists of a signal source module, voltage &amp; current detection module, the digital process module, etc. </p>
<p>The signal source module provides DC to 500K Hz sine wave, range from 10mV to 2V along with 10mA to 1A circuit source, based on DDS technology using CPLD controller. </p>
<p>Phase detection circuit calculates the phase difference of two sine wave using multi-slope analog-to-digital technology. Digital processing uses S3C6410 chip to achieve digital filter, vector decomposition. </p>
<p>I am in charge of the circuit design and CLPD program design, C++ interface design.</p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><img src="overview.jpg" alt=""></p>
<ul>
<li><code>CPLD Part</code>：1. Communication Interface; 2. Multi-Slope ADC controler; 3. SPI interface; 4.DDS(Direct Digital Systhesizer) module;</li>
<li><code>Analog Circuit</code>: 1. Measuring circuit design; 2. Amplifier stability analyse; 3. SI analyse;</li>
<li><code>Coding</code>: designed Measure class, using virtual class and virtual function.</li>
</ul>
<h1 id="FPGA-architechture"><a href="#FPGA-architechture" class="headerlink" title="FPGA architechture"></a>FPGA architechture</h1><p><img src="CPLD.png" alt=""></p>
<h1 id="Prototype"><a href="#Prototype" class="headerlink" title="Prototype"></a>Prototype</h1><p><img src="LCR.jpg" alt=""></p>
]]></content>
      <categories>
        <category>project</category>
      </categories>
  </entry>
  <entry>
    <title>Freescale Smart Car Race</title>
    <url>/blog/Freescale-smart-car/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The Freescale Cup, formerly known as the Smart Car Race, began in 2003 when Korea’s Hanyang University hosted 80 teams of students. Since that modest origin, the annual event has expanded throughout North America, Europe, China, India, Malaysia and Latin America, impacting more than 15,000 students at more than 500 schools.</p>
<p>The competition requires student teams to build, program and race a model car around a track for speed, with victory going to the fastest car that completes the course without derailing. The creation of each autonomous car requires embedded software programming and basic circuit creation, using Freescale parts included in the entry kit; student-developed motor control hardware and software, to propel and steer the intelligent vehicle; and a student-developed camera interface, to navigate the vehicle through the race.</p>
<p>In 2010, I became a member of the team of Southeast University, China. And in 2011, I became the team leader and won the <code>1st prize</code> of 7th Freescale Cup National University Students Car Race of Camera group.</p>
<h1 id="Car-model"><a href="#Car-model" class="headerlink" title="Car model"></a>Car model</h1><p>I am in charge of the circuit board design and the software programming. I designed the base algorithm to detect the roads and my team members helped me to debug the PID-control variables.</p>
<p><img src="model.png" alt="car"></p>
<p><img src="smart_car.gif" alt="video"></p>
]]></content>
      <categories>
        <category>project</category>
      </categories>
  </entry>
</search>
