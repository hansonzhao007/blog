<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/blog/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/blog/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="google59b6e75580092421.html" />











  
  
  <link href="/blog/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  
    

    
  

  
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lobster Two:300,300italic,400,400italic,700,700italiclucida grande:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="hadoop,mapreudce," />








  <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=5.1.2" />






<meta name="description" content="Hadoop Single Node InstallationJava environment setup12sudo apt-get updatesudo apt-get install default-jdk Adding a dedicated Hadoop system userThis will add the user hduser and the group hadoop to yo">
<meta name="keywords" content="hadoop,mapreudce">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop Installation">
<meta property="og:url" content="https:&#x2F;&#x2F;hansonzhao007.github.io&#x2F;blog&#x2F;Hadoop-Installation&#x2F;index.html">
<meta property="og:site_name" content="Infinite">
<meta property="og:description" content="Hadoop Single Node InstallationJava environment setup12sudo apt-get updatesudo apt-get install default-jdk Adding a dedicated Hadoop system userThis will add the user hduser and the group hadoop to yo">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-11-17T04:18:02.909Z">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'C04V4Z2EKD',
      apiKey: 'aed9efacc66cdcd0647b6fe1e4808317',
      indexName: 'myblog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://hansonzhao007.github.io/blog/Hadoop-Installation/"/>





  <title>Hadoop Installation | Infinite</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-115007486-1', 'auto');
  ga('send', 'pageview');
</script>











</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Infinite</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/blog/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      
        
        <li class="menu-item menu-item-feed">
          <a href="/blog/atom.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-rss"></i> <br />
            
            RSS
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" target="_blank" rel="noopener" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hansonzhao007.github.io/blog/blog/Hadoop-Installation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XS Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/img/avatar.webp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Infinite">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Hadoop Installation</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-25T16:20:40-05:00">
                2018-04-25
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-11-16T22:18:02-06:00">
                2019-11-16
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/os/" itemprop="url" rel="index">
                    <span itemprop="name">os</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/blog/Hadoop-Installation/" class="leancloud_visitors" data-flag-title="Hadoop Installation">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Hadoop-Single-Node-Installation"><a href="#Hadoop-Single-Node-Installation" class="headerlink" title="Hadoop Single Node Installation"></a>Hadoop Single Node Installation</h1><h2 id="Java-environment-setup"><a href="#Java-environment-setup" class="headerlink" title="Java environment setup"></a>Java environment setup</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install default-jdk</span><br></pre></td></tr></table></figure>
<h2 id="Adding-a-dedicated-Hadoop-system-user"><a href="#Adding-a-dedicated-Hadoop-system-user" class="headerlink" title="Adding a dedicated Hadoop system user"></a>Adding a dedicated Hadoop system user</h2><p>This will add the user hduser and the group hadoop to your local machine.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo addgroup hadoop</span><br><span class="line">sudo adduser --ingroup hadoop hduser</span><br></pre></td></tr></table></figure>

<h2 id="Configuring-SSH"><a href="#Configuring-SSH" class="headerlink" title="Configuring SSH"></a>Configuring SSH</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su hduser</span><br><span class="line">ssh-keygen -t rsa -P <span class="string">""</span></span><br><span class="line">cat <span class="variable">$HOME</span>/.ssh/id_rsa.pub &gt;&gt; <span class="variable">$HOME</span>/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>The output looks like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hduser@hanson:~$ ssh-keygen -t rsa -P &quot;&quot;</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/hduser/.ssh/id_rsa):</span><br><span class="line">Created directory &apos;/home/hduser/.ssh&apos;.</span><br><span class="line">Your identification has been saved in /home/hduser/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/hduser/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:VJjkPSKe0LsRjcS+avfbA/kolHR5iPP6OVDp1QaT2EM hduser@hanson</span><br><span class="line">The key&apos;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">|     ....*E.     |</span><br><span class="line">|     o.++o*      |</span><br><span class="line">|    ..=.=+o=     |</span><br><span class="line">|     o=**.o.o    |</span><br><span class="line">|     .=OS+ .     |</span><br><span class="line">|      =o=        |</span><br><span class="line">|     o.o +       |</span><br><span class="line">|    o + ooo      |</span><br><span class="line">|   . . +=o..     |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure>

<p><strong>enable SSH access to your local machine with this newly created key.</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hduser@hanson:~$ s</span><br></pre></td></tr></table></figure>
<p><strong>test the SSH setup by connecting to your local machine with the <code>hduser</code> user</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hduser@hanson:~$ ssh localhost</span><br></pre></td></tr></table></figure>
<p>You should install <code>ssh</code> first ( sudo apt-get install ssh)</p>
<p>Output should be like this:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">The authenticity of host <span class="string">'localhost (127.0.0.1)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>localhost<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* Documentation:  https://help.ubuntu.com</span></span><br><span class="line"><span class="string">* Management:     https://landscape.canonical.com</span></span><br><span class="line"><span class="string">* Support:        https://ubuntu.com/advantage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">0 packages can be updated.</span></span><br><span class="line"><span class="string">0 updates are security updates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The programs included with the Ubuntu system are free software;</span></span><br><span class="line"><span class="string">the exact distribution terms for each program are described in the</span></span><br><span class="line"><span class="string">individual files in /usr/share/doc/*/copyright.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by</span></span><br><span class="line"><span class="string">applicable law.</span></span><br></pre></td></tr></table></figure>

<h2 id="Disabling-IPv6"><a href="#Disabling-IPv6" class="headerlink" title="Disabling IPv6"></a>Disabling IPv6</h2><p>One problem with IPv6 on Ubuntu is that using <code>0.0.0.0</code> for the various networking-related Hadoop configuration options will result in Hadoop binding to the IPv6 addresses of my Ubuntu box. In my case, I realized that there’s no practical point in enabling IPv6 on a box when you are not connected to any IPv6 network. Hence, I simply disabled IPv6 on my Ubuntu machine. Your mileage may vary.</p>
<p>To disable IPv6 on Ubuntu 10.04 LTS, open <code>/etc/sysctl.conf</code> in the editor of your choice and add the following lines to the end of the file:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># disable ipv6</span></span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br></pre></td></tr></table></figure>
<p>You have to reboot your machine in order to make the changes take effect.</p>
<p>You can check whether IPv6 is enabled on your machine with the following command:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/sys/net/ipv6/conf/all/disable_ipv6</span><br></pre></td></tr></table></figure>
<p>If output is 1, then ipv6 is disabled.</p>
<h3 id="Alternative"><a href="#Alternative" class="headerlink" title="Alternative"></a>Alternative</h3><p>You can also disable IPv6 only for Hadoop as documented in <a href="https://issues.apache.org/jira/browse/HADOOP-3437" target="_blank" rel="noopener">HADOOP-3437</a>. You can do so by adding the following line to <code>hadoop/etc/hadoop/hadoop-env.sh</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_OPTS=-Djava.net.preferIPv4Stack=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="Hadoop-installation"><a href="#Hadoop-installation" class="headerlink" title="Hadoop installation"></a>Hadoop installation</h2><p>Download Hadoop from the <code>Apache Download Mirrors</code> and extract the contents of the Hadoop package to a location of your choice. I picked $HADOOP_HOME. Make sure to change the owner of all the files to the hduser user and hadoop group, for example:</p>
<ul>
<li>wget <a href="http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz" target="_blank" rel="noopener">http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz</a> (download hadoop)</li>
<li>tar -xzvf hadoop-2.7.4.tar.gz (extract compressed file)</li>
<li>sudo mv hadoop-2.7.4 $HADOOP_HOME</li>
<li>sudo chown -R hduser:hadoop hadoop</li>
</ul>
<h2 id="Update-HOME-bashrc"><a href="#Update-HOME-bashrc" class="headerlink" title="Update $HOME/.bashrc"></a>Update $HOME/.bashrc</h2><p>Add the following lines to the <code>end</code> of the <code>$HOME/.bashrc</code> file of user hduser. If you use a shell other than bash, you should of course update its appropriate configuration files instead of <code>.bashrc</code>.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Hadoop-related environment variables</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_PREFIX=~/Program/hadoop <span class="comment">#This is where your put your hadoop program</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on)</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=$(readlink -f /usr/bin/java | sed <span class="string">"s:bin/java::"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some convenient aliases and functions for running Hadoop-related commands</span></span><br><span class="line"><span class="built_in">unalias</span> fs &amp;&gt; /dev/null</span><br><span class="line"><span class="built_in">alias</span> fs=<span class="string">"hadoop fs"</span></span><br><span class="line"><span class="built_in">unalias</span> hls &amp;&gt; /dev/null</span><br><span class="line"><span class="built_in">alias</span> hls=<span class="string">"fs -ls"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If you have LZO compression enabled in your Hadoop cluster and</span></span><br><span class="line"><span class="comment"># compress job outputs with LZOP (not covered in this tutorial):</span></span><br><span class="line"><span class="comment"># Conveniently inspect an LZOP compressed file from the command</span></span><br><span class="line"><span class="comment"># line; run via:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># $ lzohead /hdfs/path/to/lzop/compressed/file.lzo</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Requires installed 'lzop' command.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="function"><span class="title">lzohead</span></span> () &#123;</span><br><span class="line">hadoop fs -cat <span class="variable">$1</span> | lzop -dc | head -1000 | less</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add Hadoop bin/ directory to PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<p>Then you should run ‘source .bashrc’ to enable the new configuration.</p>
<h2 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h2><p>The only required environment variable we have to configure for Hadoop in this tutorial is <code>JAVA_HOME</code>. Open <code>hadoop/etc/hadoop/hadoop-env.sh</code></p>
<ul>
<li><p>readlink -f /usr/bin/java | sed “s:bin/java::” (find the default Java path)</p>
</li>
<li><p>sudo vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $HADOOP_HOME/etc/hadoop/hadoop-env.sh</span></span><br><span class="line"><span class="comment">#export JAVA_HOME=$&#123;JAVA_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=$(readlink -f /usr/bin/java | sed <span class="string">"s:bin/java::"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>$HADOOP_HOME/bin/hadoop</code> or <code>hadoop</code>(run hadoop, following appears, then hadoop is installed correctly)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">CLASSNAME            run the class named CLASSNAME</span><br><span class="line">or</span><br><span class="line"><span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">fs                   run a generic filesystem user client</span><br><span class="line">version              <span class="built_in">print</span> the version</span><br><span class="line">jar &lt;jar&gt;            run a jar file</span><br><span class="line">note: please use <span class="string">"yarn jar"</span> to launch</span><br><span class="line">YARN applications, not this <span class="built_in">command</span>.</span><br><span class="line">checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">classpath            prints the class path needed to get the</span><br><span class="line">credential           interact with credential providers</span><br><span class="line">Hadoop jar and the required libraries</span><br><span class="line">daemonlog            get/<span class="built_in">set</span> the <span class="built_in">log</span> level <span class="keyword">for</span> each daemon</span><br></pre></td></tr></table></figure>
<p>You can repeat this exercise also for other users who want to use Hadoop.</p>
</li>
</ul>
<p>Create a directory called input in our home directory and copy Hadoop’s configuration files into it to use those files as our data.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/input</span><br><span class="line">cp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/*.xml ~/input</span><br></pre></td></tr></table></figure>

<p>run cmd:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar grep ~/input ~/grep_example <span class="string">'principal[.]*'</span></span><br></pre></td></tr></table></figure>
<p>You can see output like following:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">Output</span><br><span class="line">. . .</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes <span class="built_in">read</span>=1247674</span><br><span class="line">FILE: Number of bytes written=2324248</span><br><span class="line">FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=2</span><br><span class="line">Map output records=2</span><br><span class="line">Map output bytes=37</span><br><span class="line">Map output materialized bytes=47</span><br><span class="line">Input split bytes=114</span><br><span class="line">Combine input records=0</span><br><span class="line">Combine output records=0</span><br><span class="line">Reduce input groups=2</span><br><span class="line">Reduce shuffle bytes=47</span><br><span class="line">Reduce input records=2</span><br><span class="line">Reduce output records=2</span><br><span class="line">Spilled Records=4</span><br><span class="line">Shuffled Maps =1</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=1</span><br><span class="line">GC time elapsed (ms)=61</span><br><span class="line">Total committed heap usage (bytes)=263520256</span><br><span class="line">Shuffle Errors</span><br><span class="line">BAD_ID=0</span><br><span class="line">CONNECTION=0</span><br><span class="line">IO_ERROR=0</span><br><span class="line">WRONG_LENGTH=0</span><br><span class="line">WRONG_MAP=0</span><br><span class="line">WRONG_REDUCE=0</span><br><span class="line">File Input Format Counters</span><br><span class="line">Bytes Read=151</span><br><span class="line">File Output Format Counters</span><br><span class="line">Bytes Written=37</span><br></pre></td></tr></table></figure>

<p>If you run cmd:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/grep_example/*</span><br></pre></td></tr></table></figure>

<p>You will see:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Output</span><br><span class="line">6       principal</span><br><span class="line">1       principal.</span><br></pre></td></tr></table></figure>
<p>But if you run this test again, it will give some error. Don’t worry, just delete the <code>grep_example</code> folder then everything will work fine.</p>
<h2 id="Hadoop-Pseudo-distributed-Mode"><a href="#Hadoop-Pseudo-distributed-Mode" class="headerlink" title="Hadoop Pseudo-distributed Mode"></a>Hadoop Pseudo-distributed Mode</h2><h3 id="HDFS-Configuration"><a href="#HDFS-Configuration" class="headerlink" title="HDFS Configuration"></a>HDFS Configuration</h3><p>In this section, we will configure the directory where Hadoop will store its data files, the network ports it listens to, etc. Our setup will use Hadoop’s Distributed File System, <code>HDFS</code>, even though our little “cluster” only contains our single local machine.</p>
<p>You can leave the settings below “as is” with the exception of the <code>hadoop.tmp.dir</code> parameter – this parameter you must change to a directory of your choice. We will use the directory <code>/app/hadoop/tmp</code> in this tutorial. Hadoop’s default configurations use <code>hadoop.tmp.dir</code> as the base temporary directory both for the local file system and HDFS, so don’t be surprised if you see Hadoop creating the specified directory automatically on HDFS at some later point.</p>
<p>Now we create the directory and set the required ownerships and permissions:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/Program/hadoop/tmp</span><br><span class="line">sudo chown hduser:hadoop ~/Program/hadoop/tmp</span><br><span class="line"><span class="comment"># ...and if you want to tighten up security, chmod from 755 to 750...</span></span><br><span class="line">sudo chmod 750 ~/Program/hadoop/tmp</span><br></pre></td></tr></table></figure>

<p>Add the following snippets between the <configuration> … </configuration> tags in the respective configuration XML file.</p>
<p>HDFS is the distributed file system used by Hadoop to store data in the cluster, capable of hosting very very (very) large files, splitting them over the nodes of the cluster. Theoretically, you don’t need to have it running and files could instead be stored elsewhere like S3 or even the local file system (if using a purely local Hadoop installation). However, some applications require interactions with HDFS so you may have to set it up sooner or later if you’re using third party modules. HDFS is composed of a<code>NameNode</code> which holds all the metadata regarding the stored files, and <code>DataNodes</code> (one per node in the cluster) which hold the actual data.</p>
<p>The main HDFS configuration file is located at <code>$HADOOP_PREFIX/etc/hadoop/hdfs-site.xml</code>. If you’ve been following since the beginning, this file should be empty so it will use the default configurations outlined in <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">this page</a>. For a single-node installation of HDFS you’ll want to change <code>hdfs-site.xml</code> to have, at the very least, the following:</p>
<p>First you should create <code>dfs</code> folder, <code>datanode</code> &amp; <code>namenode</code> folder under <code>tmp</code> folder.<br>tmp<br> |– dfs<br>   |– datanode<br>   |– namenode</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/osboxes/Program/hadoop/tmp/dfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/osboxes/Program/hadoop/tmp/dfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Path on the local filesystem where the NameNode stores the namespace and transaction logs persistently.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Default block replication.</span><br><span class="line">The actual number of replications can be specified when the file is created.</span><br><span class="line">The default is used if replication is not specified in create time.</span><br><span class="line"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>In addition, add the following to <code>$HADOOP_PREFIX/etc/hadoop/core-site.xml</code> to let the Hadoop modules know where the <code>HDFS NameNode</code> is located.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/osboxes/Program/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>NameNode URI<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><code>Note</code>:<br>If you configured <code>core-site.xml</code>, then the bellow example testing will fail.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This test will fail in to connection refuse</span></span><br><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar grep ~/input ~/grep_example <span class="string">'principal[.]*'</span></span><br></pre></td></tr></table></figure>

<h3 id="YARN-on-a-Single-Node"><a href="#YARN-on-a-Single-Node" class="headerlink" title="YARN on a Single Node"></a>YARN on a Single Node</h3><p>YARN is the component responsible for allocating containers to run tasks, coordinating the execution of said tasks, restart them in case of failure, among other housekeeping. Just like HDFS, it also has 2 main components: a ResourceManager which keeps track of the cluster resources and NodeManagers in each of the nodes which communicates with the ResourceManager and sets up containers for execution of tasks.</p>
<p>You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.</p>
<p>Configure parameters as follows: etc/hadoop/<code>mapred-site.xml:</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p> <code>$HADOOP_PREFIX/etc/hadoop/yarn-site.xml</code>. The file should currently be empty which means it’s using the default configurations you can find <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">here</a>. For a single-node installation of YARN you’ll want to add the following to that file:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;128&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Minimum <span class="built_in">limit</span> of memory to allocate to each container request at the Resource Manager.&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Maximum <span class="built_in">limit</span> of memory to allocate to each container request at the Resource Manager.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;The minimum allocation <span class="keyword">for</span> every container request at the RM, <span class="keyword">in</span> terms of virtual CPU cores. Requests lower than this won<span class="string">'t take effect, and the specified value will get allocated the minimum.&lt;/description&gt;</span></span><br><span class="line"><span class="string">    &lt;/property&gt;</span></span><br><span class="line"><span class="string">    &lt;property&gt;</span></span><br><span class="line"><span class="string">        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span></span><br><span class="line"><span class="string">        &lt;value&gt;2&lt;/value&gt;</span></span><br><span class="line"><span class="string">        &lt;description&gt;The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this won'</span>t take effect, and will get capped to this value.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Physical memory, <span class="keyword">in</span> MB, to be made available to running containers&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Number of CPU cores that can be allocated <span class="keyword">for</span> containers.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Starting"><a href="#Starting" class="headerlink" title="Starting"></a>Starting</h1><p>Now that we’ve finished configuring everything, it’s time to setup the folders and start the daemons:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Start HDFS daemons</span></span><br><span class="line"><span class="comment"># Format the namenode directory (DO THIS ONLY ONCE, THE FIRST TIME)</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hdfs namenode -format</span><br><span class="line"><span class="comment"># Start the namenode daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh start namenode</span><br><span class="line"><span class="comment"># Start the datanode daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/hadoop-daemon.sh start datanode</span><br><span class="line"><span class="comment">## Start YARN daemons</span></span><br><span class="line"><span class="comment"># Start the resourcemanager daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="comment"># Start the nodemanager daemon</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Format the namenode directory (DO THIS ONLY ONCE, THE FIRST TIME)</span><br><span class="line">$HADOOP_PREFIX/bin/hdfs namenode -format</span><br><span class="line"># Start NameNode daemon and DataNode daemon, The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs).</span><br><span class="line">$HADOOP_PREFIX/sbin/start-dfs.sh</span><br><span class="line">$HADOOP_PREFIX/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>Hopefully, everything should be running. Use the command <code>jps</code> to see if all daemons are launched. If one is missing, check <code>$HADOOP_PREFIX/logs/&lt;daemon with problems&gt;.log</code> for any errors.</p>
<p>The output looks like this:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hduser@hanson:<span class="variable">$HADOOP_HOME</span>/etc/hadoop$ jps</span><br><span class="line">7890 DataNode</span><br><span class="line">16585 SecondaryNameNode</span><br><span class="line">12722 ResourceManager</span><br><span class="line">13013 NodeManager</span><br><span class="line">13126 Jps</span><br><span class="line">7703 NameNode</span><br></pre></td></tr></table></figure>

<p>Make the HDFS directories required to execute MapReduce jobs:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hdfs dfs -mkdir /user</span><br><span class="line">$ bin/hdfs dfs -mkdir /user/&lt;username&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><p>To test if everything is working ok, lets run one of the example applications shipped with Hadoop called DistributedShell. This application spawns a specified number of containers and runs a shell command in each of them. Lets run DistributedShell with the ‘date’ command which outputs the current time:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run Distributed shell with 2 containers and executing the script `date`.</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hadoop jar <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar org.apache.hadoop.yarn.applications.distributedshell.Client --jar <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar --shell_command date --num_containers 2 --master_memory 1024</span><br></pre></td></tr></table></figure>

<p>The output looks like this:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Initializing Client</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Running Client</span><br><span class="line">17/11/26 11:20:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Got Cluster metric info from ASM, numNodeManagers=1</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Got Cluster node info from ASM</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Got node report from ASM <span class="keyword">for</span>, nodeId=hanson:35765, nodeAddresshanson:8042, nodeRackName/default-rack, nodeNumContainers0</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: Queue info, queueName=default, queueCurrentCapacity=0.0, queueMaxCapacity=1.0, queueApplicationCount=0, queueChildQueueCount=0</span><br><span class="line">17/11/26 11:20:20 INFO distributedshell.Client: User ACL Info <span class="keyword">for</span> Queue, queueName=root, userAcl=SUBMIT_APPLICATIONS</span><br><span class="line">...</span><br><span class="line">17/11/26 11:20:32 INFO distributedshell.Client: Got application report from ASM <span class="keyword">for</span>, appId=1, clientToAMToken=null, appDiagnostics=, appMasterHost=hanson/10.211.55.3, appQueue=default, appMasterRpcPort=-1, appStartTime=1511716821975, yarnAppState=FINISHED, distributedFinalState=SUCCEEDED, appTrackingUrl=http://hanson:8088/proxy/application_1511716475099_0001/, appUser=hduser</span><br><span class="line">17/11/26 11:20:32 INFO distributedshell.Client: Application has completed successfully. Breaking monitoring loop</span><br><span class="line">17/11/26 11:20:32 INFO distributedshell.Client: Application completed successfully</span><br></pre></td></tr></table></figure>
<p>With this command we are telling hadoop to run the <code>Client class</code> in the <code>hadoop-yarn-applications-distributedshell-2.7.4.jar</code>, passing it the jar containing the definition of the ApplicationMaster (the same jar), the shell command to run in each of the <code>hosts (date)</code>, the number of containers to spawn (2) and the memory used by the ApplicationMaster (1024MB). The value of 1024 was set empirically by trying to run the program several times until it stopped failing due to the ApplicationMaster using more memory than that which had been allocated to it. You can <code>check the entire set of parameters</code> you can pass to DistributedShell by using the same command <code>without any arguments</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the parameters for the DistributedShell client.</span></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/bin/hadoop jar <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar org.apache.hadoop.yarn.applications.distributedshell.Client</span><br></pre></td></tr></table></figure>
<p>The output should look like this:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">17/11/26 11:29:46 INFO distributedshell.Client: Initializing Client</span><br><span class="line">No jar file specified <span class="keyword">for</span> application master</span><br><span class="line">usage: Client</span><br><span class="line">-appname &lt;arg&gt;                                 Application Name. Default</span><br><span class="line">value - DistributedShell</span><br><span class="line">-attempt_failures_validity_interval &lt;arg&gt;      when</span><br><span class="line">attempt_failures_validity_</span><br><span class="line">interval <span class="keyword">in</span> milliseconds</span><br><span class="line">is <span class="built_in">set</span> to &gt; 0,the failure</span><br><span class="line">number will not take</span><br><span class="line">failures <span class="built_in">which</span> happen out</span><br><span class="line">of the validityInterval</span><br><span class="line">into failure count. If</span><br><span class="line">failure count reaches to</span><br><span class="line">maxAppAttempts, the</span><br><span class="line">application will be</span><br><span class="line">failed.</span><br><span class="line">...</span><br></pre></td></tr></table></figure>


<h1 id="Hadoop-Web-Interfaces"><a href="#Hadoop-Web-Interfaces" class="headerlink" title="Hadoop Web Interfaces"></a>Hadoop Web Interfaces</h1><h2 id="Web-UIs-for-the-Common-User"><a href="#Web-UIs-for-the-Common-User" class="headerlink" title="Web UIs for the Common User"></a>Web UIs for the Common User</h2><hr>
<p>The default Hadoop ports are as follows:</p>
<table>
<thead>
<tr>
<th>Daemon</th>
<th>Default Port</th>
<th>Configuration Parameter</th>
</tr>
</thead>
<tbody><tr>
<td>——HDFS——-</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Namenode</td>
<td>50070</td>
<td>dfs.http.address</td>
</tr>
<tr>
<td>Datanodes</td>
<td>50075</td>
<td>dfs.datanode.http.address</td>
</tr>
<tr>
<td>Secondarynamenode</td>
<td>50090</td>
<td>dfs.secondary.http.address</td>
</tr>
<tr>
<td>Backup/Checkpoint node?</td>
<td>50105</td>
<td>dfs.backup.http.address</td>
</tr>
<tr>
<td>—-MapReduce—-</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Jobracker</td>
<td>50030</td>
<td>mapred.job.tracker.http.address</td>
</tr>
<tr>
<td>Tasktrackers</td>
<td>50060</td>
<td>mapred.task.tracker.http.address</td>
</tr>
</tbody></table>
<p>Hadoop daemons expose some information over HTTP. All Hadoop daemons expose the following:</p>
<ul>
<li><code>/logs</code>:<br>Exposes, for downloading, log files in the Java system property <strong>hadoop.log.dir</strong>.</li>
<li><code>/logLevel</code>:<br>Allows you to dial up or down <strong>log4j</strong> logging levels. This is similar to <strong>hadoop daemonlog</strong> on the command line.</li>
<li><code>/stacks</code>:<br>Stack traces for all threads. Useful for debugging.</li>
<li><code>/metrics</code>:<br>Metrics for the server. Use /metrics?format=json to retrieve the data in a structured form. <code>Available in 0.21</code>.</li>
</ul>
<p>Individual daemons expose extra daemon-specific endpoints as well. Note that these are not necessarily part of Hadoop’s public API, so they tend to change over time.</p>
<p>The <code>Namenode</code> exposes:</p>
<ul>
<li><code>/</code>:<br>Shows information about the namenode as well as the HDFS. There’s a link from here to browse the filesystem, as well.</li>
<li><code>/dfsnodelist.jsp?whatNodes=(DEAD|LIVE)</code>:<br>Shows lists of nodes that are disconnected from (DEAD) or connected to (LIVE) the namenode.</li>
<li><code>/fsck</code>:<br>Runs the “fsck” command. Not recommended on a busy cluster.</li>
<li><code>/listPaths</code>:<br>Returns an XML-formatted directory listing. This is useful if you wish (for example) to poll HDFS to see if a file exists. The URL can include a path (e.g., /listPaths/user/philip) and can take optional GET arguments: /listPaths?recursive=yes will return all files on the file system; /listPaths/user/philip?filter=s.* will return all files in the home directory that start with s; and /listPaths/user/philip?exclude=.txt will return all files except text files in the home directory. Beware that filter and exclude operate on the directory listed in the URL, and they ignore the recursive flag.</li>
<li><code>/data</code> and <code>/fileChecksum</code><br>These forward your HTTP request to an appropriate datanode, which in turn returns the data or the checksum.</li>
</ul>
<p><code>Datanodes</code> expose the following:</p>
<ul>
<li><code>/browseBlock.jsp</code>, <code>/browseDirectory.jsp, tail.jsp</code>, <code>/streamFile</code>, <code>/getFileChecksum</code><br>These are the endpoints that the namenode redirects to when you are browsing filesystem content. You probably wouldn’t use these directly, but this is what’s going on underneath.</li>
<li><code>/blockScannerReport</code><br>Every datanode verifies its blocks at configurable intervals. This endpoint provides a listing of that check.</li>
</ul>
<p>The <code>secondarynamenode</code> exposes a simple status page with information including which namenode it’s talking to, when the last checkpoint was, how big it was, and which directories it’s using.</p>
<p>The <code>jobtracker</code>‘s UI is commonly used to look at running jobs, and, especially, to find the causes of failed jobs. The UI is best browsed starting at <code>/jobtracker.jsp</code>. There are over a dozen related pages providing details on tasks, history, scheduling queues, jobs, etc.</p>
<p><code>Tasktrackers</code> have a simple page (<code>/tasktracker.jsp</code>), which shows running tasks. They also expose <code>/taskLog?taskid=</code>to query logs for a specific task. They use <code>/mapOutput</code> to serve the output of map tasks to reducers, but this is an internal API.</p>
<h2 id="Under-the-Covers-for-the-Developer-and-the-System-Administrator"><a href="#Under-the-Covers-for-the-Developer-and-the-System-Administrator" class="headerlink" title="Under the Covers for the Developer and the System Administrator"></a>Under the Covers for the Developer and the System Administrator</h2><hr>
<p>Internally, Hadoop mostly uses Hadoop IPC to communicate amongst servers. (Part of the goal of the Apache Avro project is to replace Hadoop IPC with something that is easier to evolve and more language-agnostic; HADOOP-6170 is the relevant ticket.) Hadoop also uses HTTP (for the secondarynamenode communicating with the namenode and for the tasktrackers serving map outputs to the reducers) and a raw network socket protocol (for datanodes copying around data).</p>
<p>The following table presents the ports and protocols (including the relevant Java class) that Hadoop uses. This table does not include the HTTP ports mentioned above.</p>
<table>
<thead>
<tr>
<th>Daemon</th>
<th>Default Port</th>
<th>Configuration Parameter</th>
<th>Protocol</th>
<th>Used for</th>
</tr>
</thead>
<tbody><tr>
<td>Namenode</td>
<td>8020</td>
<td>fs.default.name</td>
<td>IPC: ClientProtocol</td>
<td>Filesystem metadata operations</td>
</tr>
<tr>
<td>Datanode</td>
<td>50010</td>
<td>dfs.datanode.address</td>
<td>Custom Hadoop Xceiver: DataNode and DFSClient</td>
<td>DFS data transfer</td>
</tr>
<tr>
<td>Datanode</td>
<td>50020</td>
<td>dfs.datanode.ipc.address</td>
<td>IPC: InterDatanodeProtocol, ClientDatanodeProtocolClientProtocol</td>
<td>Block metadata operations and recovery</td>
</tr>
<tr>
<td>Backupnode</td>
<td>50100</td>
<td>dfs.backup.address</td>
<td>Same as namenode</td>
<td>HDFS Metadata Operations</td>
</tr>
<tr>
<td>&gt; This is the port part of hdfs://host:8020/.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Default is not well-defined. Common values are 8021, 9001, or 8012. See <a href="http://issues.apache.org/jira/browse/MAPREDUCE-566" target="_blank" rel="noopener">MAPREDUCE-566</a>.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Binds to an unused local port.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h1 id="Multinode-configuration"><a href="#Multinode-configuration" class="headerlink" title="Multinode configuration"></a>Multinode configuration</h1><h2 id="Node-configure"><a href="#Node-configure" class="headerlink" title="Node configure"></a>Node configure</h2><p>First get every machine’s ip address. Assuming we have 3 machine, 1 master and 2 slaves.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master: 10.211.55.3</span><br><span class="line">slave1: 10.211.55.6</span><br><span class="line">slave2: 10.211.55.7</span><br></pre></td></tr></table></figure>

<p>Then go to every machine’s <code>/etc/hostname</code> file, change them:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/hostname</span><br><span class="line"># change hostname to master, slave1, slave2</span><br></pre></td></tr></table></figure>

<p>Then go to every machine’s <code>/etc/hosts</code> file, add the same content:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.211.55.3 master</span><br><span class="line">10.211.55.6 slave1</span><br><span class="line">10.211.55.7 slave2</span><br></pre></td></tr></table></figure>

<p>Then reboot 3 machine and conform every machine’s hostname:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">osboxes@master:~$ hostname</span><br><span class="line">master</span><br><span class="line">osboxes@master:~$ ping slave1</span><br><span class="line">PING slave1 (10.211.55.6) 56(84) bytes of data.</span><br><span class="line">64 bytes from slave1 (10.211.55.6): icmp_seq=1 ttl=64 time=0.466 ms</span><br><span class="line">64 bytes from slave1 (10.211.55.6): icmp_seq=2 ttl=64 time=0.417 ms</span><br><span class="line">64 bytes from slave1 (10.211.55.6): icmp_seq=3 ttl=64 time=0.402 ms</span><br><span class="line"></span><br><span class="line">osboxes@master:~$ ping slave2</span><br><span class="line">PING slave2 (10.211.55.7) 56(84) bytes of data.</span><br><span class="line">64 bytes from slave2 (10.211.55.7): icmp_seq=1 ttl=64 time=0.491 ms</span><br><span class="line">64 bytes from slave2 (10.211.55.7): icmp_seq=2 ttl=64 time=0.336 ms</span><br><span class="line">64 bytes from slave2 (10.211.55.7): icmp_seq=3 ttl=64 time=0.392 ms</span><br></pre></td></tr></table></figure>

<p>Then in every machine, use <code>ssh</code> command to link every other machine: ( This is used to enable ssh without password)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">osboxes@master:~$ ssh slave1</span><br><span class="line">The authenticity of host <span class="string">'slave1 (10.211.55.6)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>slave1,10.211.55.6<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* Documentation:  https://help.ubuntu.com</span></span><br><span class="line"><span class="string">* Management:     https://landscape.canonical.com</span></span><br><span class="line"><span class="string">* Support:        https://ubuntu.com/advantage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">21 packages can be updated.</span></span><br><span class="line"><span class="string">0 updates are security updates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Last login: Wed Nov 29 17:08:56 2017 from 127.0.0.1</span></span><br><span class="line"><span class="string">osboxes@slave1:~$ exit</span></span><br><span class="line"><span class="string">logout</span></span><br><span class="line"><span class="string">Connection to slave1 closed.</span></span><br><span class="line"><span class="string">osboxes@master:~$ ssh slave2</span></span><br><span class="line"><span class="string">The authenticity of host '</span>slave2 (10.211.55.7)<span class="string">' can'</span>t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span><br><span class="line">Are you sure you want to <span class="built_in">continue</span> connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added <span class="string">'slave2,10.211.55.7'</span> (ECDSA) to the list of known hosts.</span><br><span class="line">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span><br><span class="line"></span><br><span class="line">* Documentation:  https://help.ubuntu.com</span><br><span class="line">* Management:     https://landscape.canonical.com</span><br><span class="line">* Support:        https://ubuntu.com/advantage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">21 packages can be updated.</span><br><span class="line">0 updates are security updates.</span><br><span class="line"></span><br><span class="line">Last login: Wed Nov 29 17:08:56 2017 from 127.0.0.1</span><br><span class="line">osboxes@slave2:~$ <span class="built_in">exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave2 closed.</span><br><span class="line">osboxes@master:~$ ssh master</span><br><span class="line">The authenticity of host <span class="string">'master (10.211.55.3)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:emnk3O6O2N7CK8chUMIThK3CGFUwFOS44kzsa0phArE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>master,10.211.55.3<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Welcome to Ubuntu 17.10 (GNU/Linux 4.13.0-17-generic x86_64)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">* Documentation:  https://help.ubuntu.com</span></span><br><span class="line"><span class="string">* Management:     https://landscape.canonical.com</span></span><br><span class="line"><span class="string">* Support:        https://ubuntu.com/advantage</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">21 packages can be updated.</span></span><br><span class="line"><span class="string">0 updates are security updates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Last login: Wed Nov 29 17:08:56 2017 from 127.0.0.1</span></span><br><span class="line"><span class="string">osboxes@master:~$ exit</span></span><br><span class="line"><span class="string">logout</span></span><br><span class="line"><span class="string">Connection to master closed.</span></span><br><span class="line"><span class="string">osboxes@master:~$</span></span><br></pre></td></tr></table></figure>

<h2 id="Update-core-site-xml-in-all-machine"><a href="#Update-core-site-xml-in-all-machine" class="headerlink" title="Update core-site.xml in all machine"></a>Update core-site.xml in all machine</h2><p>Delete this configure:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file:///home/osboxes/Program/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>And change all <code>fs.default</code> property to:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000/&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;NameNode URI&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Update-hdfs-site-xml-in-all-machine"><a href="#Update-hdfs-site-xml-in-all-machine" class="headerlink" title="Update hdfs-site.xml in all machine"></a>Update hdfs-site.xml in all machine</h2><p>in master node, delete <code>datanode</code> property and change <code>dfs.replication</code> to 2:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Put site-specific property overrides <span class="keyword">in</span> this file. --&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file:///home/osboxes/Program/hadoop/tmp/dfs/namenode&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Path on the <span class="built_in">local</span> filesystem <span class="built_in">where</span> the NameNode stores the namespace and transaction logs persistently.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Default block replication.</span><br><span class="line">The actual number of replications can be specified when the file is created.</span><br><span class="line">The default is used <span class="keyword">if</span> replication is not specified <span class="keyword">in</span> create time.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>In the slave node, delete <code>namenode</code> property and change <code>dfs.replication</code> to 2</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file:///home/osboxes/Program/hadoop/tmp/dfs/datanode&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Comma separated list of paths on the <span class="built_in">local</span> filesystem of a DataNode <span class="built_in">where</span> it should store its blocks.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Default block replication.</span><br><span class="line">The actual number of replications can be specified when the file is created.</span><br><span class="line">The default is used <span class="keyword">if</span> replication is not specified <span class="keyword">in</span> create time.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Update-yarn-site-xml-in-all-machine"><a href="#Update-yarn-site-xml-in-all-machine" class="headerlink" title="Update yarn-site.xml in all machine"></a>Update yarn-site.xml in all machine</h2><p>insert the following to all 3 machines.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:8025&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:8030&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:8050&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Update-mapred-site-xml"><a href="#Update-mapred-site-xml" class="headerlink" title="Update mapred-site.xml"></a>Update mapred-site.xml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Master-or-Slaves-only-configuration"><a href="#Master-or-Slaves-only-configuration" class="headerlink" title="Master or Slaves only configuration"></a>Master or Slaves only configuration</h2><p>edit <code>~/Program/hadoop/etc/hadoop/slaves</code> file (Master only)<br>change to this value:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<p>edit <code>~/Program/hadoop/etc/hadoop/masters</code> file (Master only)<br>change to this value:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master</span><br></pre></td></tr></table></figure>

<p>Recreate Namenode folder (Master Only):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/Program/hadoop/tmp/dfs/namenode</span><br><span class="line">chown osboxes:osboxes -R ~/Program/hadoop/tmp/</span><br><span class="line">chmod 777 ~/Program/hadoop/tmp/dfs/namenode</span><br></pre></td></tr></table></figure>

<p>Recreate Datanode folder (All slave Nodes only):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/Program/hadoop/tmp/dfs/datanode</span><br><span class="line">chown osboxes:osboxes -R ~/Program/hadoop/tmp/</span><br><span class="line">chmod 777 ~/Program/hadoop/tmp/dfs/datanode</span><br></pre></td></tr></table></figure>

<p>Format the Namenode （Master only）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h2 id="Start-the-DFS-amp-Yarn（Master-only）"><a href="#Start-the-DFS-amp-Yarn（Master-only）" class="headerlink" title="Start the DFS &amp; Yarn（Master only）"></a>Start the DFS &amp; Yarn（Master only）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">osboxes@master:~/Program/hadoop/sbin$ ./start-dfs.sh</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">master: starting namenode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-namenode-master.out</span><br><span class="line">slave2: starting datanode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-datanode-slave2.out</span><br><span class="line">slave1: starting datanode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-datanode-slave1.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /home/osboxes/Program/hadoop/logs/hadoop-osboxes-secondarynamenode-master.out</span><br><span class="line">osboxes@master:~/Program/hadoop/sbin$ jps</span><br><span class="line">7316 Jps</span><br><span class="line">6904 NameNode</span><br><span class="line">7180 SecondaryNameNode</span><br><span class="line">osboxes@master:~/Program/hadoop/sbin$ ./start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/osboxes/Program/hadoop/logs/yarn-osboxes-resourcemanager-master.out</span><br><span class="line">slave2: starting nodemanager, logging to /home/osboxes/Program/hadoop/logs/yarn-osboxes-nodemanager-slave2.out</span><br><span class="line">slave1: starting nodemanager, logging to /home/osboxes/Program/hadoop/logs/yarn-osboxes-nodemanager-slave1.out</span><br><span class="line">osboxes@master:~/Program/hadoop/sbin$ jps</span><br><span class="line">7665 Jps</span><br><span class="line">7382 ResourceManager</span><br><span class="line">6904 NameNode</span><br><span class="line">7180 SecondaryNameNode</span><br></pre></td></tr></table></figure>

<p>If you go to your slave machine, then you will see：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">osboxes@slave1:~/Program/hadoop/tmp/dfs$ jps</span><br><span class="line">2769 NodeManager</span><br><span class="line">2538 DataNode</span><br><span class="line">2891 Jps</span><br></pre></td></tr></table></figure>

<h2 id="Review-Yarn-console"><a href="#Review-Yarn-console" class="headerlink" title="Review Yarn console"></a>Review Yarn console</h2><p>If all the services started successfully on all nodes, then you should see all of your nodes listed under Yarn nodes. You can hit the following url on your browser and verify that:</p>
<p><a href="http://master:8088/cluster/nodes" target="_blank" rel="noopener">http://master:8088/cluster/nodes</a><br><a href="http://master:50070" target="_blank" rel="noopener">http://master:50070</a></p>
<p>You can also get the report of your cluster by issuing the below commands:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -report</span><br></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#installation" target="_blank" rel="noopener">Running Hadoop on Ubuntu Linux (Single-Node Cluster)</a></li>
<li><a href="https://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide/" target="_blank" rel="noopener">Hadoop YARN Installation: The definitive guide</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-16-04" target="_blank" rel="noopener">How to Install Hadoop in Stand-Alone Mode on Ubuntu 16.04</a></li>
<li><a href="http://www.powerxing.com/install-hadoop/" target="_blank" rel="noopener">Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04</a></li>
<li><a href="http://blog.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/" target="_blank" rel="noopener">Hadoop Default Ports Quick Reference</a></li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Hadoop: Setting up a Single Node Cluster.</a></li>
</ul>

      
    </div>

    
    

    


    
    
    

    


    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    XS Zhao
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="https://hansonzhao007.github.io/blog/Hadoop-Installation/" title="Hadoop Installation">https://hansonzhao007.github.io/blog/Hadoop-Installation/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/hadoop/" rel="tag"># hadoop</a>
          
            <a href="/blog/tags/mapreudce/" rel="tag"># mapreudce</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div id="wpac-rating"></div>
          </div>
        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/shared-libraries-%E6%A6%82%E8%BF%B0/" rel="next" title="Shared Libraries 概述">
                <i class="fa fa-chevron-left"></i> Shared Libraries 概述
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/%E7%AC%AC%E4%B8%80%E4%B8%AA-Dapp/" rel="prev" title="第一个 Dapp">
                第一个 Dapp <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <a href="/" class="site-author-image" rel="start" style="border:none">
          <img class="site-author-image" itemprop="image"
               src="/blog/img/avatar.webp"
               alt="XS Zhao" />
        </a>
          <p class="site-author-name" itemprop="name">XS Zhao</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/blog/archives/">
                <span class="site-state-item-count">60</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/blog/categories/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/blog/tags/index.html">
                <span class="site-state-item-count">36</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/hansonzhao007" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/hansonzhao007" target="_blank" title="Facebook">
                  
                    <i class="fa fa-fw fa-facebook"></i>
                  
                    
                      Facebook
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/hansonzhao007" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                    
                      Instagram
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:xingshengzhao@gmail.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        
        
        <div id="days"></div>
</script>
<script language="javascript">
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("03/25/2017 12:00:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="Running: "+daysold+" days "+hrsold+":"+minsold+":"+seconds+"";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>


    <script type="text/javascript" src="//ra.revolvermaps.com/0/0/8.js?i=0457258m5ho&amp;m=2&amp;c=ff007e&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
    <div class="links-of-blogroll motion-element links-of-blogroll-block">
      <div class="links-of-blogroll-title">
        <!-- modify icon to fire by szw -->
        <i class="fa fa-history fa-" aria-hidden="true"></i>
        Recent Posts
      </div>
      <ol style="text-align: left;">
        
        
        
          <li>
            <a href="/blog/Ethereum-Project-Infrastructure/" title="Ethereum Project Infrastructure" target="_blank">Ethereum Project Infrastructure</a>
          </li>
        
        
          <li>
            <a href="/blog/Dapp-Lottery-Contract/" title="Dapp: Lottery Contract" target="_blank">Dapp: Lottery Contract</a>
          </li>
        
        
          <li>
            <a href="/blog/Write-ethereum-test-code/" title="Write ethereum test code" target="_blank">Write ethereum test code</a>
          </li>
        
        
          <li>
            <a href="/blog/Review-bLSM-%E2%88%97-A-General-Purpose-Log-Structured-Merge-Tree/" title="Review: bLSM:* A General Purpose Log Structured Merge Tree" target="_blank">Review: bLSM:* A General Purpose Log Structured Merge Tree</a>
          </li>
        
        
          <li>
            <a href="/blog/Review-ElasticBF-Fine-grained-and-Elastic-Bloom-Filter-Towards-Efficient-Read-for-LSM-tree-based-KV-Stores/" title="Review: ElasticBF: Fine-grained and Elastic Bloom Filter Towards Efficient Read for LSM-tree-based KV Stores" target="_blank">Review: ElasticBF: Fine-grained and Elastic Bloom Filter Towards Efficient Read for LSM-tree-based KV Stores</a>
          </li>
        
      </ol>
    </div>



      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop-Single-Node-Installation"><span class="nav-number">1.</span> <span class="nav-text">Hadoop Single Node Installation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Java-environment-setup"><span class="nav-number">1.1.</span> <span class="nav-text">Java environment setup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adding-a-dedicated-Hadoop-system-user"><span class="nav-number">1.2.</span> <span class="nav-text">Adding a dedicated Hadoop system user</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Configuring-SSH"><span class="nav-number">1.3.</span> <span class="nav-text">Configuring SSH</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Disabling-IPv6"><span class="nav-number">1.4.</span> <span class="nav-text">Disabling IPv6</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Alternative"><span class="nav-number">1.4.1.</span> <span class="nav-text">Alternative</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop-installation"><span class="nav-number">1.5.</span> <span class="nav-text">Hadoop installation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-HOME-bashrc"><span class="nav-number">1.6.</span> <span class="nav-text">Update $HOME/.bashrc</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hadoop-env-sh"><span class="nav-number">1.7.</span> <span class="nav-text">hadoop-env.sh</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop-Pseudo-distributed-Mode"><span class="nav-number">1.8.</span> <span class="nav-text">Hadoop Pseudo-distributed Mode</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HDFS-Configuration"><span class="nav-number">1.8.1.</span> <span class="nav-text">HDFS Configuration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#YARN-on-a-Single-Node"><span class="nav-number">1.8.2.</span> <span class="nav-text">YARN on a Single Node</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Starting"><span class="nav-number">2.</span> <span class="nav-text">Starting</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Testing"><span class="nav-number">3.</span> <span class="nav-text">Testing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop-Web-Interfaces"><span class="nav-number">4.</span> <span class="nav-text">Hadoop Web Interfaces</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Web-UIs-for-the-Common-User"><span class="nav-number">4.1.</span> <span class="nav-text">Web UIs for the Common User</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Under-the-Covers-for-the-Developer-and-the-System-Administrator"><span class="nav-number">4.2.</span> <span class="nav-text">Under the Covers for the Developer and the System Administrator</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multinode-configuration"><span class="nav-number">5.</span> <span class="nav-text">Multinode configuration</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Node-configure"><span class="nav-number">5.1.</span> <span class="nav-text">Node configure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-core-site-xml-in-all-machine"><span class="nav-number">5.2.</span> <span class="nav-text">Update core-site.xml in all machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-hdfs-site-xml-in-all-machine"><span class="nav-number">5.3.</span> <span class="nav-text">Update hdfs-site.xml in all machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-yarn-site-xml-in-all-machine"><span class="nav-number">5.4.</span> <span class="nav-text">Update yarn-site.xml in all machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-mapred-site-xml"><span class="nav-number">5.5.</span> <span class="nav-text">Update mapred-site.xml</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Master-or-Slaves-only-configuration"><span class="nav-number">5.6.</span> <span class="nav-text">Master or Slaves only configuration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Start-the-DFS-amp-Yarn（Master-only）"><span class="nav-number">5.7.</span> <span class="nav-text">Start the DFS &amp; Yarn（Master only）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Review-Yarn-console"><span class="nav-number">5.8.</span> <span class="nav-text">Review Yarn console</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XS Zhao</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/blog/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/blog/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/blog/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: '1524691240000',
            owner: 'hansonzhao007',
            repo: 'blog',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: '48dc7c1aa2e663a79e994f8cffc90423ea3f5e25',
            
                client_id: '4fe1228acaf3e7b057de'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    




  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/blog/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("LnSmAXnUoFenNXvIMojtgaj7-gzGzoHsz", "Ei6PVzIV1zEq1wuXHCWHdDPe");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 11916,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  
  


  
  <script type="text/javascript" src="/blog/js/src/js.cookie.js?v=5.1.2"></script>
  <script type="text/javascript" src="/blog/js/src/scroll-cookie.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/blog/js/src/custom.js"></script>

  <script type="text/javascript" src="/blog/js/src/clipboard.min.js"></script>



 
</body>
</html>
